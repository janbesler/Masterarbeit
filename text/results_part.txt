For the Vanilla Transformer, the embedding size of $64$ has the lowest losses across the different embedding lengths, therefor further experiments are conducted using this hyperparameter. The average duration is similar to the other models, therefor no special weights have to be assigned here to these influences. \par
For the different forecast horizons it is consistent that the L1 Loss is training faster than the MSE, less so for the 1-step forecast though the difference is negligible with two more epochs and roughly 6 minutes longer for the training and validation phase. In contrast to the results from the linear models, the average epoch duration is not the same for the 1-step and 6-step forecast and increases significantly for the longer forecast horizons This can be attributed next to the larger matrices to the more complex calculations inside the model in comparison to the linear model with its single linear layer. Though the Test Loss for the different models stays the same. Hence here again there is no preference for the forecast horizon in terms of the maginitude of the error. Therefore the Loss function of L1 is advised to be taken for the Vanilla Transformer with an embedding length of 64 across the different forecast horizons.

For the Informer Model the embedding length of $32$ results in the lowest error scores for the MSE and L1 Loss, therefore this is the hyperparameter of choice. Though the average epoch duration for the MSE Loss is here the highest across the different embedding lengths, the average epoch duration for the L1 Loss on the other hand is the lowest across all experiments. \par



The Linear Model has no mebddings, hence one saves computational capacity and time in that regard as well as the lower average epoch times across all experiments and parameters. For the 1-step ahead forecast the MSE Loss is the fastest and resource efficient one, but for longer step ahead forecasts the L1 Loss has the least epochs to train with the lowest average epoch duration. With a patience of 7 for the early stopping this mean that for the L1 Loss the local minimum in the training for the forecast horizon of 72 and 144 is reached after just three epochs. Which allows for fast training processes in retraining.
Hence the L1 Loss is the resource efficient loss function to train with. Though as expected the training time increases with longer forecasts, the average epoch duration for 1-step and 6-step forecasts are very similar and only for the 72-steps and 144-steps the epoch duration increases significantly. This can be attributed to the increased sequence length during the training process as the weight matrices become larger. The Test Loss in contrast to the training time remains constant across the different forecast horizons, this is not expected as stated earlier. The error was expected to increase with farther forecasts as the variance from each step within the recurrent forecast approach is carried to the next, and therefor it was expected to increase. Though seemingly this is not the case with this model, therefor the forecast horizon as investigated in these experiments plays no role in the determination of an accurate model, as they are all equally accurate. 

Further research:
Is the training time and the forecast horizon a logarithmic relationship and if it, where is the opimal point on the curve for the fastest convergence to reliable forecast.
