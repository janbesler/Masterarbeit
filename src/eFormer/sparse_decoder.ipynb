{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# machine learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, encoder_output_dim, forecast_horizon=22, max_len=5000, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(SparseDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        d_ff = d_ff or 4*d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "        # Initialize PositionalEncoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # Sparse Attention Module for cross attention\n",
    "        self.cross_attention = DetSparseAttentionModule(d_model, n_heads, prob_sparse_factor=5)\n",
    "\n",
    "        # Feed-forward network components\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        \n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, encoder_output, attn_mask=None):\n",
    "        # Generate positional encodings\n",
    "        dummy_input = torch.zeros(self.forecast_horizon, self.d_model).unsqueeze(0)\n",
    "        pos_encodings = self.pos_encoder(dummy_input)\n",
    "\n",
    "        # Apply encoder-decoder attention using positional encodings as queries and encoder outputs as keys and values\n",
    "        attn_output = self.cross_attention(pos_encodings, encoder_output, encoder_output, attn_mask)\n",
    "        attn_output = self.norm1(attn_output + self.dropout(attn_output))\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_output = attn_output.transpose(-1, 1)  # Prepare for conv1d\n",
    "        ff_output = self.dropout(self.activation(self.conv1(ff_output)))\n",
    "        ff_output = self.dropout(self.conv2(ff_output))\n",
    "        ff_output = ff_output.transpose(-1, 1)  # Back to original dims\n",
    "        ff_output = self.norm2(attn_output + self.dropout(ff_output))\n",
    "\n",
    "        # Generate forecasts based on the attention output\n",
    "        forecasts = self.output_layer(ff_output).squeeze(-1)\n",
    "        \n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetSparseDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(DetSparseAttentionModule, self).__init__()\n",
    "        self.attention_layer = AttentionLayer(\n",
    "            ProbAttention(mask_flag=False, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        # calculate attention\n",
    "        attention_output, _ = self.attention_layer(queries, keys, values, attn_mask)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbSparseAttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(ProbSparseAttentionModule, self).__init__()\n",
    "        # Attention layers for both means and variances\n",
    "        self.attention_layer_means = AttentionLayer(\n",
    "            ProbAttention(mask_flag=False, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "        self.attention_layer_vars = AttentionLayer(\n",
    "            ProbAttention(mask_flag=False, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        # Split the input tensors to extract means and variances\n",
    "        queries_means, queries_vars = queries[0], queries[1]\n",
    "        keys_means, keys_vars = keys[0], keys[1]\n",
    "        values_means, values_vars = values[0], values[1]\n",
    "\n",
    "        # Process means through the attention layer for means\n",
    "        attention_output_means, _ = self.attention_layer_means(queries_means, keys_means, values_means, attn_mask)\n",
    "        \n",
    "        # Process variances through the attention layer for variances\n",
    "        attention_output_vars, _ = self.attention_layer_vars(queries_vars, keys_vars, values_vars, attn_mask)\n",
    "\n",
    "        # Combine the processed means and variances\n",
    "        combined_output = torch.stack([attention_output_means, attention_output_vars], dim=0)\n",
    "\n",
    "        return combined_output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
