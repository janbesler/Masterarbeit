{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Attention\n",
    "\n",
    "Credit to [Autoformer](https://github.com/thuml/Autoformer/blob/main/layers/AutoCorrelation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class SeasonalDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # print(f\"tmp_corr non-zero:\", len(tmp_corr[tmp_corr != 0]))\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[:,:,i,:].unsqueeze(1))\n",
    "            \n",
    "        # print(f\"nonzero output: {len(delays_agg[delays_agg!=0])}\")\n",
    "        \n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        #print(f\"out shape: {self.out_projection(out).shape}\")\n",
    "        #print(f\"attn shape: {attn.shape}\")\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, auto_corr, d_model, n_heads):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        # initialize AutoCorr\n",
    "        self.auto_corr = auto_corr \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        # Use the provided AutoCorrelation instance\n",
    "        return self.auto_corr(queries, keys, values, attn_mask)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=2, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "\n",
    "        # Create the AutoCorrelationLayer with the AutoCorrelation instance\n",
    "        self.n_heads = n_heads\n",
    "        self.auto_corr = AutoCorrelation(top_k=top_k)\n",
    "        self.auto_corr_layer = AutoCorrelationLayer(self.auto_corr, d_model, n_heads)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, prob_embeddings):\n",
    "        H = self.n_heads\n",
    "        E = input_dim // H # size per head\n",
    "\n",
    "        # Ensure that E is an even number\n",
    "        if input_dim % H != 0:\n",
    "            raise ValueError(\"Half the feature dimension is not divisible by the number of heads.\")\n",
    "\n",
    "        # Split tensor into means and variances\n",
    "        means, variances = prob_embeddings.split(input_dim, dim=-1)\n",
    "\n",
    "        # Reshape both halves for multi-head format: [B, L, H, E]\n",
    "        reshaped_means = means.view(B, L, H, E)\n",
    "        reshaped_variances = variances.view(B, L, H, E)\n",
    "\n",
    "        # Concatenate the reshaped means and variances\n",
    "        reshaped_embeddings = torch.cat([reshaped_means, reshaped_variances], dim=-1)\n",
    "\n",
    "        return self.auto_corr_layer(reshaped_embeddings, reshaped_embeddings, reshaped_embeddings, None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(AutoCorrEncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class AutoCorrEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code from eFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class SeasonalDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model, mask_flag=False, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # print(f\"tmp_corr non-zero:\", len(tmp_corr[tmp_corr != 0]))\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[:,:,i,:].unsqueeze(1))\n",
    "            \n",
    "        # print(f\"nonzero output: {len(delays_agg[delays_agg!=0])}\")\n",
    "        \n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        print(\"AR\")\n",
    "        print(f\"queries shape: {queries.shape}\")\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "        \n",
    "        print(f\"corr shape: {corr.shape}\")\n",
    "        print(f\"values shape: {values.shape}\")\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "        \"\"\"\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrEncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(AutoCorrEncoderLayer, self).__init__()\n",
    "        self.attention = attention  # Use an instance of AutoCorrelation\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "\n",
    "        # Projection layers for queries, keys, and values\n",
    "        self.query_projection = nn.Linear(d_model, self.d_k * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, self.d_k * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, self.d_v * n_heads)\n",
    "\n",
    "        # Output projection layer\n",
    "        self.out_projection = nn.Linear(self.d_v * n_heads, d_model)\n",
    "\n",
    "        # Optional feed forward layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff or 4 * d_model, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff or 4 * d_model, out_channels=d_model, kernel_size=1, bias=False)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Projection\n",
    "        queries = self.query_projection(x).view(-1, x.size(1), self.n_heads, self.d_k).transpose(1,2)\n",
    "        keys = self.key_projection(x).view(-1, x.size(1), self.n_heads, self.d_k).transpose(1,2)\n",
    "        values = self.value_projection(x).view(-1, x.size(1), self.n_heads, self.d_v).transpose(1,2)\n",
    "\n",
    "        # Apply AutoCorrelation Attention\n",
    "        # Note: Adjust AutoCorrelation to handle multi-head input properly if needed\n",
    "        attn_output, _ = self.attention(queries, keys, values, attn_mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(-1, x.size(1), self.d_model)\n",
    "        x = x + self.dropout(self.out_projection(attn_output))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Optional Feed Forward\n",
    "        y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        y = self.norm2(x + y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class AutoCorrEncoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(layer.attention.d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        print(\"Encoder\")\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=4, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.auto_corr = AutoCorrelation(\n",
    "            mask_flag=False,\n",
    "            factor=1,\n",
    "            scale=None,\n",
    "            attention_dropout=0.1,\n",
    "            output_attention=False,\n",
    "            top_k=top_k,\n",
    "            d_model=d_model)\n",
    "        self.encoder_layer = AutoCorrEncoderLayer(\n",
    "            attention=self.auto_corr,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads)\n",
    "        self.encoder = AutoCorrEncoder(layer=self.encoder_layer, N=1)  # Adjust N for more layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"Model\")\n",
    "        return self.encoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "def AutoCorrelationFunction(input):\n",
    "    B, L, _ = input.shape\n",
    "    input_dim = input.shape[-1]\n",
    "\n",
    "    model = MyAutoCorrelationModel(\n",
    "        d_model=input_dim,\n",
    "        n_heads=n_heads_global,\n",
    "        B=B,\n",
    "        L=L,\n",
    "        input_dim=input_dim,\n",
    "        top_k=4,  # Adjust as needed\n",
    "        device=input.device\n",
    "    )\n",
    "\n",
    "    return model(input)\n",
    "\n",
    "# initialize layer\n",
    "series_decomp_layer = SeriesDecomp(kernel_size=7) \n",
    "\n",
    "# apply decomposition\n",
    "seasonal, trend = series_decomp_layer(embeddings)\n",
    "\n",
    "# Assume output_sparse is a properly formatted input tensor\n",
    "output_ar_prob = AutoCorrelationFunction(embeddings)\n",
    "\n",
    "# investigate number of dead nodes\n",
    "print(len(output_ar_prob[output_ar_prob == 0]))\n",
    "print(len(output_ar_prob[output_ar_prob != 0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
