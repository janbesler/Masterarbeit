{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from masking import TriangularCausalMask, ProbMask\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "probabilistic embedding & positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = Kelmarsh_df['1'][['# Date and time', 'Energy Export (kWh)']][-1024:]\n",
    "\n",
    "# First, ensure that the column is in datetime format\n",
    "test_df['# Date and time'] = pd.to_datetime(test_df['# Date and time'])\n",
    "\n",
    "# Then convert it to timestamps\n",
    "test_df['Timestamp'] = test_df['# Date and time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# interpolate NaN values\n",
    "test_df = test_df.interpolate(method='linear')\n",
    "\n",
    "features_matrix = test_df[['Energy Export (kWh)', 'Timestamp']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "def t2v(\n",
    "    tau, # input tensor\n",
    "    f, # activation function (sin or cosin)\n",
    "    out_features, # size of output vector\n",
    "    w, # weights\n",
    "    b, # biases\n",
    "    w0, # weights for linear part of time2vec layer\n",
    "    b0, # biases for linear part of time2vec layer\n",
    "    arg=None # optional arguments\n",
    "    ):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sineact = SineActivation(2, 64)\n",
    "    cosact = CosineActivation(2, 64)\n",
    "\n",
    "feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "# check for NaN values early\n",
    "if torch.isnan(feature_tensor).any():\n",
    "    raise ValueError('NaN values detected in Input')\n",
    "    \n",
    "t2v_tensor = sineact(feature_tensor)\n",
    "\n",
    "print(t2v_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "def t2v(\n",
    "    tau, # input tensor\n",
    "    f, # activation function (sin or cosin)\n",
    "    out_features, # size of output vector\n",
    "    w, # weights\n",
    "    b, # biases\n",
    "    w0, # weights for linear part of time2vec layer\n",
    "    b0, # biases for linear part of time2vec layer\n",
    "    arg=None # optional arguments\n",
    "    ):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        #print(w.shape, t1.shape, b.shape)\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    #print(v1.shape)\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class ProbabilisticSineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProbabilisticSineActivation, self).__init__()\n",
    "        self.out_features = out_features // 2  # Half for mean, half for variance\n",
    "        self.w0 = nn.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.Parameter(torch.randn(1))\n",
    "        self.w = nn.Parameter(torch.randn(in_features, self.out_features - 1))\n",
    "        self.b = nn.Parameter(torch.randn(self.out_features - 1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        # Calculate mean\n",
    "        mean = t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "        \n",
    "        # Calculate variance (use another set of weights and biases, ensure positive variance)\n",
    "        variance = F.softplus(t2v(tau, self.f, self.out_features, \n",
    "                                  torch.randn_like(self.w), torch.randn_like(self.b), \n",
    "                                  torch.randn_like(self.w0), torch.randn_like(self.b0)))\n",
    "        \n",
    "        return torch.cat([mean, variance], -1)\n",
    "\n",
    "class ProbabilisticCosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProbabilisticCosineActivation, self).__init__()\n",
    "        self.out_features = out_features // 2  # Half for mean, half for variance\n",
    "        self.w0 = nn.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.Parameter(torch.randn(1))\n",
    "        self.w = nn.Parameter(torch.randn(in_features, self.out_features - 1))\n",
    "        self.b = nn.Parameter(torch.randn(self.out_features - 1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        # Calculate mean\n",
    "        mean = t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "        \n",
    "        # Calculate variance (use another set of weights and biases, ensure positive variance)\n",
    "        variance = F.softplus(t2v(tau, self.f, self.out_features, \n",
    "                                  torch.randn_like(self.w), torch.randn_like(self.b), \n",
    "                                  torch.randn_like(self.w0), torch.randn_like(self.b0)))\n",
    "        \n",
    "        return torch.cat([mean, variance], -1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prob_sineact = ProbabilisticSineActivation(2, 64)\n",
    "    prob_cosact = ProbabilisticCosineActivation(2,64)\n",
    "\n",
    "    feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "    # check for NaN values early\n",
    "    if torch.isnan(feature_tensor).any():\n",
    "        raise ValueError('NaN values detected in Input Tensor')\n",
    "\n",
    "    prob_embeddings = prob_sineact(feature_tensor)\n",
    "\n",
    "    # check for NaN values early\n",
    "    if torch.isnan(prob_embeddings).any():\n",
    "        raise ValueError('NaN values detected in probabilistic Embeddings')\n",
    "\n",
    "    # Ensure prob_embeddings has the correct shape [B, L, E]\n",
    "    if len(prob_embeddings.shape) == 2:\n",
    "        prob_embeddings = prob_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    elif len(prob_embeddings.shape) == 1:\n",
    "        prob_embeddings = prob_embeddings.unsqueeze(0).unsqueeze(0)  # Add batch and length dimensions\n",
    "\n",
    "    # prob_embeddings now contains both mean and variance for each feature\n",
    "    print(prob_embeddings.shape)  # Should be [batch_size, 64] (32 for mean, 32 for variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparse Attention\n",
    "\n",
    "Credit to [Informer](https://github.com/zhouhaoyi/Informer2020/blob/0ac81e04d4095ecb97a3a78c7b49c936d8aa9933/models/attn.py#L38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1./sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), A)\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :] # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # V_sum = V.sum(dim=-2)\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else: # use mask\n",
    "            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            contex = V.cumsum(dim=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return (context_in, attns)\n",
    "        else:\n",
    "            return (context_in, None)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2,1)\n",
    "        keys = keys.transpose(2,1)\n",
    "        values = values.transpose(2,1)\n",
    "\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item() # c*ln(L_k)\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item() # c*ln(L_q) \n",
    "\n",
    "        U_part = U_part if U_part<L_K else L_K\n",
    "        u = u if u<L_Q else L_Q\n",
    "        \n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u) \n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        # update the context with selected top_k queries\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
    "        \n",
    "        return context.transpose(2,1).contiguous(), attn\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, \n",
    "                 d_keys=None, d_values=None, mix=False):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        self.mix = mix\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        if self.mix:\n",
    "            out = out.transpose(2,1).contiguous()\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(SparseAttentionModule, self).__init__()\n",
    "        self.attention_layer = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Assuming embeddings are concatenated means and variances\n",
    "        # Split embeddings into mean and variance\n",
    "        means, variances = embeddings.split(embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "        # Use means for attention calculation\n",
    "        attention_output, _ = self.attention_layer(means, means, means, None)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "# Example usage\n",
    "model = SparseAttentionModule(\n",
    "    d_model=(prob_embeddings.shape[-1] // 2),\n",
    "    n_heads=3,\n",
    "    prob_sparse_factor=5\n",
    "    )\n",
    "\n",
    "# Ensure prob_embeddings has the correct shape [B, L, E]\n",
    "if len(prob_embeddings.shape) == 2:\n",
    "    prob_embeddings = prob_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "elif len(prob_embeddings.shape) == 1:\n",
    "    prob_embeddings = prob_embeddings.unsqueeze(0).unsqueeze(0)  # Add batch and length dimensions\n",
    "\n",
    "output = model(prob_embeddings)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing mean and variance:\n",
    "\n",
    "- Separate Attention Layers: The model now has separate attention layers for processing means and variances. This allows each component to be updated based on its own dynamics.\n",
    "- Processing Means and Variances: Both components are processed through their respective attention layers.\n",
    "- Combining Outputs: The outputs (updated means and variances) are then concatenated to form the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(SparseAttentionModule, self).__init__()\n",
    "        # Attention layers for both means and variances\n",
    "        self.attention_layer_means = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "        self.attention_layer_vars = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Split embeddings into mean and variance\n",
    "        means, variances = embeddings.split(embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "        # Process means and variances separately through attention layers\n",
    "        attention_output_means, _ = self.attention_layer_means(means, means, means, None)\n",
    "        attention_output_vars, _ = self.attention_layer_vars(variances, variances, variances, None)\n",
    "\n",
    "        # Combine the results\n",
    "        combined_output = torch.cat([attention_output_means, attention_output_vars], dim=-1)\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "# Example usage\n",
    "model = SparseAttentionModule(\n",
    "    d_model=(prob_embeddings.shape[-1] // 2),\n",
    "    n_heads=3,\n",
    "    prob_sparse_factor=5\n",
    "    )\n",
    "output_mean_var = model(prob_embeddings)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output_mean_var).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 32])\n",
      "torch.Size([1, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(output_mean_var.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Autocorrelation Attention\n",
    "\n",
    "Credit to [Autoformer](https://github.com/thuml/Autoformer/blob/main/layers/AutoCorrelation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        self._mask = torch.triu(torch.ones((L, L), dtype=torch.bool), diagonal=1).to(device)\n",
    "        self._mask = self._mask.unsqueeze(0).unsqueeze(1)  # Shape: [1, 1, L, L]\n",
    "        self._mask = self._mask.repeat(B, 1, 1, 1)  # Repeat for batch and head dimensions\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize layer\n",
    "series_decomp_layer = SeriesDecomp(kernel_size=7) \n",
    "\n",
    "# split mean and variance\n",
    "means, variances = prob_embeddings.split(prob_embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "# apply decomposition\n",
    "seasonal_means, trend_means = series_decomp_layer(means)\n",
    "seasonal = torch.cat([seasonal_means, variances], dim=-1)\n",
    "trend = torch.cat([trend_means, variances], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scoring Network: The ScoringNetwork is a simple linear layer followed by a sigmoid activation. It outputs scores for each element in the input sequence.\n",
    "- ProbMask Class: This class now includes the scoring network. It takes an input sequence, computes scores, selects the top-k scores to generate indices, and then creates the probabilistic mask based on these indices.\n",
    "- Forward Method: The forward method of ProbMask handles the entire process of scoring, index selection, and mask generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after masking: tensor([False, False, False, False, False, False,  True,  True,  True,  True])\n",
      "corr after masking: tensor([526.7306,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000], grad_fn=<SliceBackward0>)\n",
      "corr Sample: tensor([526.7306,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        print(f\"corr Sample:\", corr[0,0,0, :10])\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            print(f\"after masking:\", mask[0,0,5, :10])\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "\n",
    "        print(f\"corr after masking:\", corr[0,0,0, :10])\n",
    "\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, auto_corr, d_model, n_heads):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        # initialize AutoCorr\n",
    "        self.auto_corr = auto_corr \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        # Use the provided AutoCorrelation instance\n",
    "        return self.auto_corr(queries, keys, values, attn_mask)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=2, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "\n",
    "        # Create the AutoCorrelationLayer with the AutoCorrelation instance\n",
    "        self.n_heads = n_heads\n",
    "        self.auto_corr = AutoCorrelation(top_k=top_k)\n",
    "        self.auto_corr_layer = AutoCorrelationLayer(self.auto_corr, d_model, n_heads)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, prob_embeddings):\n",
    "        H = self.n_heads\n",
    "        E = input_dim // H # size per head\n",
    "\n",
    "        # Ensure that E is an even number\n",
    "        if input_dim % H != 0:\n",
    "            raise ValueError(\"Half the feature dimension is not divisible by the number of heads.\")\n",
    "\n",
    "        # Split tensor into means and variances\n",
    "        means, variances = prob_embeddings.split(input_dim, dim=-1)\n",
    "\n",
    "        # Reshape both halves for multi-head format: [B, L, H, E]\n",
    "        reshaped_means = means.view(B, L, H, E)\n",
    "        reshaped_variances = variances.view(B, L, H, E)\n",
    "\n",
    "        # Concatenate the reshaped means and variances\n",
    "        reshaped_embeddings = torch.cat([reshaped_means, reshaped_variances], dim=-1)\n",
    "\n",
    "        return self.auto_corr_layer(reshaped_embeddings, reshaped_embeddings, reshaped_embeddings, None)[0]\n",
    "\n",
    "# Example usage\n",
    "B, L, _ = prob_embeddings.shape\n",
    "input_dim = prob_embeddings.shape[-1] // 2  # Assuming this matches your input dimension\n",
    "\n",
    "model = MyAutoCorrelationModel(\n",
    "    d_model=input_dim,\n",
    "    n_heads=4,\n",
    "    B=B,\n",
    "    L=L,\n",
    "    input_dim=input_dim,\n",
    "    top_k=2,  # Adjust as needed\n",
    "    device=prob_embeddings.device\n",
    ")\n",
    "\n",
    "output = model(prob_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean CRPS: 1.3489\n"
     ]
    }
   ],
   "source": [
    "# CRPS (continouos ranked probability score)\n",
    "def crps(forecast, observations, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    forecast (pd.DataFrame or np.ndarray): Forecasts from the model (ensemble).\n",
    "    observations (pd.Series or np.ndarray): Observed values.\n",
    "    weights (np.array): Corresponding weights for the CRPS scores, derived from sparse attention.\n",
    "\n",
    "    Returns:\n",
    "    float: Weighted mean of the CRPS for all forecasts.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays if input is Pandas\n",
    "    if isinstance(forecast, pd.DataFrame):\n",
    "        forecast = forecast.to_numpy()\n",
    "    if isinstance(observations, pd.Series):\n",
    "        observations = observations.to_numpy()\n",
    "    \n",
    "    # Sort forecast samples\n",
    "    forecast.sort(axis=0)\n",
    "\n",
    "    # Ensure observations are broadcastable over the forecast_samples\n",
    "    observations = observations[np.newaxis, :]\n",
    "\n",
    "    # Calculate CRPS\n",
    "    cumsum_forecast = np.cumsum(forecast, axis=0) / forecast.shape[0]\n",
    "    crps = np.mean((cumsum_forecast - (forecast > observations).astype(float)) ** 2, axis=0)\n",
    "    \n",
    "    # weighted median of CRPS\n",
    "    if len(crps) != len(weights):\n",
    "        raise ValueError(\"Length of CRPS series and weights must be equal\")\n",
    "\n",
    "    weighted_sum = np.sum(crps * weights)\n",
    "    total_weights = np.sum(weights)\n",
    "\n",
    "    if total_weights == 0:\n",
    "        raise ValueError(\"Total weight cannot be zero\")\n",
    "\n",
    "    weighted_crps = weighted_sum / total_weights\n",
    "    \n",
    "    return round(weighted_crps, 4)\n",
    "\n",
    "# Example usage\n",
    "forecast_samples = pd.DataFrame(np.random.randn(1000, 5))  # Example forecast samples\n",
    "observations = pd.Series(np.random.randn(5))  # Example observations\n",
    "weights = np.random.rand(5)  # Example weights\n",
    "\n",
    "weighted_crps = crps(forecast_samples, observations, weights)\n",
    "print(\"Weighted Mean CRPS:\", weighted_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vanilla Embedding\n",
    "class VanillaTimeEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "# Probabilistic Time Embedding\n",
    "class ProbabilisticTimeEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.mu = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.log_sigma = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.mu(x)\n",
    "        log_sigma = self.log_sigma(x)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return mu + sigma * torch.randn_like(sigma)\n",
    "\n",
    "# Generate some random indices to simulate time steps\n",
    "time_indices = torch.tensor(np.random.randint(0, 50, size=50))\n",
    "\n",
    "# Initialize models\n",
    "vanilla_model = VanillaTimeEmbedding(50, 2)\n",
    "probabilistic_model = ProbabilisticTimeEmbedding(50, 2)\n",
    "\n",
    "# Get embeddings\n",
    "vanilla_embeddings = vanilla_model(time_indices).detach().numpy()\n",
    "probabilistic_embeddings = probabilistic_model(time_indices).detach().numpy()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(vanilla_embeddings[:, 0], vanilla_embeddings[:, 1], c='blue')\n",
    "plt.title('Vanilla Time Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(probabilistic_embeddings[:, 0], probabilistic_embeddings[:, 1], c='red')\n",
    "plt.title('Probabilistic Time Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoCorr Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seasonal_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with seasonal decomposition and masking.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_lag=1, mask_type='triangular', factor=1, scale=None, attention_dropout=0.1, output_attention=False, moving_avg_kernel=25, device=\"cpu\"):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_type = mask_type\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.seasonal_decomp = SeriesDecomp(moving_avg_kernel)\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "        self.max_lag = max_lag\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "\n",
    "        # Apply seasonal decomposition\n",
    "        seasonal, trend = self.seasonal_decomp(values)\n",
    "\n",
    "        # Choose masking approach\n",
    "        if self.mask_type == 'triangular':\n",
    "            mask = TriangularCausalMask(B, L, device=self.device).mask\n",
    "        elif self.mask_type == 'prob':\n",
    "            # ProbMask requires additional inputs like scores and index\n",
    "            # Assuming scores and index are provided in some way\n",
    "            scores = ... # Define or obtain scores\n",
    "            index = ...  # Define or obtain index\n",
    "            mask = ProbMask(B, self.d_model, L, index, scores, device=self.device).mask\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mask type\")\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if attn_mask is not None:\n",
    "            mask = mask & attn_mask\n",
    "\n",
    "        # Compute time-lagged correlations\n",
    "        lagged_correlation = torch.zeros_like(queries)\n",
    "        for lag in range(1, self.max_lag + 1):\n",
    "            lagged_correlation += torch.roll(queries, shifts=lag, dims=-1) * keys\n",
    "\n",
    "        # Normalize the correlation\n",
    "        lagged_correlation /= self.max_lag\n",
    "\n",
    "        # Processed values\n",
    "        processed_values = self.dropout(lagged_correlation)\n",
    "\n",
    "        # Return the processed values and optionally the correlation coefficients\n",
    "        if self.output_attention:\n",
    "            return processed_values, lagged_correlation\n",
    "        else:\n",
    "            return processed_values, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# probabilistic masking\n",
    "class ProbMask(nn.Module):\n",
    "    def __init__(self, B, H, L, input_dim, top_k=5, device=\"cpu\"):\n",
    "        super(ProbMask, self).__init__()\n",
    "        self.B = B\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.scoring_network = ScoringNetwork(input_dim, L).to(device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # Compute scores using the scoring network\n",
    "        scores = self.scoring_network(input_sequence)  # input_sequence shape: [B, L, input_dim]\n",
    "\n",
    "        # Select top-k scores to generate indices\n",
    "        _, indices = torch.topk(scores, self.top_k, dim=-1)  # Selecting indices of top-k scores\n",
    "\n",
    "        # Create a mask for all positions\n",
    "        full_mask = torch.ones((self.B, self.L), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Update mask for top-k positions\n",
    "        batch_indices = torch.arange(self.B, device=self.device)[:, None]\n",
    "        full_mask[batch_indices, indices] = False\n",
    "\n",
    "        # Expand mask for all heads\n",
    "        mask = full_mask[:, None, :].expand(-1, self.H, -1)\n",
    "\n",
    "        # Mask should be of shape [B, H, L, L]\n",
    "        mask = mask.unsqueeze(2).expand(-1, -1, self.L, -1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "class ScoringNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ScoringNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Using sigmoid to keep scores between 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
