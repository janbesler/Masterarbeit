{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import SineActivation, CosineActivation\n",
    "from eFormer.sparse_prob import SparseAttentionModule_Prob\n",
    "from eFormer.sparse_det import SparseAttentionModule_Det\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "\n",
    "n_heads_global = 4\n",
    "probabilistic_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "probabilistic embedding & positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'features_matrix' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "test_df = Kelmarsh_df['1'][['# Date and time', 'Energy Export (kWh)']][-1024:]\n",
    "\n",
    "# First, ensure that the column is in datetime format\n",
    "test_df['# Date and time'] = pd.to_datetime(test_df['# Date and time'])\n",
    "\n",
    "# Then convert it to timestamps\n",
    "test_df['Timestamp'] = test_df['# Date and time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# interpolate NaN values\n",
    "test_df = test_df.interpolate(method='linear')\n",
    "\n",
    "features_matrix = test_df[['Energy Export (kWh)', 'Timestamp']].values\n",
    "\n",
    "%store features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the tensor -> 2 for true values and time stamps\n",
    "in_features = features_matrix.shape[-1]\n",
    "# length of embedding vector\n",
    "out_features = 64\n",
    "\n",
    "encoding_model = Encoding(in_features, out_features)\n",
    "\n",
    "# Forward pass through the model\n",
    "feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(feature_tensor).any():\n",
    "    raise ValueError('NaN values detected in Input')\n",
    "\n",
    "det_embeddings = encoding_model(feature_tensor)\n",
    "\n",
    "# Check for NaN values after computation\n",
    "if torch.isnan(det_embeddings).any():\n",
    "    raise ValueError('NaN values detected in Embeddings')\n",
    "\n",
    "print(det_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "In the case of probabilistic modeling the embeddings are modified by alearnable random noise drawn from a gaussian distribution based on the embeddings itself. Therefor the two outputs have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which model to use\n",
    "if probabilistic_model == True:\n",
    "    model = SparseAttentionModule_Prob(\n",
    "        d_model=embeddings.shape[-1],\n",
    "        n_heads=n_heads_global,\n",
    "        prob_sparse_factor=5\n",
    "        )\n",
    "else:\n",
    "    model = SparseAttentionModule_Det(\n",
    "        d_model=embeddings.shape[-1],\n",
    "        n_heads=n_heads_global,\n",
    "        prob_sparse_factor=5\n",
    "    )\n",
    "\n",
    "output_sparse = model(embeddings)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output_sparse).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionModule(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(SparseAttentionModule, self).__init__()\n",
    "        # Attention layers for both means and variances\n",
    "        self.attention_layer_means = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "        self.attention_layer_vars = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Split embeddings into mean and variance\n",
    "        means, variances = embeddings.split(embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "        # Process means and variances separately through attention layers\n",
    "        attention_output_means, _ = self.attention_layer_means(means, means, means, None)\n",
    "        attention_output_vars, _ = self.attention_layer_vars(variances, variances, variances, None)\n",
    "\n",
    "        # Combine the results\n",
    "        combined_output = torch.cat([attention_output_means, attention_output_vars], dim=-1)\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "# Example usage\n",
    "model = SparseAttentionModule(\n",
    "    d_model=(prob_embeddings.shape[-1] // 2),\n",
    "    n_heads=3,\n",
    "    prob_sparse_factor=5\n",
    "    )\n",
    "\n",
    "# Ensure prob_embeddings has the correct shape [B, L, E]\n",
    "if len(prob_embeddings.shape) == 2:\n",
    "    prob_embeddings = prob_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "elif len(prob_embeddings.shape) == 1:\n",
    "    prob_embeddings = prob_embeddings.unsqueeze(0).unsqueeze(0)  # Add batch and length dimensions\n",
    "\n",
    "output_mean_var = model(prob_embeddings)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Attention\n",
    "\n",
    "Credit to [Autoformer](https://github.com/thuml/Autoformer/blob/main/layers/AutoCorrelation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class SeasonalDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize layer\n",
    "series_decomp_layer = SeriesDecomp(kernel_size=7) \n",
    "\n",
    "# apply decomposition\n",
    "seasonal, trend = series_decomp_layer(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self,d_model, mask_flag=False, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # print(f\"tmp_corr non-zero:\", len(tmp_corr[tmp_corr != 0]))\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[:,:,i,:].unsqueeze(1))\n",
    "            \n",
    "        # print(f\"nonzero output: {len(delays_agg[delays_agg!=0])}\")\n",
    "        \n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        print(\"AR\")\n",
    "        print(f\"queries shape: {queries.shape}\")\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "        \n",
    "        print(f\"corr shape: {corr.shape}\")\n",
    "        print(f\"values shape: {values.shape}\")\n",
    "        \n",
    "        \"\"\"\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "        \"\"\"\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrEncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(AutoCorrEncoderLayer, self).__init__()\n",
    "        self.attention = attention  # Use an instance of AutoCorrelation\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "\n",
    "        # Projection layers for queries, keys, and values\n",
    "        self.query_projection = nn.Linear(d_model, self.d_k * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, self.d_k * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, self.d_v * n_heads)\n",
    "\n",
    "        # Output projection layer\n",
    "        self.out_projection = nn.Linear(self.d_v * n_heads, d_model)\n",
    "\n",
    "        # Optional feed forward layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff or 4 * d_model, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff or 4 * d_model, out_channels=d_model, kernel_size=1, bias=False)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Projection\n",
    "        queries = self.query_projection(x).view(-1, x.size(1), self.n_heads, self.d_k).transpose(1,2)\n",
    "        keys = self.key_projection(x).view(-1, x.size(1), self.n_heads, self.d_k).transpose(1,2)\n",
    "        values = self.value_projection(x).view(-1, x.size(1), self.n_heads, self.d_v).transpose(1,2)\n",
    "\n",
    "        # Apply AutoCorrelation Attention\n",
    "        # Note: Adjust AutoCorrelation to handle multi-head input properly if needed\n",
    "        attn_output, _ = self.attention(queries, keys, values, attn_mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(-1, x.size(1), self.d_model)\n",
    "        x = x + self.dropout(self.out_projection(attn_output))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Optional Feed Forward\n",
    "        y = self.dropout(self.activation(self.conv1(x.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        y = self.norm2(x + y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class AutoCorrEncoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(layer.attention.d_model)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        print(\"Encoder\")\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=4, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.auto_corr = AutoCorrelation(\n",
    "            mask_flag=False,\n",
    "            factor=1,\n",
    "            scale=None,\n",
    "            attention_dropout=0.1,\n",
    "            output_attention=False,\n",
    "            top_k=top_k,\n",
    "            d_model=d_model)\n",
    "        self.encoder_layer = AutoCorrEncoderLayer(\n",
    "            attention=self.auto_corr,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads)\n",
    "        self.encoder = AutoCorrEncoder(layer=self.encoder_layer, N=1)  # Adjust N for more layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(\"Model\")\n",
    "        return self.encoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "Encoder\n",
      "AR\n",
      "queries shape: torch.Size([1, 4, 1024, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1024, 4, 4]' is invalid for input of size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Assume output_sparse is a properly formatted input tensor\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m output_ar_prob \u001b[39m=\u001b[39m AutoCorrelationFunction(embeddings)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# investigate number of dead nodes\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(output_ar_prob[output_ar_prob \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]))\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m MyAutoCorrelationModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     d_model\u001b[39m=\u001b[39minput_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_heads\u001b[39m=\u001b[39mn_heads_global,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEncoder\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     x \u001b[39m=\u001b[39m layer(x, attn_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_projection(x)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_v)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# Apply AutoCorrelation Attention\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39m# Note: Adjust AutoCorrelation to handle multi-head input properly if needed\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m attn_output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(queries, keys, values, attn_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_projection(attn_output))\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m expand_multiplier \u001b[39m=\u001b[39m L \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m corr\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m corr \u001b[39m=\u001b[39m corr\u001b[39m.\u001b[39mexpand(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, expand_multiplier, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m corr \u001b[39m=\u001b[39m corr\u001b[39m.\u001b[39;49mreshape(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     corr\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     corr\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     L,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     corr\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcorr shape: \u001b[39m\u001b[39m{\u001b[39;00mcorr\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X42sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues shape: \u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1024, 4, 4]' is invalid for input of size 0"
     ]
    }
   ],
   "source": [
    "def AutoCorrelationFunction(input):\n",
    "    B, L, _ = input.shape\n",
    "    input_dim = input.shape[-1]\n",
    "\n",
    "    model = MyAutoCorrelationModel(\n",
    "        d_model=input_dim,\n",
    "        n_heads=n_heads_global,\n",
    "        B=B,\n",
    "        L=L,\n",
    "        input_dim=input_dim,\n",
    "        top_k=4,  # Adjust as needed\n",
    "        device=input.device\n",
    "    )\n",
    "\n",
    "    return model(input)\n",
    "\n",
    "# Assume output_sparse is a properly formatted input tensor\n",
    "output_ar_prob = AutoCorrelationFunction(embeddings)\n",
    "\n",
    "# investigate number of dead nodes\n",
    "print(len(output_ar_prob[output_ar_prob == 0]))\n",
    "print(len(output_ar_prob[output_ar_prob != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean CRPS: 1.0589\n"
     ]
    }
   ],
   "source": [
    "# CRPS (continouos ranked probability score)\n",
    "def crps(forecast, observations, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    forecast (pd.DataFrame or np.ndarray): Forecasts from the model (ensemble).\n",
    "    observations (pd.Series or np.ndarray): Observed values.\n",
    "    weights (np.array): Corresponding weights for the CRPS scores, derived from sparse attention.\n",
    "\n",
    "    Returns:\n",
    "    float: Weighted mean of the CRPS for all forecasts.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays if input is Pandas\n",
    "    if isinstance(forecast, pd.DataFrame):\n",
    "        forecast = forecast.to_numpy()\n",
    "    if isinstance(observations, pd.Series):\n",
    "        observations = observations.to_numpy()\n",
    "    \n",
    "    # Sort forecast samples\n",
    "    forecast.sort(axis=0)\n",
    "\n",
    "    # Ensure observations are broadcastable over the forecast_samples\n",
    "    observations = observations[np.newaxis, :]\n",
    "\n",
    "    # Calculate CRPS\n",
    "    cumsum_forecast = np.cumsum(forecast, axis=0) / forecast.shape[0]\n",
    "    crps = np.mean((cumsum_forecast - (forecast > observations).astype(float)) ** 2, axis=0)\n",
    "    \n",
    "    # weighted median of CRPS\n",
    "    if len(crps) != len(weights):\n",
    "        raise ValueError(\"Length of CRPS series and weights must be equal\")\n",
    "\n",
    "    weighted_sum = np.sum(crps * weights)\n",
    "    total_weights = np.sum(weights)\n",
    "\n",
    "    if total_weights == 0:\n",
    "        raise ValueError(\"Total weight cannot be zero\")\n",
    "\n",
    "    weighted_crps = weighted_sum / total_weights\n",
    "    \n",
    "    return round(weighted_crps, 4)\n",
    "\n",
    "# Example usage\n",
    "forecast_samples = pd.DataFrame(np.random.randn(1000, 5))  # Example forecast samples\n",
    "observations = pd.Series(np.random.randn(5))  # Example observations\n",
    "weights = np.random.rand(5)  # Example weights\n",
    "\n",
    "weighted_crps = crps(forecast_samples, observations, weights)\n",
    "print(\"Weighted Mean CRPS:\", weighted_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, sine_activation, cosine_activation, model):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.sine_activation = sine_activation\n",
    "        self.cosine_activation = cosine_activation\n",
    "        self.sparse_attention = model\n",
    "\n",
    "    def forward(self, input_series, future_time_stamps):\n",
    "        # Assuming future_time_stamps is preprocessed to match the input dimensionality\n",
    "        # and choosing either sine or cosine activation based on the use case\n",
    "        prediction_encodings = self.Encoding(prediction_time_stamps)  # or cosine_activation\n",
    "\n",
    "        # Use the Sparse Attention Module for predicting future values\n",
    "        future_predictions = self.sparse_attention(prediction_encodings)\n",
    "\n",
    "        # future_predictions could be further processed to predict specific values, e.g., through a linear layer\n",
    "        linear_layer = nn.Linear(in_features, out_features)(future_predictions)\n",
    "        predictions = softmax(linear_layer)\n",
    "        \n",
    "        return future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformer(\n",
       "  (sine_activation): SineActivation()\n",
       "  (cosine_activation): CosineActivation()\n",
       "  (sparse_attention): SparseAttentionModule_Prob(\n",
       "    (sampler): ProbabilisticEmbeddingSampler()\n",
       "    (attention_layer): AttentionLayer(\n",
       "      (inner_attention): ProbAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoCorr Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seasonal_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic masking\n",
    "class ProbMask(nn.Module):\n",
    "    def __init__(self, B, H, L, input_dim, top_k=5, device=\"cpu\"):\n",
    "        super(ProbMask, self).__init__()\n",
    "        self.B = B\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.scoring_network = ScoringNetwork(input_dim, L).to(device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # Compute scores using the scoring network\n",
    "        scores = self.scoring_network(input_sequence)  # input_sequence shape: [B, L, input_dim]\n",
    "\n",
    "        # Select top-k scores to generate indices\n",
    "        _, indices = torch.topk(scores, self.top_k, dim=-1)  # Selecting indices of top-k scores\n",
    "\n",
    "        # Create a mask for all positions\n",
    "        full_mask = torch.ones((self.B, self.L), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Update mask for top-k positions\n",
    "        batch_indices = torch.arange(self.B, device=self.device)[:, None]\n",
    "        full_mask[batch_indices, indices] = False\n",
    "\n",
    "        # Expand mask for all heads\n",
    "        mask = full_mask[:, None, :].expand(-1, self.H, -1)\n",
    "\n",
    "        # Mask should be of shape [B, H, L, L]\n",
    "        mask = mask.unsqueeze(2).expand(-1, -1, self.L, -1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "class ScoringNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ScoringNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Using sigmoid to keep scores between 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
