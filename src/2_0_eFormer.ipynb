{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from masking import TriangularCausalMask, ProbMask\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "\n",
    "n_heads_global = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "probabilistic embedding & positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = Kelmarsh_df['1'][['# Date and time', 'Energy Export (kWh)']][-1024:]\n",
    "\n",
    "# First, ensure that the column is in datetime format\n",
    "test_df['# Date and time'] = pd.to_datetime(test_df['# Date and time'])\n",
    "\n",
    "# Then convert it to timestamps\n",
    "test_df['Timestamp'] = test_df['# Date and time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# interpolate NaN values\n",
    "test_df = test_df.interpolate(method='linear')\n",
    "\n",
    "features_matrix = test_df[['Energy Export (kWh)', 'Timestamp']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "def t2v(\n",
    "    tau, # input tensor\n",
    "    f, # activation function (sin or cosin)\n",
    "    out_features, # size of output vector\n",
    "    w, # weights\n",
    "    b, # biases\n",
    "    w0, # weights for linear part of time2vec layer\n",
    "    b0, # biases for linear part of time2vec layer\n",
    "    arg=None # optional arguments\n",
    "    ):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "class CosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CosineActivation, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.w0 = nn.parameter.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.parameter.Parameter(torch.randn(1))\n",
    "        self.w = nn.parameter.Parameter(torch.randn(in_features, out_features-1))\n",
    "        self.b = nn.parameter.Parameter(torch.randn(out_features-1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        return t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sineact = SineActivation(2, 64)\n",
    "    cosact = CosineActivation(2, 64)\n",
    "\n",
    "feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "# check for NaN values early\n",
    "if torch.isnan(feature_tensor).any():\n",
    "    raise ValueError('NaN values detected in Input')\n",
    "    \n",
    "t2v_tensor = sineact(feature_tensor)\n",
    "\n",
    "print(t2v_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 64])\n"
     ]
    }
   ],
   "source": [
    "def t2v(\n",
    "    tau, # input tensor\n",
    "    f, # activation function (sin or cosin)\n",
    "    out_features, # size of output vector\n",
    "    w, # weights\n",
    "    b, # biases\n",
    "    w0, # weights for linear part of time2vec layer\n",
    "    b0, # biases for linear part of time2vec layer\n",
    "    arg=None # optional arguments\n",
    "    ):\n",
    "    if arg:\n",
    "        v1 = f(torch.matmul(tau, w) + b, arg)\n",
    "    else:\n",
    "        #print(w.shape, t1.shape, b.shape)\n",
    "        v1 = f(torch.matmul(tau, w) + b)\n",
    "    v2 = torch.matmul(tau, w0) + b0\n",
    "    #print(v1.shape)\n",
    "    return torch.cat([v1, v2], -1)\n",
    "\n",
    "class ProbabilisticSineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProbabilisticSineActivation, self).__init__()\n",
    "        self.out_features = out_features // 2  # Half for mean, half for variance\n",
    "        self.w0 = nn.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.Parameter(torch.randn(1))\n",
    "        self.w = nn.Parameter(torch.randn(in_features, self.out_features - 1))\n",
    "        self.b = nn.Parameter(torch.randn(self.out_features - 1))\n",
    "        self.f = torch.sin\n",
    "\n",
    "    def forward(self, tau):\n",
    "        # Calculate mean\n",
    "        mean = t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "        \n",
    "        # Calculate variance (use another set of weights and biases, ensure positive variance)\n",
    "        variance = F.softplus(t2v(tau, self.f, self.out_features, \n",
    "                                  torch.randn_like(self.w), torch.randn_like(self.b), \n",
    "                                  torch.randn_like(self.w0), torch.randn_like(self.b0)))\n",
    "        \n",
    "        return torch.cat([mean, variance], -1)\n",
    "\n",
    "class ProbabilisticCosineActivation(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ProbabilisticCosineActivation, self).__init__()\n",
    "        self.out_features = out_features // 2  # Half for mean, half for variance\n",
    "        self.w0 = nn.Parameter(torch.randn(in_features, 1))\n",
    "        self.b0 = nn.Parameter(torch.randn(1))\n",
    "        self.w = nn.Parameter(torch.randn(in_features, self.out_features - 1))\n",
    "        self.b = nn.Parameter(torch.randn(self.out_features - 1))\n",
    "        self.f = torch.cos\n",
    "\n",
    "    def forward(self, tau):\n",
    "        # Calculate mean\n",
    "        mean = t2v(tau, self.f, self.out_features, self.w, self.b, self.w0, self.b0)\n",
    "        \n",
    "        # Calculate variance (use another set of weights and biases, ensure positive variance)\n",
    "        variance = F.softplus(t2v(tau, self.f, self.out_features, \n",
    "                                  torch.randn_like(self.w), torch.randn_like(self.b), \n",
    "                                  torch.randn_like(self.w0), torch.randn_like(self.b0)))\n",
    "        \n",
    "        return torch.cat([mean, variance], -1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prob_sineact = ProbabilisticSineActivation(2, 64)\n",
    "    prob_cosact = ProbabilisticCosineActivation(2,64)\n",
    "\n",
    "    feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "    # check for NaN values early\n",
    "    if torch.isnan(feature_tensor).any():\n",
    "        raise ValueError('NaN values detected in Input Tensor')\n",
    "\n",
    "    prob_embeddings = prob_sineact(feature_tensor)\n",
    "\n",
    "    # check for NaN values early\n",
    "    if torch.isnan(prob_embeddings).any():\n",
    "        raise ValueError('NaN values detected in probabilistic Embeddings')\n",
    "\n",
    "    # Ensure prob_embeddings has the correct shape [B, L, E]\n",
    "    if len(prob_embeddings.shape) == 2:\n",
    "        prob_embeddings = prob_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    elif len(prob_embeddings.shape) == 1:\n",
    "        prob_embeddings = prob_embeddings.unsqueeze(0).unsqueeze(0)  # Add batch and length dimensions\n",
    "\n",
    "    # prob_embeddings now contains both mean and variance for each feature\n",
    "    print(prob_embeddings.shape)  # Should be [batch_size, 64] (32 for mean, 32 for variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Attention\n",
    "\n",
    "Credit to [Informer](https://github.com/zhouhaoyi/Informer2020/blob/0ac81e04d4095ecb97a3a78c7b49c936d8aa9933/models/attn.py#L38)\n",
    "\n",
    "Processing mean and variance:\n",
    "\n",
    "- Separate Attention Layers: The model now has separate attention layers for processing means and variances. This allows each component to be updated based on its own dynamics.\n",
    "- Processing Means and Variances: Both components are processed through their respective attention layers.\n",
    "- Combining Outputs: The outputs (updated means and variances) are then concatenated to form the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :] # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # V_sum = V.sum(dim=-2)\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else: # use mask\n",
    "            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            contex = V.cumsum(dim=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return (context_in, attns)\n",
    "        else:\n",
    "            return (context_in, None)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2,1)\n",
    "        keys = keys.transpose(2,1)\n",
    "        values = values.transpose(2,1)\n",
    "\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item() # c*ln(L_k)\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item() # c*ln(L_q) \n",
    "\n",
    "        U_part = U_part if U_part<L_K else L_K\n",
    "        u = u if u<L_Q else L_Q\n",
    "        \n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u) \n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        # update the context with selected top_k queries\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n",
    "        \n",
    "        return context.transpose(2,1).contiguous(), attn\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, \n",
    "                 d_keys=None, d_values=None, mix=False):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "        self.mix = mix\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        if self.mix:\n",
    "            out = out.transpose(2,1).contiguous()\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttentionModule_mean(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(SparseAttentionModule_mean, self).__init__()\n",
    "        self.attention_layer = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Assuming embeddings are concatenated means and variances\n",
    "        # Split embeddings into mean and variance\n",
    "        means, variances = embeddings.split(embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "        # Use means for attention calculation\n",
    "        attention_output, _ = self.attention_layer(means, means, means, None)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "class SparseAttentionModule_mean_var(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, prob_sparse_factor=5, attention_dropout=0.1):\n",
    "        super(SparseAttentionModule_mean_var, self).__init__()\n",
    "        # Attention layers for both means and variances\n",
    "        self.attention_layer_means = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "        self.attention_layer_vars = AttentionLayer(\n",
    "            ProbAttention(mask_flag=True, factor=prob_sparse_factor, scale=None, attention_dropout=attention_dropout),\n",
    "            d_model=d_model, n_heads=n_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Split embeddings into mean and variance\n",
    "        means, variances = embeddings.split(embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "        # Process means and variances separately through attention layers\n",
    "        attention_output_means, _ = self.attention_layer_means(means, means, means, None)\n",
    "        attention_output_vars, _ = self.attention_layer_vars(variances, variances, variances, None)\n",
    "\n",
    "        # Combine the results\n",
    "        combined_output = torch.cat([attention_output_means, attention_output_vars], dim=-1)\n",
    "\n",
    "        return combined_output\n",
    "\n",
    "# Example usage\n",
    "model = SparseAttentionModule_mean_var(\n",
    "    d_model=(prob_embeddings.shape[-1] // 2),\n",
    "    n_heads=n_heads_global,\n",
    "    prob_sparse_factor=5\n",
    "    )\n",
    "\n",
    "output_sparse = model(prob_embeddings)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output_sparse).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Attention\n",
    "\n",
    "Credit to [Autoformer](https://github.com/thuml/Autoformer/blob/main/layers/AutoCorrelation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class SeasonalDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize layer\n",
    "series_decomp_layer = SeriesDecomp(kernel_size=7) \n",
    "\n",
    "# split mean and variance\n",
    "means, variances = prob_embeddings.split(prob_embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "# apply decomposition\n",
    "seasonal_means, trend_means = series_decomp_layer(means)\n",
    "seasonal = torch.cat([seasonal_means, variances], dim=-1)\n",
    "trend = torch.cat([trend_means, variances], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # print(f\"tmp_corr non-zero:\", len(tmp_corr[tmp_corr != 0]))\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[:,:,i,:].unsqueeze(1))\n",
    "            \n",
    "        # print(f\"nonzero output: {len(delays_agg[delays_agg!=0])}\")\n",
    "        \n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        #print(f\"out shape: {self.out_projection(out).shape}\")\n",
    "        #print(f\"attn shape: {attn.shape}\")\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, auto_corr, d_model, n_heads):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        # initialize AutoCorr\n",
    "        self.auto_corr = auto_corr \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        # Use the provided AutoCorrelation instance\n",
    "        return self.auto_corr(queries, keys, values, attn_mask)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=2, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "\n",
    "        # Create the AutoCorrelationLayer with the AutoCorrelation instance\n",
    "        self.n_heads = n_heads\n",
    "        self.auto_corr = AutoCorrelation(top_k=top_k)\n",
    "        self.auto_corr_layer = AutoCorrelationLayer(self.auto_corr, d_model, n_heads)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, prob_embeddings):\n",
    "        H = self.n_heads\n",
    "        E = input_dim // H # size per head\n",
    "\n",
    "        # Ensure that E is an even number\n",
    "        if input_dim % H != 0:\n",
    "            raise ValueError(\"Half the feature dimension is not divisible by the number of heads.\")\n",
    "\n",
    "        # Split tensor into means and variances\n",
    "        means, variances = prob_embeddings.split(input_dim, dim=-1)\n",
    "\n",
    "        # Reshape both halves for multi-head format: [B, L, H, E]\n",
    "        reshaped_means = means.view(B, L, H, E)\n",
    "        reshaped_variances = variances.view(B, L, H, E)\n",
    "\n",
    "        # Concatenate the reshaped means and variances\n",
    "        reshaped_embeddings = torch.cat([reshaped_means, reshaped_variances], dim=-1)\n",
    "\n",
    "        return self.auto_corr_layer(reshaped_embeddings, reshaped_embeddings, reshaped_embeddings, None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(AutoCorrEncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class AutoCorrEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m MyAutoCorrelationModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         d_model\u001b[39m=\u001b[39minput_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         n_heads\u001b[39m=\u001b[39mn_heads_global,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output_ar_prob \u001b[39m=\u001b[39m AutoCorrelationFunction(output_sparse_prob)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(output_ar_prob[output_ar_prob \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(output_ar_prob[output_ar_prob \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]))\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Assuming this matches your input dimension\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m MyAutoCorrelationModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     d_model\u001b[39m=\u001b[39minput_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_heads\u001b[39m=\u001b[39mn_heads_global,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Concatenate the reshaped means and variances\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m reshaped_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([reshaped_means, reshaped_variances], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_corr_layer(reshaped_embeddings, reshaped_embeddings, reshaped_embeddings, \u001b[39mNone\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, queries, keys, values, attn_mask):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     B, L, _ \u001b[39m=\u001b[39m queries\u001b[39m.\u001b[39mshape\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     _, S, _ \u001b[39m=\u001b[39m keys\u001b[39m.\u001b[39mshape\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X45sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "def AutoCorrelationFunction(input):\n",
    "    B, L, _ = input.shape\n",
    "    input_dim = input.shape[-1] // 2  # Assuming this matches your input dimension\n",
    "\n",
    "    model = MyAutoCorrelationModel(\n",
    "        d_model=input_dim,\n",
    "        n_heads=n_heads_global,\n",
    "        B=B,\n",
    "        L=L,\n",
    "        input_dim=input_dim,\n",
    "        top_k=4,  # Adjust as needed\n",
    "        device=input.device\n",
    "    )\n",
    "\n",
    "    return model(input)\n",
    "\n",
    "output_ar_prob = AutoCorrelationFunction(output_sparse_prob)\n",
    "\n",
    "print(len(output_ar_prob[output_ar_prob == 0]))\n",
    "print(len(output_ar_prob[output_ar_prob != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean CRPS: 1.3489\n"
     ]
    }
   ],
   "source": [
    "# CRPS (continouos ranked probability score)\n",
    "def crps(forecast, observations, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    forecast (pd.DataFrame or np.ndarray): Forecasts from the model (ensemble).\n",
    "    observations (pd.Series or np.ndarray): Observed values.\n",
    "    weights (np.array): Corresponding weights for the CRPS scores, derived from sparse attention.\n",
    "\n",
    "    Returns:\n",
    "    float: Weighted mean of the CRPS for all forecasts.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays if input is Pandas\n",
    "    if isinstance(forecast, pd.DataFrame):\n",
    "        forecast = forecast.to_numpy()\n",
    "    if isinstance(observations, pd.Series):\n",
    "        observations = observations.to_numpy()\n",
    "    \n",
    "    # Sort forecast samples\n",
    "    forecast.sort(axis=0)\n",
    "\n",
    "    # Ensure observations are broadcastable over the forecast_samples\n",
    "    observations = observations[np.newaxis, :]\n",
    "\n",
    "    # Calculate CRPS\n",
    "    cumsum_forecast = np.cumsum(forecast, axis=0) / forecast.shape[0]\n",
    "    crps = np.mean((cumsum_forecast - (forecast > observations).astype(float)) ** 2, axis=0)\n",
    "    \n",
    "    # weighted median of CRPS\n",
    "    if len(crps) != len(weights):\n",
    "        raise ValueError(\"Length of CRPS series and weights must be equal\")\n",
    "\n",
    "    weighted_sum = np.sum(crps * weights)\n",
    "    total_weights = np.sum(weights)\n",
    "\n",
    "    if total_weights == 0:\n",
    "        raise ValueError(\"Total weight cannot be zero\")\n",
    "\n",
    "    weighted_crps = weighted_sum / total_weights\n",
    "    \n",
    "    return round(weighted_crps, 4)\n",
    "\n",
    "# Example usage\n",
    "forecast_samples = pd.DataFrame(np.random.randn(1000, 5))  # Example forecast samples\n",
    "observations = pd.Series(np.random.randn(5))  # Example observations\n",
    "weights = np.random.rand(5)  # Example weights\n",
    "\n",
    "weighted_crps = crps(forecast_samples, observations, weights)\n",
    "print(\"Weighted Mean CRPS:\", weighted_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutocorrelation_Encoder():\n",
    "    def __init__(self):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.\n",
    "\n",
    "        # AutoCorrelation Part\n",
    "        self.autocorr_encoder = AutoCorrEncoder(\n",
    "            [\n",
    "                AutoCorrEncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                        output_attention=configs.output_attention),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=SeasonalDecomp(configs.d_model)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoCorr Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seasonal_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic masking\n",
    "class ProbMask(nn.Module):\n",
    "    def __init__(self, B, H, L, input_dim, top_k=5, device=\"cpu\"):\n",
    "        super(ProbMask, self).__init__()\n",
    "        self.B = B\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.scoring_network = ScoringNetwork(input_dim, L).to(device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # Compute scores using the scoring network\n",
    "        scores = self.scoring_network(input_sequence)  # input_sequence shape: [B, L, input_dim]\n",
    "\n",
    "        # Select top-k scores to generate indices\n",
    "        _, indices = torch.topk(scores, self.top_k, dim=-1)  # Selecting indices of top-k scores\n",
    "\n",
    "        # Create a mask for all positions\n",
    "        full_mask = torch.ones((self.B, self.L), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Update mask for top-k positions\n",
    "        batch_indices = torch.arange(self.B, device=self.device)[:, None]\n",
    "        full_mask[batch_indices, indices] = False\n",
    "\n",
    "        # Expand mask for all heads\n",
    "        mask = full_mask[:, None, :].expand(-1, self.H, -1)\n",
    "\n",
    "        # Mask should be of shape [B, H, L, L]\n",
    "        mask = mask.unsqueeze(2).expand(-1, -1, self.L, -1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "class ScoringNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ScoringNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Using sigmoid to keep scores between 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
