{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# measuring ressources\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
    "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
    "from eFormer.loss_function import CRPS, weighted_CRPS\n",
    "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
    "from eFormer.Dataloader import TimeSeriesDataProcessor\n",
    "\n",
    "# transformer Benchmarks\n",
    "from Benchmarks.Benchmarks import VanillaTransformer, Informer\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'hyperparameters' (dict)\n"
     ]
    }
   ],
   "source": [
    "# set global parameters\n",
    "hyperparameters = {\n",
    "    'n_heads': 4,\n",
    "    'ProbabilisticModel': False,\n",
    "    # embeddings\n",
    "    'len_embedding': 64,\n",
    "    'batch_size': 512,\n",
    "    # general\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 72,\n",
    "    'patience': 7,\n",
    "    'dropout': 0.05,\n",
    "    'learning_rate': 6e-4,\n",
    "    'WeightDecay': 1e-1,\n",
    "    'train_epochs': 2,\n",
    "    'num_workers': 10,\n",
    "    'step_forecast': 6,\n",
    "    # benchmarks\n",
    "    'factor': 1,\n",
    "    'output_attention': True,\n",
    "    'd_model': 64,\n",
    "    'c_out': 6,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'activation': 'relu',\n",
    "    'd_ff': 1,\n",
    "    'distil': True,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%store hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_conditions():\n",
    "    # Get CPU usage for each core\n",
    "    cpu_percent = round(psutil.cpu_percent(), 4)\n",
    "\n",
    "    # Get memory information\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_used_gb = round(memory_info.used / (1024 ** 3), 4)\n",
    "\n",
    "    # Get GPU information\n",
    "    try:\n",
    "      gpu_info = GPUtil.getGPUs()[0]\n",
    "      gpu_memory_used_gb = round(gpu_info.memoryUsed / 1024, 4)\n",
    "    except IndexError:\n",
    "      # If no GPU is found, set variables to None\n",
    "      gpu_memory_used_gb = None\n",
    "\n",
    "    # Collect data in a dictionary\n",
    "    comp_usage = {\n",
    "        'CPU Usage': cpu_percent,\n",
    "        'Memory Usage (GB)': memory_used_gb,\n",
    "        'GPU Usage (GB)': gpu_memory_used_gb\n",
    "    }\n",
    "\n",
    "    return comp_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_data(data, forecast, look_back):\n",
    "    data = data.set_index('# Date and time')\n",
    "    data.index.names = [None]\n",
    "    data = data.drop(['Long Term Wind (m/s)'], axis=1)\n",
    "    shifts = range(forecast, look_back + forecast)\n",
    "    variables = data.columns\n",
    "        \n",
    "    shifted_columns = []\n",
    "    for column in variables:\n",
    "        for i in shifts:\n",
    "            shifted_df = data[[column]].shift(i)\n",
    "            shifted_df.rename(columns={column: f\"{column} (lag {i})\"}, inplace=True)\n",
    "            shifted_columns.append(shifted_df)\n",
    "        \n",
    "    data = data.drop(['Wind speed (m/s)'], axis=1)\n",
    "    data_shifted = pd.concat([data] + shifted_columns, axis=1)\n",
    "    data_shifted.dropna(inplace=True)\n",
    "        \n",
    "    return data_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Kelmarsh_df['1']\n",
    "data = data.set_index('# Date and time')\n",
    "data.index.names = [None]\n",
    "data = data.drop(['Long Term Wind (m/s)'], axis=1)\n",
    "\n",
    "data.to_csv('../data/Results/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = shifted_data(Kelmarsh_df['1'], 6, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your initial DataFrame\n",
    "processor = TimeSeriesDataProcessor(\n",
    "    dataframe=data,\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    look_back=hyperparameters['seq_len'],\n",
    "    batch_size=hyperparameters['batch_size'])\n",
    "    \n",
    "train_loader, test_loader, eval_loader = processor.create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'features' (Tensor)\n",
      "Stored 'labels' (Tensor)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    features, labels = batch\n",
    "    break\n",
    "\n",
    "%store features labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "            self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Layer Perceptron Model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        weights = self.linear.weight.data\n",
    "        \n",
    "        return x.squeeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Linear(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = LinearModel(\n",
    "        input_size=(hyperparameters['seq_len']),\n",
    "        output_size=hyperparameters['pred_len'],\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred.unsqueeze(1)), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]).unsqueeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 with Loss: 6458.083128\n",
      "Epoch Duration: 5.6675s\n",
      "Epoch 2 / 2 with Loss: 1924.895431\n",
      "Epoch Duration: 4.2783s\n",
      "Validation loss decreased (inf --> 1576.784230)\n",
      "Model test loss: 20868.485352\n"
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = LinearModel(\n",
    "    input_size=(hyperparameters['seq_len'] * 2),\n",
    "    output_size=hyperparameters['pred_len'],\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    #print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# test data set\n",
    "test_losses = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_forecast(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eFormer(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, in_features, forecast, len_embedding_vector, n_heads, probabilistic_model=False):\n",
    "        super(eFormer, self).__init__()\n",
    "        self.probabilistic_model = probabilistic_model\n",
    "        self.n_heads = n_heads\n",
    "        self.len_embedding_vector = len_embedding_vector\n",
    "\n",
    "        # Initialize encoding model\n",
    "        if probabilistic_model:\n",
    "            self.encoding_model = ProbEncoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "        else:\n",
    "            self.encoding_model = Encoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "\n",
    "        # Initialize attention module\n",
    "        if probabilistic_model:\n",
    "            self.attention_module = ProbSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "        else:\n",
    "            self.attention_module = DetSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "\n",
    "        # Initialize decoder\n",
    "        # Assuming the decoder initialization does not actually require the output shape directly but parameters that depend on the model configuration\n",
    "        if probabilistic_model:\n",
    "            self.decoder = ProbSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "        else:\n",
    "            self.decoder = DetSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "\n",
    "    def forward(self, features_matrix):\n",
    "        if torch.isnan(features_matrix).any():\n",
    "            raise ValueError('NaN values detected in Input')\n",
    "\n",
    "        embeddings = self.encoding_model(features_matrix)\n",
    "        if torch.isnan(embeddings).any():\n",
    "            raise ValueError('NaN values detected in Embeddings')\n",
    "\n",
    "        encoder_output = self.attention_module(embeddings, embeddings, embeddings)\n",
    "        if torch.isnan(encoder_output).any():\n",
    "            raise ValueError('NaN values detected in Sparse Attention Output')\n",
    "\n",
    "        forecasts, crps_weights = self.decoder(encoder_output)\n",
    "        return forecasts, crps_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_eFormer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 with Loss: 18821.498356\n",
      "Validation Loss: 16472.054651\n",
      "Epoch Duration: 279.9072s\n",
      "Epoch 2 / 2 with Loss: 13616.619696\n",
      "Validation Loss: 11170.302771\n",
      "Epoch Duration: 249.8827s\n",
      "Validation loss decreased (inf --> 11170.302771)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Perform the multi-step forecast\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m predictions, weights \u001b[39m=\u001b[39m multi_step_forecast(model, features, hyperparameters[\u001b[39m'\u001b[39;49m\u001b[39mstep_forecast\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m test_loss \u001b[39m=\u001b[39m loss_fn(predictions[:\u001b[39m-\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]], labels[hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m forecasted_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m current_input \u001b[39m=\u001b[39m initial_input\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model_2 \u001b[39m=\u001b[39m LinearModel(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     input_size\u001b[39m=\u001b[39m(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output_size\u001b[39m=\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mpred_len\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m# split tensor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearModel' is not defined"
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = eFormer(\n",
    "    in_features=(hyperparameters['seq_len'] * 2),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data set\n",
    "test_losses = []\n",
    "predictions_collected = []\n",
    "groundtruth_collected = []\n",
    "batches_collected = 0  # Counter to keep track of the batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_eFormer(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-(hyperparameters['step_forecast']-1)], labels[(hyperparameters['step_forecast']-1):])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        if batches_collected >= 10:\n",
    "            pass  # Exit loop after collecting from 10 batches\n",
    "        else:\n",
    "            predictions_collected.extend(predictions.tolist())\n",
    "            groundtruth_collected.extend(labels.tolist())\n",
    "            batches_collected += 1\n",
    "\n",
    "test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "df_eval = pd.DataFrame({\n",
    "        'Predictions':predictions_collected,\n",
    "        'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/Benchmarks.py:85\u001b[0m, in \u001b[0;36mVanillaTransformer.forward\u001b[0;34m(self, input_data, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m enc_out, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(enc_in, attn_mask\u001b[39m=\u001b[39menc_self_mask)\n\u001b[1;32m     84\u001b[0m dec_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDecoderInput(enc_in)\n\u001b[0;32m---> 85\u001b[0m dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(dec_in, enc_in, x_mask\u001b[39m=\u001b[39;49mdec_self_mask, cross_mask\u001b[39m=\u001b[39;49mdec_enc_mask)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attention:\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m dec_out[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_len:, \u001b[39m0\u001b[39m], attns\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:123\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, cross, x_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cross_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 123\u001b[0m         x \u001b[39m=\u001b[39m layer(x, cross, x_mask\u001b[39m=\u001b[39;49mx_mask, cross_mask\u001b[39m=\u001b[39;49mcross_mask)\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:102\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m     97\u001b[0m     x, x, x,\n\u001b[1;32m     98\u001b[0m     attn_mask\u001b[39m=\u001b[39mx_mask\n\u001b[1;32m     99\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    100\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 102\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention(\n\u001b[1;32m    103\u001b[0m     x, cross, cross,\n\u001b[1;32m    104\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mcross_mask\n\u001b[1;32m    105\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)\n\u001b[1;32m    108\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(y\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/SelfAttention_Family.py:151\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    149\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_projection(queries)\u001b[39m.\u001b[39mview(B, L, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_projection(keys)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_projection(values)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    153\u001b[0m out, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_attention(\n\u001b[1;32m    154\u001b[0m     queries,\n\u001b[1;32m    155\u001b[0m     keys,\n\u001b[1;32m    156\u001b[0m     values,\n\u001b[1;32m    157\u001b[0m     attn_mask\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(B, L, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = VanillaTransformer(\n",
    "    configs = Config(hyperparameters)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = Informer(\n",
    "    configs = Config(hyperparameters)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Informer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step(model, initial_input, steps, hyperparameters, device):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "    - hyperparameters: A dictionary containing hyperparameters for the model.\n",
    "    - device: The device (CPU or GPU) on which the models are to run.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input.to(device)\n",
    "\n",
    "    # Assuming model_2 is already initialized and available here.\n",
    "    # If model_2's in_features need to be adjusted dynamically, you would adjust it here.\n",
    "    # Since the exact mechanism depends on the model's architecture, we'll assume\n",
    "    # it can be done without affecting the model's trained state or performance.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # Split tensor into first and second halves\n",
    "            first_half = current_input[:, :(current_input.shape[1] // 2)]\n",
    "            second_half = current_input[:, (current_input.shape[1] // 2):]\n",
    "\n",
    "            # Adjust model_2 for first half forecasting (conceptual)\n",
    "            # This would be replaced with your model-specific adjustment logic\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'])\n",
    "\n",
    "            # Forecast the first half\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "\n",
    "            # Update the first half with new predictions\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "\n",
    "            # Restore model_2 to original state if needed (conceptual)\n",
    "            # This step depends on your specific model's design\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'] * 2)\n",
    "\n",
    "            # Use the original model for the second half forecast\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "\n",
    "            # Create new tensor for the next step\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # Store forecasted values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.tensor(forecasted_steps[-1]).to(device), weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
    "Kelmarsh_full_df = pd.concat(Kelmarsh_df.values(), ignore_index=True)\n",
    "Penmanshiel_full_df = pd.concat(Penmanshiel_df.values(), ignore_index=True)\n",
    "\n",
    "Wind_df = pd.concat([Kelmarsh_full_df, Penmanshiel_full_df], ignore_index=True, sort=False)\n",
    "\n",
    "Wind_df = Wind_df.drop('date', axis=1)\n",
    "\n",
    "Wind_df.to_csv('../data/Windturbinen/Wind_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do's\n",
    "\n",
    "- ground truth and forecasted values in graph as time series\n",
    "- seperate model for wind and company, one model can't output 2 different results for same equation\n",
    "\n",
    "- label_length = look_back\n",
    "- sequence_length -> window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
