{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# measuring ressources\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
    "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
    "from eFormer.loss_function import CRPS, weighted_CRPS\n",
    "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
    "from eFormer.Dataloader import TimeSeriesDataProcessor\n",
    "\n",
    "# transformer Benchmarks\n",
    "from Benchmarks.Benchmarks import VanillaTransformer, Informer\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'hyperparameters' (dict)\n"
     ]
    }
   ],
   "source": [
    "# set global parameters\n",
    "hyperparameters = {\n",
    "    'n_heads': 4,\n",
    "    'ProbabilisticModel': False,\n",
    "    # embeddings\n",
    "    'len_embedding': 64,\n",
    "    'batch_size': 512,\n",
    "    # general\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 72,\n",
    "    'patience': 7,\n",
    "    'dropout': 0.05,\n",
    "    'learning_rate': 6e-4,\n",
    "    'WeightDecay': 1e-1,\n",
    "    'train_epochs': 2,\n",
    "    'num_workers': 10,\n",
    "    'step_forecast': 6,\n",
    "    # benchmarks\n",
    "    'factor': 1,\n",
    "    'output_attention': True,\n",
    "    'd_model': 64,\n",
    "    'c_out': 6,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'activation': 'relu',\n",
    "    'd_ff': 1,\n",
    "    'distil': True,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%store hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_conditions():\n",
    "    # Get CPU usage for each core\n",
    "    cpu_percent = round(psutil.cpu_percent(), 4)\n",
    "\n",
    "    # Get memory information\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_used_gb = round(memory_info.used / (1024 ** 3), 4)\n",
    "\n",
    "    # Get GPU information\n",
    "    try:\n",
    "      gpu_info = GPUtil.getGPUs()[0]\n",
    "      gpu_memory_used_gb = round(gpu_info.memoryUsed / 1024, 4)\n",
    "    except IndexError:\n",
    "      # If no GPU is found, set variables to None\n",
    "      gpu_memory_used_gb = None\n",
    "\n",
    "    # Collect data in a dictionary\n",
    "    comp_usage = {\n",
    "        'CPU Usage': cpu_percent,\n",
    "        'Memory Usage (GB)': memory_used_gb,\n",
    "        'GPU Usage (GB)': gpu_memory_used_gb\n",
    "    }\n",
    "\n",
    "    return comp_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Kelmarsh_df['1']\n",
    "data = data.set_index('# Date and time')\n",
    "data.index.names = [None]\n",
    "data = data.drop(['Long Term Wind (m/s)'], axis=1)\n",
    "\n",
    "data.to_csv('../data/Results/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your initial DataFrame\n",
    "processor = TimeSeriesDataProcessor(\n",
    "    dataframe=data,\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    look_back=hyperparameters['seq_len'],\n",
    "    batch_size=hyperparameters['batch_size'])\n",
    "    \n",
    "train_loader, test_loader, eval_loader = processor.create_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAM saven durh zwischenspeichern & garbage collecten und wieder neu laden\n",
    "\n",
    "table captions Ã¼ber Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'features' (Tensor)\n",
      "Stored 'labels' (Tensor)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    features, labels = batch\n",
    "    break\n",
    "\n",
    "%store features labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "            self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Layer Perceptron Model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        weights = self.linear.weight.data\n",
    "        \n",
    "        return x.squeeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Linear(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = LinearModel(\n",
    "        input_size=(hyperparameters['seq_len']),\n",
    "        output_size=hyperparameters['pred_len'],\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred.unsqueeze(1)), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction)\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]).unsqueeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x288 and 144x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 16\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m), weights\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x288 and 144x1)"
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = LinearModel(\n",
    "    input_size=(hyperparameters['seq_len'] * 2),\n",
    "    output_size=hyperparameters['pred_len'],\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    #print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# test data set\n",
    "test_losses = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_forecast(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:].unsqueeze(1))\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eFormer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eFormer(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, in_features, forecast, len_embedding_vector, n_heads, probabilistic_model=False):\n",
    "        super(eFormer, self).__init__()\n",
    "        self.probabilistic_model = probabilistic_model\n",
    "        self.n_heads = n_heads\n",
    "        self.len_embedding_vector = len_embedding_vector\n",
    "\n",
    "        # Initialize encoding model\n",
    "        if probabilistic_model:\n",
    "            self.encoding_model = ProbEncoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "        else:\n",
    "            self.encoding_model = Encoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "\n",
    "        # Initialize attention module\n",
    "        if probabilistic_model:\n",
    "            self.attention_module = ProbSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "        else:\n",
    "            self.attention_module = DetSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "\n",
    "        # Initialize decoder\n",
    "        # Assuming the decoder initialization does not actually require the output shape directly but parameters that depend on the model configuration\n",
    "        if probabilistic_model:\n",
    "            self.decoder = ProbSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "        else:\n",
    "            self.decoder = DetSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "\n",
    "    def forward(self, features_matrix):\n",
    "        if torch.isnan(features_matrix).any():\n",
    "            raise ValueError('NaN values detected in Input')\n",
    "\n",
    "        embeddings = self.encoding_model(features_matrix)\n",
    "        if torch.isnan(embeddings).any():\n",
    "            raise ValueError('NaN values detected in Embeddings')\n",
    "\n",
    "        encoder_output = self.attention_module(embeddings, embeddings, embeddings)\n",
    "        if torch.isnan(encoder_output).any():\n",
    "            raise ValueError('NaN values detected in Sparse Attention Output')\n",
    "\n",
    "        forecasts, crps_weights = self.decoder(encoder_output)\n",
    "        return forecasts, crps_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_eFormer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 with Loss: 18821.498356\n",
      "Validation Loss: 16472.054651\n",
      "Epoch Duration: 279.9072s\n",
      "Epoch 2 / 2 with Loss: 13616.619696\n",
      "Validation Loss: 11170.302771\n",
      "Epoch Duration: 249.8827s\n",
      "Validation loss decreased (inf --> 11170.302771)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Perform the multi-step forecast\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m predictions, weights \u001b[39m=\u001b[39m multi_step_forecast(model, features, hyperparameters[\u001b[39m'\u001b[39;49m\u001b[39mstep_forecast\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m test_loss \u001b[39m=\u001b[39m loss_fn(predictions[:\u001b[39m-\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]], labels[hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m forecasted_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m current_input \u001b[39m=\u001b[39m initial_input\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model_2 \u001b[39m=\u001b[39m LinearModel(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     input_size\u001b[39m=\u001b[39m(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     output_size\u001b[39m=\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mpred_len\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m# split tensor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearModel' is not defined"
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = eFormer(\n",
    "    in_features=(hyperparameters['seq_len'] * 2),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data set\n",
    "test_losses = []\n",
    "predictions_collected = []\n",
    "groundtruth_collected = []\n",
    "batches_collected = 0  # Counter to keep track of the batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_eFormer(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-(hyperparameters['step_forecast']-1)], labels[(hyperparameters['step_forecast']-1):])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        if batches_collected >= 10:\n",
    "            pass  # Exit loop after collecting from 10 batches\n",
    "        else:\n",
    "            predictions_collected.extend(predictions.tolist())\n",
    "            groundtruth_collected.extend(labels.tolist())\n",
    "            batches_collected += 1\n",
    "\n",
    "test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "df_eval = pd.DataFrame({\n",
    "        'Predictions':predictions_collected,\n",
    "        'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X46sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/Benchmarks.py:85\u001b[0m, in \u001b[0;36mVanillaTransformer.forward\u001b[0;34m(self, input_data, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m enc_out, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(enc_in, attn_mask\u001b[39m=\u001b[39menc_self_mask)\n\u001b[1;32m     84\u001b[0m dec_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDecoderInput(enc_in)\n\u001b[0;32m---> 85\u001b[0m dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(dec_in, enc_in, x_mask\u001b[39m=\u001b[39;49mdec_self_mask, cross_mask\u001b[39m=\u001b[39;49mdec_enc_mask)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attention:\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m dec_out[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_len:, \u001b[39m0\u001b[39m], attns\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:123\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, cross, x_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cross_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 123\u001b[0m         x \u001b[39m=\u001b[39m layer(x, cross, x_mask\u001b[39m=\u001b[39;49mx_mask, cross_mask\u001b[39m=\u001b[39;49mcross_mask)\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:102\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m     97\u001b[0m     x, x, x,\n\u001b[1;32m     98\u001b[0m     attn_mask\u001b[39m=\u001b[39mx_mask\n\u001b[1;32m     99\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    100\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 102\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention(\n\u001b[1;32m    103\u001b[0m     x, cross, cross,\n\u001b[1;32m    104\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mcross_mask\n\u001b[1;32m    105\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)\n\u001b[1;32m    108\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(y\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/SelfAttention_Family.py:151\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    149\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_projection(queries)\u001b[39m.\u001b[39mview(B, L, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    150\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_projection(keys)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_projection(values)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    153\u001b[0m out, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_attention(\n\u001b[1;32m    154\u001b[0m     queries,\n\u001b[1;32m    155\u001b[0m     keys,\n\u001b[1;32m    156\u001b[0m     values,\n\u001b[1;32m    157\u001b[0m     attn_mask\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(B, L, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = VanillaTransformer(\n",
    "    configs = Config(hyperparameters)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 with Loss: 18785.266602\n",
      "Validation Loss: 16526.207458\n",
      "Epoch Duration: 129.1507s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m features, labels \u001b[39min\u001b[39;49;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     features, labels \u001b[39m=\u001b[39;49m features\u001b[39m.\u001b[39;49mto(device), labels\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/eFormer/Dataloader.py:91\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[39m# %% [markdown]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Training Data Set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# recurrent forecast\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mclass TimeSeriesDataProcessor:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    def __init__(self, dataframe, forecast, look_back, batch_size=64, train_size=0.7, test_size=0.5, random_state=42):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m        self.dataframe = dataframe\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m        self.forecast = forecast\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m        self.look_back = look_back\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m        self.batch_size = batch_size\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m        self.train_size = train_size\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m        self.test_size = test_size\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m        self.random_state = random_state\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    def padding_data(self, dataframe):\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m        remainder = dataframe.shape[0] % self.batch_size\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m        if remainder == 0:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m            return dataframe # Already divisible by batch size\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m        discard = remainder\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m        if isinstance(dataframe, pd.DataFrame):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m            return dataframe[discard:]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    def shifted_data(self):\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m        data = self.dataframe\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m        forecast = self.forecast\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m        look_back = self.look_back\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m        shifts = range(forecast, look_back + forecast)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m        variables = data.columns\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39m        shifted_columns = []\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m        for column in variables:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m            for i in shifts:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m                shifted_df = data[[column]].shift(i)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m                shifted_df.rename(columns={column: f\"{column} (lag {i})\"}, inplace=True)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m                shifted_columns.append(shifted_df)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m        \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m        data_shifted = pd.concat([data] + shifted_columns, axis=1)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m        data_shifted.dropna(inplace=True)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39m        return data_shifted\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    def prepare_datasets(self):\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m        try:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m            s_df = self.shifted_data().drop(['Wind speed (m/s)'], axis=1)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m        except KeyError:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m            s_df = self.shifted_data().copy()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m        # Splitting dataset\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m        df_train, df_rem = train_test_split(s_df, train_size=self.train_size, random_state=False)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m        df_eval, df_test = train_test_split(df_rem, test_size=self.test_size, random_state=False)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[39m        df_train = self.padding_data(df_train)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m        df_eval = self.padding_data(df_eval)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m        df_test = self.padding_data(df_test)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m        # Wrapping datasets\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m        self.train_dataset = TimeSeriesDataset(df_train)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m        self.test_dataset = TimeSeriesDataset(df_test)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m        self.eval_dataset = TimeSeriesDataset(df_eval)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[39m    def create_dataloaders(self):\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m        self.prepare_datasets()\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m        self.eval_loader = DataLoader(self.eval_dataset, batch_size=self.batch_size, shuffle=False)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39m        return self.train_loader, self.test_loader, self.eval_loader\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[39mclass TimeSeriesDataset(Dataset):\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    def __init__(self, dataframe):\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m        self.labels = dataframe.iloc[:, 0].values\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m        self.features = dataframe.iloc[:, 1:].values\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m    def __len__(self):\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m        return len(self.labels)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m    def __getitem__(self, idx):\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m        features = torch.FloatTensor(self.features[idx])\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m        labels = torch.FloatTensor([self.labels[idx]])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m        return features, labels\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTimeSeriesDataProcessor\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, forecast, look_back, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, train_size\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = Informer(\n",
    "    configs = Config(hyperparameters)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Informer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step(model, initial_input, steps, hyperparameters, device):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "    - hyperparameters: A dictionary containing hyperparameters for the model.\n",
    "    - device: The device (CPU or GPU) on which the models are to run.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input.to(device)\n",
    "\n",
    "    # Assuming model_2 is already initialized and available here.\n",
    "    # If model_2's in_features need to be adjusted dynamically, you would adjust it here.\n",
    "    # Since the exact mechanism depends on the model's architecture, we'll assume\n",
    "    # it can be done without affecting the model's trained state or performance.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # Split tensor into first and second halves\n",
    "            first_half = current_input[:, :(current_input.shape[1] // 2)]\n",
    "            second_half = current_input[:, (current_input.shape[1] // 2):]\n",
    "\n",
    "            # Adjust model_2 for first half forecasting (conceptual)\n",
    "            # This would be replaced with your model-specific adjustment logic\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'])\n",
    "\n",
    "            # Forecast the first half\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "\n",
    "            # Update the first half with new predictions\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "\n",
    "            # Restore model_2 to original state if needed (conceptual)\n",
    "            # This step depends on your specific model's design\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'] * 2)\n",
    "\n",
    "            # Use the original model for the second half forecast\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "\n",
    "            # Create new tensor for the next step\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # Store forecasted values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.tensor(forecasted_steps[-1]).to(device), weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
    "Kelmarsh_full_df = pd.concat(Kelmarsh_df.values(), ignore_index=True)\n",
    "Penmanshiel_full_df = pd.concat(Penmanshiel_df.values(), ignore_index=True)\n",
    "\n",
    "Wind_df = pd.concat([Kelmarsh_full_df, Penmanshiel_full_df], ignore_index=True, sort=False)\n",
    "\n",
    "Wind_df = Wind_df.drop('date', axis=1)\n",
    "\n",
    "Wind_df.to_csv('../data/Windturbinen/Wind_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do's\n",
    "\n",
    "- ground truth and forecasted values in graph as time series\n",
    "- seperate model for wind and company, one model can't output 2 different results for same equation\n",
    "\n",
    "- label_length = look_back\n",
    "- sequence_length -> window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
