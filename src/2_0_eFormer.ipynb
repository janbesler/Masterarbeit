{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# eFormer\n",
    "from embeddings import SineActivation, CosineActivation\n",
    "from sparse_prob import \n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "\n",
    "n_heads_global = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "probabilistic embedding & positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = Kelmarsh_df['1'][['# Date and time', 'Energy Export (kWh)']][-1024:]\n",
    "\n",
    "# First, ensure that the column is in datetime format\n",
    "test_df['# Date and time'] = pd.to_datetime(test_df['# Date and time'])\n",
    "\n",
    "# Then convert it to timestamps\n",
    "test_df['Timestamp'] = test_df['# Date and time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# interpolate NaN values\n",
    "test_df = test_df.interpolate(method='linear')\n",
    "\n",
    "features_matrix = test_df[['Energy Export (kWh)', 'Timestamp']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 64])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sineact = SineActivation(2, 64)\n",
    "    cosact = CosineActivation(2, 64)\n",
    "\n",
    "feature_tensor = torch.tensor(features_matrix, dtype=torch.float32)\n",
    "# check for NaN values early\n",
    "if torch.isnan(feature_tensor).any():\n",
    "    raise ValueError('NaN values detected in Input')\n",
    "    \n",
    "t2v_tensor = sineact(feature_tensor)\n",
    "\n",
    "print(t2v_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SparseAttentionModule_Prob(\n",
    "    d_model=embeddings,\n",
    "    n_heads=n_heads_global,\n",
    "    prob_sparse_factor=5\n",
    "    )\n",
    "\n",
    "output_sparse = model(t2v_tensor)\n",
    "\n",
    "# check for NaN values early\n",
    "if torch.isnan(output_sparse).any():\n",
    "    raise ValueError('NaN values detected in ProbSparse Output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Attention\n",
    "\n",
    "Credit to [Autoformer](https://github.com/thuml/Autoformer/blob/main/layers/AutoCorrelation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride=1):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class SeriesDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for extracting trend and seasonal components.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend\n",
    "\n",
    "class SeasonalDecomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    \"\"\" \n",
    "    Masking the future data points using a triangle.\n",
    "    \"\"\" \n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize layer\n",
    "series_decomp_layer = SeriesDecomp(kernel_size=7) \n",
    "\n",
    "# split mean and variance\n",
    "means, variances = prob_embeddings.split(prob_embeddings.shape[-1] // 2, dim=-1)\n",
    "\n",
    "# apply decomposition\n",
    "seasonal_means, trend_means = series_decomp_layer(means)\n",
    "seasonal = torch.cat([seasonal_means, variances], dim=-1)\n",
    "trend = torch.cat([trend_means, variances], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False, top_k=2):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def time_delay_agg(self, values, corr):\n",
    "        \"\"\"\n",
    "        Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = self.top_k\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "\n",
    "        weights, delay = torch.topk(corr, self.top_k, dim=-1)\n",
    "        # Reshape delay and weights dynamically\n",
    "        delay = delay.reshape(batch, head, self.top_k, length)\n",
    "        weights = weights.reshape(batch, head, self.top_k, length)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # print(f\"tmp_corr non-zero:\", len(tmp_corr[tmp_corr != 0]))\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(init_index).float()\n",
    "\n",
    "        for i in range(self.top_k):\n",
    "            tmp_delay = init_index.expand_as(delay) + delay[..., i].unsqueeze(-1)\n",
    "            tmp_delay = torch.clamp(tmp_delay, min=0, max=length-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[:,:,i,:].unsqueeze(1))\n",
    "            \n",
    "        # print(f\"nonzero output: {len(delays_agg[delays_agg!=0])}\")\n",
    "        \n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # Ensure corr has the shape [B, H, L, L]\n",
    "        corr = corr.unsqueeze(2)\n",
    "        expand_multiplier = L // corr.shape[3]\n",
    "        corr = corr.expand(-1, -1, expand_multiplier, -1, -1)\n",
    "        corr = corr.reshape(\n",
    "            corr.shape[0],\n",
    "            corr.shape[1],\n",
    "            L,\n",
    "            corr.shape[-1]\n",
    "        )\n",
    "\n",
    "        # Create and apply the mask\n",
    "        if self.mask_flag:\n",
    "            mask = TriangularCausalMask(B, L, device=queries.device).mask\n",
    "            mask = mask.expand(-1, H, -1, -1)  # Shape: [B, H, L, L]\n",
    "            corr = corr.masked_fill(mask, 0)  # Zero out the masked positions\n",
    "\n",
    "        # time delay aggregation\n",
    "        V = self.time_delay_agg(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        #print(f\"out shape: {self.out_projection(out).shape}\")\n",
    "        #print(f\"attn shape: {attn.shape}\")\n",
    "\n",
    "        return self.out_projection(out), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, auto_corr, d_model, n_heads):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        # initialize AutoCorr\n",
    "        self.auto_corr = auto_corr \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        # Use the provided AutoCorrelation instance\n",
    "        return self.auto_corr(queries, keys, values, attn_mask)\n",
    "\n",
    "class MyAutoCorrelationModel(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, B, L, input_dim, top_k=2, device=\"cpu\"):\n",
    "        super(MyAutoCorrelationModel, self).__init__()\n",
    "\n",
    "        # Create the AutoCorrelationLayer with the AutoCorrelation instance\n",
    "        self.n_heads = n_heads\n",
    "        self.auto_corr = AutoCorrelation(top_k=top_k)\n",
    "        self.auto_corr_layer = AutoCorrelationLayer(self.auto_corr, d_model, n_heads)\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, prob_embeddings):\n",
    "        H = self.n_heads\n",
    "        E = input_dim // H # size per head\n",
    "\n",
    "        # Ensure that E is an even number\n",
    "        if input_dim % H != 0:\n",
    "            raise ValueError(\"Half the feature dimension is not divisible by the number of heads.\")\n",
    "\n",
    "        # Split tensor into means and variances\n",
    "        means, variances = prob_embeddings.split(input_dim, dim=-1)\n",
    "\n",
    "        # Reshape both halves for multi-head format: [B, L, H, E]\n",
    "        reshaped_means = means.view(B, L, H, E)\n",
    "        reshaped_variances = variances.view(B, L, H, E)\n",
    "\n",
    "        # Concatenate the reshaped means and variances\n",
    "        reshaped_embeddings = torch.cat([reshaped_means, reshaped_variances], dim=-1)\n",
    "\n",
    "        return self.auto_corr_layer(reshaped_embeddings, reshaped_embeddings, reshaped_embeddings, None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(AutoCorrEncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class AutoCorrEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model \u001b[39m=\u001b[39m MyAutoCorrelationModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         d_model\u001b[39m=\u001b[39minput_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         n_heads\u001b[39m=\u001b[39mn_heads_global,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output_ar_prob \u001b[39m=\u001b[39m AutoCorrelationFunction(output_sparse)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(output_ar_prob[output_ar_prob \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(output_ar_prob[output_ar_prob \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m]))\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m input_dim \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Assuming this matches your input dimension\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m MyAutoCorrelationModel(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     d_model\u001b[39m=\u001b[39minput_dim,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     n_heads\u001b[39m=\u001b[39mn_heads_global,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, prob_embeddings):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     E \u001b[39m=\u001b[39m input_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m H \u001b[39m# size per head\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Ensure that E is an even number\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m input_dim \u001b[39m%\u001b[39m H \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "def AutoCorrelationFunction(input):\n",
    "    B, L, _ = input.shape\n",
    "    input_dim = input.shape[-1] // 2  # Assuming this matches your input dimension\n",
    "\n",
    "    model = MyAutoCorrelationModel(\n",
    "        d_model=input_dim,\n",
    "        n_heads=n_heads_global,\n",
    "        B=B,\n",
    "        L=L,\n",
    "        input_dim=input_dim,\n",
    "        top_k=4,  # Adjust as needed\n",
    "        device=input.device\n",
    "    )\n",
    "\n",
    "    return model(input)\n",
    "\n",
    "output_ar_prob = AutoCorrelationFunction(output_sparse)\n",
    "\n",
    "print(len(output_ar_prob[output_ar_prob == 0]))\n",
    "print(len(output_ar_prob[output_ar_prob != 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean CRPS: 1.3489\n"
     ]
    }
   ],
   "source": [
    "# CRPS (continouos ranked probability score)\n",
    "def crps(forecast, observations, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    forecast (pd.DataFrame or np.ndarray): Forecasts from the model (ensemble).\n",
    "    observations (pd.Series or np.ndarray): Observed values.\n",
    "    weights (np.array): Corresponding weights for the CRPS scores, derived from sparse attention.\n",
    "\n",
    "    Returns:\n",
    "    float: Weighted mean of the CRPS for all forecasts.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy arrays if input is Pandas\n",
    "    if isinstance(forecast, pd.DataFrame):\n",
    "        forecast = forecast.to_numpy()\n",
    "    if isinstance(observations, pd.Series):\n",
    "        observations = observations.to_numpy()\n",
    "    \n",
    "    # Sort forecast samples\n",
    "    forecast.sort(axis=0)\n",
    "\n",
    "    # Ensure observations are broadcastable over the forecast_samples\n",
    "    observations = observations[np.newaxis, :]\n",
    "\n",
    "    # Calculate CRPS\n",
    "    cumsum_forecast = np.cumsum(forecast, axis=0) / forecast.shape[0]\n",
    "    crps = np.mean((cumsum_forecast - (forecast > observations).astype(float)) ** 2, axis=0)\n",
    "    \n",
    "    # weighted median of CRPS\n",
    "    if len(crps) != len(weights):\n",
    "        raise ValueError(\"Length of CRPS series and weights must be equal\")\n",
    "\n",
    "    weighted_sum = np.sum(crps * weights)\n",
    "    total_weights = np.sum(weights)\n",
    "\n",
    "    if total_weights == 0:\n",
    "        raise ValueError(\"Total weight cannot be zero\")\n",
    "\n",
    "    weighted_crps = weighted_sum / total_weights\n",
    "    \n",
    "    return round(weighted_crps, 4)\n",
    "\n",
    "# Example usage\n",
    "forecast_samples = pd.DataFrame(np.random.randn(1000, 5))  # Example forecast samples\n",
    "observations = pd.Series(np.random.randn(5))  # Example observations\n",
    "weights = np.random.rand(5)  # Example weights\n",
    "\n",
    "weighted_crps = crps(forecast_samples, observations, weights)\n",
    "print(\"Weighted Mean CRPS:\", weighted_crps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutocorrelation_Encoder():\n",
    "    def __init__(self):\n",
    "        super(AutoCorrEncoder, self).__init__()\n",
    "        self.\n",
    "\n",
    "        # AutoCorrelation Part\n",
    "        self.autocorr_encoder = AutoCorrEncoder(\n",
    "            [\n",
    "                AutoCorrEncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                        output_attention=configs.output_attention),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=SeasonalDecomp(configs.d_model)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoCorr Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seasonal_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic masking\n",
    "class ProbMask(nn.Module):\n",
    "    def __init__(self, B, H, L, input_dim, top_k=5, device=\"cpu\"):\n",
    "        super(ProbMask, self).__init__()\n",
    "        self.B = B\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.top_k = top_k\n",
    "        self.device = device\n",
    "        self.scoring_network = ScoringNetwork(input_dim, L).to(device)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        # Compute scores using the scoring network\n",
    "        scores = self.scoring_network(input_sequence)  # input_sequence shape: [B, L, input_dim]\n",
    "\n",
    "        # Select top-k scores to generate indices\n",
    "        _, indices = torch.topk(scores, self.top_k, dim=-1)  # Selecting indices of top-k scores\n",
    "\n",
    "        # Create a mask for all positions\n",
    "        full_mask = torch.ones((self.B, self.L), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        # Update mask for top-k positions\n",
    "        batch_indices = torch.arange(self.B, device=self.device)[:, None]\n",
    "        full_mask[batch_indices, indices] = False\n",
    "\n",
    "        # Expand mask for all heads\n",
    "        mask = full_mask[:, None, :].expand(-1, self.H, -1)\n",
    "\n",
    "        # Mask should be of shape [B, H, L, L]\n",
    "        mask = mask.unsqueeze(2).expand(-1, -1, self.L, -1)\n",
    "\n",
    "        return mask\n",
    "\n",
    "class ScoringNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ScoringNetwork, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Using sigmoid to keep scores between 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
