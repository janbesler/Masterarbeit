{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# measuring ressources\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "from memory_profiler import profile\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
    "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
    "from eFormer.loss_function import CRPS, weighted_CRPS\n",
    "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
    "from eFormer.Dataloader import TimeSeriesDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "hyperparameters = {\n",
    "    'n_heads': 4,\n",
    "    'ProbabilisticModel': True,\n",
    "    # embeddings\n",
    "    'len_embedding': 64,\n",
    "    'batch_size': 512,\n",
    "    # general\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 72,\n",
    "    'patience': 7,\n",
    "    'dropout': 0.05,\n",
    "    'learning_rate': 6e-4,\n",
    "    'WeightDecay': 1e-1,\n",
    "    'train_epochs': 2,\n",
    "    'num_workers': 10,\n",
    "    'step_forecast': 6,\n",
    "    # benchmarks\n",
    "    'factor': 1,\n",
    "    'output_attention': True,\n",
    "    'd_model': 64,\n",
    "    'c_out': 6,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'activation': 'relu',\n",
    "    'd_ff': 1,\n",
    "    'distil': True,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Processing turbine files: 100%|██████████| 36/36 [00:13<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dictionary keys:\n",
      "dict_keys(['6', '4', '1', '3', '2', '5'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files: 0it [00:00, ?it/s]\n",
      "Processing turbine files: 100%|██████████| 84/84 [00:33<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dictionary keys:\n",
      "dict_keys(['07', '08', '02', '05', '06', '15', '10', '14', '01', '04', '11', '12', '13', '09'])\n"
     ]
    }
   ],
   "source": [
    "def reading_Windturbines(turbine_directory):\n",
    "\n",
    "    def safe_datetime_conversion(s):\n",
    "        try:\n",
    "            return pd.to_datetime(s)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "    def days_since_last_maintenance(row_date, maintenance_dates):\n",
    "        # Exclude None values from the maintenance_dates list before making comparisons\n",
    "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
    "        if not preceding_maintenance_dates:\n",
    "            return float('NaN')\n",
    "        last_maintenance_date = max(preceding_maintenance_dates)\n",
    "        delta = (row_date - last_maintenance_date).days\n",
    "        return delta\n",
    "\n",
    "    # Columns to keep\n",
    "    columns_turbine = [\n",
    "        '# Date and time',\n",
    "        'Wind speed (m/s)',\n",
    "        'Power (kW)'\n",
    "    ]\n",
    "    columns_status = [\n",
    "        'Timestamp end',\n",
    "        'IEC category'\n",
    "    ]\n",
    "\n",
    "    # Directory containing CSV files\n",
    "    directory = f'../data/Windturbinen/{turbine_directory}/'\n",
    "\n",
    "    # Dictionary to hold DataFrames for each turbine\n",
    "    turbine_dataframes = defaultdict(list)\n",
    "    status_lists = defaultdict(list)\n",
    "\n",
    "    # Get a list of CSV files in the directory\n",
    "    turbine_files = [f for f in os.listdir(directory) if f.startswith(f\"Turbine_Data_{turbine_directory}_\") and f.endswith(\".csv\")]\n",
    "    status_files = [f for f in os.listdir(directory) if f.startswith(f\"Status_{turbine_directory}_\") and f.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate through the status files\n",
    "    for filename in tqdm(status_files, desc='Processing status files'):\n",
    "        turbine_number = filename.split(\"_\")[2]\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
    "        df['Timestamp end'] = df['Timestamp end'].apply(safe_datetime_conversion)\n",
    "        maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].apply(lambda x: x.strftime('%Y-%m-%d') if not pd.isna(x) else None).unique()\n",
    "        status_lists[turbine_number].extend(maintenance_dates)\n",
    "\n",
    "    # Iterate through the turbine files\n",
    "    for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
    "        turbine_number = filename.split(\"_\")[3]\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
    "        df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "        maintenance_dates = [pd.to_datetime(date) for date in status_lists[turbine_number]]\n",
    "        #df['Days Since Maintenance'] = df['# Date and time'].apply(lambda row_date: days_since_last_maintenance(row_date, maintenance_dates))\n",
    "        turbine_dataframes[turbine_number].append(df)\n",
    "\n",
    "    # Concatenate the DataFrames for each turbine\n",
    "    for turbine_number, dfs in turbine_dataframes.items():\n",
    "        turbine_dataframes[turbine_number] = pd.concat(dfs)\n",
    "        turbine_dataframes[turbine_number].sort_values('# Date and time', inplace=True)\n",
    "        turbine_dataframes[turbine_number] = turbine_dataframes[turbine_number].reset_index(drop=True)\n",
    "        turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
    "        turbine_dataframes[turbine_number].index.names = [None]\n",
    "        turbine_dataframes[turbine_number] = turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1)\n",
    "\n",
    "    print(f\"dictionary keys: {len(turbine_dataframes.keys())}\")\n",
    "\n",
    "    return turbine_dataframes\n",
    "\n",
    "Kelmarsh_dict = reading_Windturbines('Kelmarsh')\n",
    "Penmanshiel_dict = reading_Windturbines('Penmanshiel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DataFrame: 6\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 4\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 1\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 3\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 2\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 5\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 07\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 08\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 02\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 05\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 06\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 15\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 10\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 14\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 01\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 04\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 11\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 12\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 13\n",
      " missing values interpolation & removal...\n",
      "Processing DataFrame: 09\n",
      " missing values interpolation & removal...\n"
     ]
    }
   ],
   "source": [
    "def check_missing_sequences(df, column='Power (kW)'):\n",
    "    sequences = []\n",
    "    current_sequence = 0\n",
    "    long_sequence_indices = []\n",
    "    start_index = None\n",
    "    \n",
    "    # Loop through the DataFrame to find missing values\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row[column]):\n",
    "            current_sequence += 1\n",
    "            if start_index is None:\n",
    "                start_index = i\n",
    "        else:\n",
    "            if current_sequence >= 19:\n",
    "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                long_sequence_indices.extend(sequence_indices)\n",
    "                # Mask these indices\n",
    "                df.loc[sequence_indices, column] = np.inf\n",
    "            if current_sequence > 0:\n",
    "                sequences.append(current_sequence)\n",
    "            current_sequence = 0\n",
    "            start_index = None\n",
    "    \n",
    "    # last sequence if it exists\n",
    "    if current_sequence > 0:\n",
    "        sequences.append(current_sequence)\n",
    "        if current_sequence >= 19:\n",
    "            sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "            long_sequence_indices.extend(sequence_indices)\n",
    "            # Mask these indices\n",
    "            df.loc[sequence_indices, column] = np.inf\n",
    "\n",
    "    # Interpolate the missing values (excluding the long sequences)\n",
    "    df[column] = df[column].replace(np.inf, np.nan).interpolate(method = 'linear')\n",
    "    \n",
    "    # Remove the rows corresponding to long sequences\n",
    "    df.drop(long_sequence_indices, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_data_frames(df_dict, dependent_var):\n",
    "    for key in df_dict.keys():\n",
    "        print(f\"Processing DataFrame: {key}\")\n",
    "                \n",
    "        # Check for missing sequences\n",
    "        print(\" missing values interpolation & removal...\")\n",
    "        check_missing_sequences(df_dict[key], dependent_var)\n",
    "\n",
    "process_data_frames(Kelmarsh_dict, 'Power (kW)')\n",
    "process_data_frames(Penmanshiel_dict, 'Power (kW)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Processing turbine files: 100%|██████████| 36/36 [00:14<00:00,  2.50it/s]\n",
      "Processing status files: 0it [00:00, ?it/s]\n",
      "Processing turbine files: 100%|██████████| 84/84 [00:35<00:00,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "class WindTurbineDataProcessor:\n",
    "    def __init__(self, turbine_directory, dependent_var='Power (kW)'):\n",
    "        self.directory = f'../data/Windturbinen/{turbine_directory}/'\n",
    "        self.dependent_var = dependent_var\n",
    "\n",
    "    def safe_datetime_conversion(self, s):\n",
    "        try:\n",
    "            return pd.to_datetime(s)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "    def days_since_last_maintenance(self, row_date, maintenance_dates):\n",
    "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
    "        if not preceding_maintenance_dates:\n",
    "            return float('NaN')\n",
    "        last_maintenance_date = max(preceding_maintenance_dates)\n",
    "        delta = (row_date - last_maintenance_date).days\n",
    "        return delta\n",
    "\n",
    "    def check_missing_sequences(self, df):\n",
    "        sequences = []\n",
    "        current_sequence = 0\n",
    "        long_sequence_indices = []\n",
    "        start_index = None\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isnull(row[self.dependent_var]):\n",
    "                current_sequence += 1\n",
    "                if start_index is None:\n",
    "                    start_index = i\n",
    "            else:\n",
    "                if current_sequence >= 19:\n",
    "                    sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                    long_sequence_indices.extend(sequence_indices)\n",
    "                    df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "                if current_sequence > 0:\n",
    "                    sequences.append(current_sequence)\n",
    "                current_sequence = 0\n",
    "                start_index = None\n",
    "        \n",
    "        if current_sequence > 0:\n",
    "            sequences.append(current_sequence)\n",
    "            if current_sequence >= 19:\n",
    "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                long_sequence_indices.extend(sequence_indices)\n",
    "                df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "\n",
    "        df[self.dependent_var] = df[self.dependent_var].replace(np.inf, np.nan).interpolate(method='linear')\n",
    "        df.drop(long_sequence_indices, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def process_and_load_data(self):\n",
    "        turbine_dataframes = defaultdict(list)\n",
    "        status_lists = defaultdict(list)\n",
    "\n",
    "        columns_turbine = ['# Date and time', 'Wind speed (m/s)', 'Power (kW)']\n",
    "        columns_status = ['Timestamp end', 'IEC category']\n",
    "\n",
    "        turbine_files = [f for f in os.listdir(self.directory) if f.startswith(\"Turbine_Data_\") and f.endswith(\".csv\")]\n",
    "        status_files = [f for f in os.listdir(self.directory) if f.startswith(\"Status_\") and f.endswith(\".csv\")]\n",
    "\n",
    "        for filename in tqdm(status_files, desc='Processing status files'):\n",
    "            turbine_number = filename.split(\"_\")[2]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
    "            df['Timestamp end'] = df['Timestamp end'].apply(self.safe_datetime_conversion)\n",
    "            maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].unique()\n",
    "            status_lists[turbine_number].extend(maintenance_dates)\n",
    "\n",
    "        for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
    "            turbine_number = filename.split(\"_\")[3]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
    "            df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "            turbine_dataframes[turbine_number].append(df)\n",
    "\n",
    "        for turbine_number, dfs in turbine_dataframes.items():\n",
    "            turbine_dataframes[turbine_number] = pd.concat(dfs).sort_values('# Date and time').reset_index(drop=True)\n",
    "            turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
    "            turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1, inplace=True)\n",
    "            self.check_missing_sequences(turbine_dataframes[turbine_number])\n",
    "\n",
    "        gc.collect()\n",
    "        return turbine_dataframes\n",
    "\n",
    "def process_wind_turbines(turbine_directory, dependent_var):\n",
    "    processor = WindTurbineDataProcessor(turbine_directory, dependent_var)\n",
    "    return processor.process_and_load_data()\n",
    "\n",
    "Kelmarsh_dict = process_wind_turbines('Kelmarsh', 'Power (kW)')\n",
    "Penmanshiel_dict = process_wind_turbines('Penmanshiel', 'Power (kW)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataLoader:\n",
    "    def __init__(self, df_dict_1, df_dict_2, hyperparameters):\n",
    "        self.df_dict_1 = df_dict_1\n",
    "        self.df_dict_2 = df_dict_2\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.train_datasets = []\n",
    "        self.test_datasets = []\n",
    "        self.eval_datasets = []\n",
    "\n",
    "    def process_datasets(self, dataframe_dict):\n",
    "        for key, df in dataframe_dict.items():\n",
    "            processor = TimeSeriesDataProcessor(\n",
    "                dataframe=df,\n",
    "                forecast=1,\n",
    "                look_back=self.hyperparameters['seq_len'],\n",
    "                batch_size=self.hyperparameters['batch_size']\n",
    "            )\n",
    "            train_dataset, test_dataset, eval_dataset = processor.prepare_datasets()\n",
    "            self.train_datasets.append(train_dataset)\n",
    "            self.test_datasets.append(test_dataset)\n",
    "            self.eval_datasets.append(eval_dataset)\n",
    "            # Invoke garbage collection after processing each dataframe\n",
    "            gc.collect()\n",
    "\n",
    "    def create_concat_datasets(self):\n",
    "        self.process_datasets(self.df_dict_1)\n",
    "        self.process_datasets(self.df_dict_2)\n",
    "\n",
    "        # Concatenating the datasets\n",
    "        self.concat_train_dataset = ConcatDataset(self.train_datasets)\n",
    "        self.concat_test_dataset = ConcatDataset(self.test_datasets)\n",
    "        self.concat_eval_dataset = ConcatDataset(self.eval_datasets)\n",
    "        \n",
    "        # Clear the lists to free up memory\n",
    "        self.train_datasets.clear()\n",
    "        self.test_datasets.clear()\n",
    "        self.eval_datasets.clear()\n",
    "        \n",
    "        # Invoke garbage collection after clearing the lists\n",
    "        gc.collect()\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        self.create_concat_datasets()\n",
    "\n",
    "        # Creating the data loaders\n",
    "        self.train_loader = DataLoader(self.concat_train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
    "        self.test_loader = DataLoader(self.concat_test_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        self.eval_loader = DataLoader(self.concat_eval_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Invoke garbage collection after dataloaders are created\n",
    "        gc.collect()\n",
    "\n",
    "        return self.train_loader, self.test_loader, self.eval_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the UnifiedDataLoader class\n",
    "loader = UnifiedDataLoader(\n",
    "    df_dict_1=Kelmarsh_dict,\n",
    "    df_dict_2=Penmanshiel_dict,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "# Use the new method to get the data loaders\n",
    "train_loader, test_loader, eval_loader = loader.create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 144])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    features, labels = batch\n",
    "    print(features.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
