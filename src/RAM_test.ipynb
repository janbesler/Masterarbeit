{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# measuring ressources\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "from memory_profiler import profile\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
    "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
    "from eFormer.loss_function import CRPS, weighted_CRPS\n",
    "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
    "from eFormer.Dataloader import TimeSeriesDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "hyperparameters = {\n",
    "    'n_heads': 4,\n",
    "    'ProbabilisticModel': True,\n",
    "    # embeddings\n",
    "    'len_embedding': 64,\n",
    "    'batch_size': 512,\n",
    "    # general\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 72,\n",
    "    'patience': 7,\n",
    "    'dropout': 0.05,\n",
    "    'learning_rate': 6e-4,\n",
    "    'WeightDecay': 1e-1,\n",
    "    'train_epochs': 2,\n",
    "    'num_workers': 10,\n",
    "    'step_forecast': 6,\n",
    "    # benchmarks\n",
    "    'factor': 1,\n",
    "    'output_attention': True,\n",
    "    'd_model': 64,\n",
    "    'c_out': 6,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'activation': 'relu',\n",
    "    'd_ff': 1,\n",
    "    'distil': True,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Processing turbine files: 100%|██████████| 36/36 [00:13<00:00,  2.59it/s]\n",
      "Processing status files: 0it [00:00, ?it/s]\n",
      "Processing turbine files: 100%|██████████| 84/84 [00:33<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "class WindTurbineDataProcessor:\n",
    "    def __init__(self, turbine_directory, dependent_var='Power (kW)'):\n",
    "        self.directory = f'../data/Windturbinen/{turbine_directory}/'\n",
    "        self.dependent_var = dependent_var\n",
    "\n",
    "    def safe_datetime_conversion(self, s):\n",
    "        try:\n",
    "            return pd.to_datetime(s)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "    def days_since_last_maintenance(self, row_date, maintenance_dates):\n",
    "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
    "        if not preceding_maintenance_dates:\n",
    "            return float('NaN')\n",
    "        last_maintenance_date = max(preceding_maintenance_dates)\n",
    "        delta = (row_date - last_maintenance_date).days\n",
    "        return delta\n",
    "\n",
    "    def check_missing_sequences(self, df):\n",
    "        sequences = []\n",
    "        current_sequence = 0\n",
    "        long_sequence_indices = []\n",
    "        start_index = None\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isnull(row[self.dependent_var]):\n",
    "                current_sequence += 1\n",
    "                if start_index is None:\n",
    "                    start_index = i\n",
    "            else:\n",
    "                if current_sequence >= 19:\n",
    "                    sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                    long_sequence_indices.extend(sequence_indices)\n",
    "                    df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "                if current_sequence > 0:\n",
    "                    sequences.append(current_sequence)\n",
    "                current_sequence = 0\n",
    "                start_index = None\n",
    "        \n",
    "        if current_sequence > 0:\n",
    "            sequences.append(current_sequence)\n",
    "            if current_sequence >= 19:\n",
    "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                long_sequence_indices.extend(sequence_indices)\n",
    "                df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "\n",
    "        df[self.dependent_var] = df[self.dependent_var].replace(np.inf, np.nan).interpolate(method='linear')\n",
    "        df.drop(long_sequence_indices, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def process_and_load_data(self):\n",
    "        turbine_dataframes = defaultdict(list)\n",
    "        status_lists = defaultdict(list)\n",
    "\n",
    "        columns_turbine = ['# Date and time', 'Wind speed (m/s)', 'Power (kW)']\n",
    "        columns_status = ['Timestamp end', 'IEC category']\n",
    "\n",
    "        turbine_files = [f for f in os.listdir(self.directory) if f.startswith(\"Turbine_Data_\") and f.endswith(\".csv\")]\n",
    "        status_files = [f for f in os.listdir(self.directory) if f.startswith(\"Status_\") and f.endswith(\".csv\")]\n",
    "\n",
    "        for filename in tqdm(status_files, desc='Processing status files'):\n",
    "            turbine_number = filename.split(\"_\")[2]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
    "            df['Timestamp end'] = df['Timestamp end'].apply(self.safe_datetime_conversion)\n",
    "            maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].unique()\n",
    "            status_lists[turbine_number].extend(maintenance_dates)\n",
    "\n",
    "        for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
    "            turbine_number = filename.split(\"_\")[3]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
    "            df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "            turbine_dataframes[turbine_number].append(df)\n",
    "\n",
    "        for turbine_number, dfs in turbine_dataframes.items():\n",
    "            turbine_dataframes[turbine_number] = pd.concat(dfs).sort_values('# Date and time').reset_index(drop=True)\n",
    "            turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
    "            turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1, inplace=True)\n",
    "            self.check_missing_sequences(turbine_dataframes[turbine_number])\n",
    "\n",
    "        gc.collect()\n",
    "        return turbine_dataframes\n",
    "\n",
    "def process_wind_turbines(turbine_directory, dependent_var):\n",
    "    processor = WindTurbineDataProcessor(turbine_directory, dependent_var)\n",
    "    return processor.process_and_load_data()\n",
    "\n",
    "Kelmarsh_dict = process_wind_turbines('Kelmarsh', 'Power (kW)')\n",
    "Penmanshiel_dict = process_wind_turbines('Penmanshiel', 'Power (kW)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataLoader:\n",
    "    def __init__(self, df_dict_1, df_dict_2, hyperparameters):\n",
    "        self.df_dict_1 = df_dict_1\n",
    "        self.df_dict_2 = df_dict_2\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.train_datasets = []\n",
    "        self.test_datasets = []\n",
    "        self.eval_datasets = []\n",
    "\n",
    "    def process_datasets(self, dataframe_dict):\n",
    "        for key, df in dataframe_dict.items():\n",
    "            processor = TimeSeriesDataProcessor(\n",
    "                dataframe=df,\n",
    "                forecast=1,\n",
    "                look_back=self.hyperparameters['seq_len'],\n",
    "                batch_size=self.hyperparameters['batch_size']\n",
    "            )\n",
    "            train_dataset, test_dataset, eval_dataset = processor.prepare_datasets()\n",
    "            self.train_datasets.append(train_dataset)\n",
    "            self.test_datasets.append(test_dataset)\n",
    "            self.eval_datasets.append(eval_dataset)\n",
    "            # Invoke garbage collection after processing each dataframe\n",
    "            gc.collect()\n",
    "\n",
    "    def create_concat_datasets(self):\n",
    "        self.process_datasets(self.df_dict_1)\n",
    "        self.process_datasets(self.df_dict_2)\n",
    "\n",
    "        # Concatenating the datasets\n",
    "        self.concat_train_dataset = ConcatDataset(self.train_datasets)\n",
    "        self.concat_test_dataset = ConcatDataset(self.test_datasets)\n",
    "        self.concat_eval_dataset = ConcatDataset(self.eval_datasets)\n",
    "        \n",
    "        # Clear the lists to free up memory\n",
    "        self.train_datasets.clear()\n",
    "        self.test_datasets.clear()\n",
    "        self.eval_datasets.clear()\n",
    "        \n",
    "        # Invoke garbage collection after clearing the lists\n",
    "        gc.collect()\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        self.create_concat_datasets()\n",
    "\n",
    "        # Creating the data loaders\n",
    "        self.train_loader = DataLoader(self.concat_train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
    "        self.test_loader = DataLoader(self.concat_test_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        self.eval_loader = DataLoader(self.concat_eval_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Invoke garbage collection after dataloaders are created\n",
    "        gc.collect()\n",
    "\n",
    "        return self.train_loader, self.test_loader, self.eval_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the UnifiedDataLoader class\n",
    "loader = UnifiedDataLoader(\n",
    "    df_dict_1=Kelmarsh_dict,\n",
    "    df_dict_2=Penmanshiel_dict,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "# Use the new method to get the data loaders\n",
    "train_loader, test_loader, eval_loader = loader.create_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 144])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    features, labels = batch\n",
    "    print(features.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
