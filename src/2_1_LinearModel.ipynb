{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGGnr_VVI6xP"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfuJYq4FI3uV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/eFormer/src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea4Z94SMJI9G"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9uzmFD_rJGrr",
        "outputId": "32654ff6-ae82-4b2d-9121-f58241eb516c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=e75503de958b7639b3c1b98ea0b2f9c9f036c8c9ea19dda227ab46ab36a10c3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil\n",
        "!pip install memory_profiler\n",
        "\n",
        "%load_ext memory_profiler\n",
        "\n",
        "# standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from math import sqrt\n",
        "import time\n",
        "\n",
        "# reading data\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.fft import rfft, irfft, fftn, ifftn\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# resources\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "import threading\n",
        "import gc\n",
        "\n",
        "# eFormer\n",
        "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
        "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
        "from eFormer.loss_function import CRPS, weighted_CRPS\n",
        "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
        "from eFormer.Dataloader import TimeSeriesDataProcessor\n",
        "\n",
        "# transformer Benchmarks\n",
        "from Benchmarks.Benchmarks import VanillaTransformer, Informer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRJdVXHKJPJu"
      },
      "source": [
        "## Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Yzp_C0ojan"
      },
      "outputs": [],
      "source": [
        "class WindTurbineDataProcessor:\n",
        "    def __init__(self, turbine_directory, dependent_var='Power (kW)'):\n",
        "        self.directory = f'../data/Windturbinen/{turbine_directory}/'\n",
        "        self.dependent_var = dependent_var\n",
        "\n",
        "    def safe_datetime_conversion(self, s):\n",
        "        try:\n",
        "            return pd.to_datetime(s)\n",
        "        except:\n",
        "            return pd.NaT\n",
        "\n",
        "    def days_since_last_maintenance(self, row_date, maintenance_dates):\n",
        "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
        "        if not preceding_maintenance_dates:\n",
        "            return float('NaN')\n",
        "        last_maintenance_date = max(preceding_maintenance_dates)\n",
        "        delta = (row_date - last_maintenance_date).days\n",
        "        return delta\n",
        "\n",
        "    def check_missing_sequences(self, df):\n",
        "        sequences = []\n",
        "        current_sequence = 0\n",
        "        long_sequence_indices = []\n",
        "        start_index = None\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            if pd.isnull(row[self.dependent_var]):\n",
        "                current_sequence += 1\n",
        "                if start_index is None:\n",
        "                    start_index = i\n",
        "            else:\n",
        "                if current_sequence >= 19:\n",
        "                    sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
        "                    long_sequence_indices.extend(sequence_indices)\n",
        "                    df.loc[sequence_indices, self.dependent_var] = np.inf\n",
        "                if current_sequence > 0:\n",
        "                    sequences.append(current_sequence)\n",
        "                current_sequence = 0\n",
        "                start_index = None\n",
        "\n",
        "        if current_sequence > 0:\n",
        "            sequences.append(current_sequence)\n",
        "            if current_sequence >= 19:\n",
        "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
        "                long_sequence_indices.extend(sequence_indices)\n",
        "                df.loc[sequence_indices, self.dependent_var] = np.inf\n",
        "\n",
        "        df[self.dependent_var] = df[self.dependent_var].replace(np.inf, np.nan).interpolate(method='linear')\n",
        "        df.drop(long_sequence_indices, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def process_and_load_data(self):\n",
        "        turbine_dataframes = defaultdict(list)\n",
        "        status_lists = defaultdict(list)\n",
        "\n",
        "        columns_turbine = ['# Date and time', 'Wind speed (m/s)', 'Power (kW)']\n",
        "        columns_status = ['Timestamp end', 'IEC category']\n",
        "\n",
        "        turbine_files = [f for f in os.listdir(self.directory) if f.startswith(\"Turbine_Data_\") and f.endswith(\".csv\")]\n",
        "        status_files = [f for f in os.listdir(self.directory) if f.startswith(\"Status_\") and f.endswith(\".csv\")]\n",
        "\n",
        "        for filename in tqdm(status_files, desc='Processing status files'):\n",
        "            turbine_number = filename.split(\"_\")[2]\n",
        "            filepath = os.path.join(self.directory, filename)\n",
        "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
        "            df['Timestamp end'] = df['Timestamp end'].apply(self.safe_datetime_conversion)\n",
        "            maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].unique()\n",
        "            status_lists[turbine_number].extend(maintenance_dates)\n",
        "\n",
        "        for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
        "            turbine_number = filename.split(\"_\")[3]\n",
        "            filepath = os.path.join(self.directory, filename)\n",
        "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
        "            df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
        "            turbine_dataframes[turbine_number].append(df)\n",
        "\n",
        "        for turbine_number, dfs in turbine_dataframes.items():\n",
        "            turbine_dataframes[turbine_number] = pd.concat(dfs).sort_values('# Date and time').reset_index(drop=True)\n",
        "            turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
        "            turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1, inplace=True)\n",
        "            self.check_missing_sequences(turbine_dataframes[turbine_number])\n",
        "\n",
        "        gc.collect()\n",
        "        return turbine_dataframes\n",
        "\n",
        "def process_wind_turbines(turbine_directory, dependent_var):\n",
        "    processor = WindTurbineDataProcessor(turbine_directory, dependent_var)\n",
        "    return processor.process_and_load_data()\n",
        "\n",
        "Kelmarsh_dict = process_wind_turbines('Kelmarsh', 'Power (kW)')\n",
        "Penmanshiel_dict = process_wind_turbines('Penmanshiel', 'Power (kW)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_FM2MMYJVHN"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGvV8eS7JW6n"
      },
      "outputs": [],
      "source": [
        "# set global parameters\n",
        "hyperparameters = {\n",
        "    'n_heads': 4,\n",
        "    'ProbabilisticModel': False,\n",
        "    # embeddings\n",
        "    'len_embedding': 64,\n",
        "    'batch_size': 512,\n",
        "    # general\n",
        "    'pred_len': 1,\n",
        "    'seq_len': 576,\n",
        "    'patience': 7,\n",
        "    'dropout': 0.05,\n",
        "    'learning_rate': 6e-4,\n",
        "    'WeightDecay': 1e-1,\n",
        "    'train_epochs': 100,\n",
        "    'num_workers': 10,\n",
        "    'step_forecast': 144,\n",
        "    # benchmarks\n",
        "    'factor': 1,\n",
        "    'output_attention': True,\n",
        "    'd_model': 64,\n",
        "    'c_out': 6,\n",
        "    'e_layers': 2,\n",
        "    'd_layers': 2,\n",
        "    'activation': 'relu',\n",
        "    'd_ff': 1,\n",
        "    'distil': True,\n",
        "    }\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # Use the first GPU available\n",
        "    print(\"Running on GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Fallback to CPU if no GPU is available\n",
        "    print(\"Running on CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CldfKwDhQ76o"
      },
      "outputs": [],
      "source": [
        "def check_system_conditions():\n",
        "    # Get CPU usage for each core\n",
        "    cpu_percent = round(psutil.cpu_percent(), 4)\n",
        "\n",
        "    # Get memory information\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    memory_used_gb = round(memory_info.used / (1024 ** 3), 4)\n",
        "\n",
        "    # Get GPU information\n",
        "    try:\n",
        "      gpu_info = GPUtil.getGPUs()[0]\n",
        "      gpu_memory_used_gb = round(gpu_info.memoryUsed / 1024, 4)\n",
        "    except IndexError:\n",
        "      # If no GPU is found, set variables to None\n",
        "      gpu_memory_used_gb = None\n",
        "\n",
        "    # Collect data in a dictionary\n",
        "    comp_usage = {\n",
        "        'CPU Usage': cpu_percent,\n",
        "        'Memory Usage (GB)': memory_used_gb,\n",
        "        'GPU Usage (GB)': gpu_memory_used_gb\n",
        "    }\n",
        "\n",
        "    return comp_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azCTAMIgn4lQ"
      },
      "outputs": [],
      "source": [
        "class UnifiedDataLoader:\n",
        "    def __init__(self, df_dict_1, df_dict_2, hyperparameters):\n",
        "        self.df_dict_1 = df_dict_1\n",
        "        self.df_dict_2 = df_dict_2\n",
        "        self.hyperparameters = hyperparameters\n",
        "        self.train_datasets = []\n",
        "        self.test_datasets = []\n",
        "        self.eval_datasets = []\n",
        "\n",
        "    def process_datasets(self, dataframe_dict):\n",
        "        for key, df in dataframe_dict.items():\n",
        "            processor = TimeSeriesDataProcessor(\n",
        "                dataframe=df,\n",
        "                forecast=1,\n",
        "                look_back=self.hyperparameters['seq_len'],\n",
        "                batch_size=self.hyperparameters['batch_size']\n",
        "            )\n",
        "            train_dataset, test_dataset, eval_dataset = processor.prepare_datasets()\n",
        "            self.train_datasets.append(train_dataset)\n",
        "            self.test_datasets.append(test_dataset)\n",
        "            self.eval_datasets.append(eval_dataset)\n",
        "            # Invoke garbage collection after processing each dataframe\n",
        "            gc.collect()\n",
        "\n",
        "    def create_concat_datasets(self):\n",
        "        self.process_datasets(self.df_dict_1)\n",
        "        self.process_datasets(self.df_dict_2)\n",
        "\n",
        "        # Concatenating the datasets\n",
        "        self.concat_train_dataset = ConcatDataset(self.train_datasets)\n",
        "        self.concat_test_dataset = ConcatDataset(self.test_datasets)\n",
        "        self.concat_eval_dataset = ConcatDataset(self.eval_datasets)\n",
        "\n",
        "        # Clear the lists to free up memory\n",
        "        self.train_datasets.clear()\n",
        "        self.test_datasets.clear()\n",
        "        self.eval_datasets.clear()\n",
        "\n",
        "        # Invoke garbage collection after clearing the lists\n",
        "        gc.collect()\n",
        "\n",
        "    def create_dataloaders(self):\n",
        "        self.create_concat_datasets()\n",
        "\n",
        "        # Creating the data loaders\n",
        "        self.train_loader = DataLoader(self.concat_train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
        "        self.test_loader = DataLoader(self.concat_test_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
        "        self.eval_loader = DataLoader(self.concat_eval_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
        "\n",
        "        # Invoke garbage collection after dataloaders are created\n",
        "        gc.collect()\n",
        "\n",
        "        return self.train_loader, self.test_loader, self.eval_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX0Oxtkr2uGq"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            #if self.verbose:\n",
        "                #print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
        "            self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekYLVpURNenf"
      },
      "source": [
        "# Linear Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EhJGxAYNlfQ"
      },
      "outputs": [],
      "source": [
        "# Single Layer Perceptron Model\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        weights = self.linear.weight.data\n",
        "\n",
        "        return x.squeeze(1), weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAfuLfcnLdye"
      },
      "outputs": [],
      "source": [
        "def multi_step_Linear(model, initial_input, steps):\n",
        "    \"\"\"\n",
        "    Perform a recurrent multi-step forecast using the provided model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained model for prediction.\n",
        "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
        "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
        "    - steps: The number of steps to forecast ahead.\n",
        "\n",
        "    Returns:\n",
        "    - A list containing the forecasted values for each step ahead.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    forecasted_steps = []\n",
        "    current_input = initial_input\n",
        "    model_2 = LinearModel(\n",
        "        input_size=(hyperparameters['seq_len']),\n",
        "        output_size=hyperparameters['pred_len'],\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(steps):\n",
        "            # split tensor\n",
        "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
        "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
        "            # forecast wind\n",
        "            first_half_pred, _ = model_2(first_half)\n",
        "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred.unsqueeze(1)), dim=1)\n",
        "            # update input\n",
        "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
        "            # forecast output\n",
        "            prediction, weights = model(current_input)\n",
        "            updated_second_half = torch.cat((second_half[:, 1:], prediction.unsqueeze(1)), dim=1)\n",
        "\n",
        "            # create new tensor\n",
        "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
        "\n",
        "            # store values\n",
        "            forecasted_steps.append(prediction)\n",
        "\n",
        "    return torch.Tensor(forecasted_steps[-1]).unsqueeze(1), weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gk6I1QzlNggb",
        "outputId": "dcc79ce8-02e4-47e5-8d42-1a708a0982ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('MSELoss', 1)\n",
            "Epoch 1 / 100 with Loss: 35671.12433\n",
            "Epoch 2 / 100 with Loss: 24695.945758\n",
            "Epoch 3 / 100 with Loss: 24676.965222\n",
            "Epoch 4 / 100 with Loss: 24677.352229\n",
            "Epoch 5 / 100 with Loss: 24680.732053\n",
            "Epoch 6 / 100 with Loss: 24684.533916\n",
            "Epoch 7 / 100 with Loss: 24670.046699\n",
            "Epoch 8 / 100 with Loss: 24680.85437\n",
            "Epoch 9 / 100 with Loss: 24680.208228\n",
            "Epoch 10 / 100 with Loss: 24684.978052\n",
            "Epoch 11 / 100 with Loss: 24682.64763\n",
            "Epoch 12 / 100 with Loss: 24673.707252\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 840817.220944\n",
            "('MSELoss', 6)\n",
            "Epoch 1 / 100 with Loss: 39747.257347\n",
            "Epoch 2 / 100 with Loss: 24727.365943\n",
            "Epoch 3 / 100 with Loss: 24674.072217\n",
            "Epoch 4 / 100 with Loss: 24678.741662\n",
            "Epoch 5 / 100 with Loss: 24677.069597\n",
            "Epoch 6 / 100 with Loss: 24683.27322\n",
            "Epoch 7 / 100 with Loss: 24672.66039\n",
            "Epoch 8 / 100 with Loss: 24676.213053\n",
            "Epoch 9 / 100 with Loss: 24679.201524\n",
            "Epoch 10 / 100 with Loss: 24677.141768\n",
            "Epoch 11 / 100 with Loss: 24679.541201\n",
            "Epoch 12 / 100 with Loss: 24665.032305\n",
            "Epoch 13 / 100 with Loss: 24677.966562\n",
            "Epoch 14 / 100 with Loss: 24667.39652\n",
            "Epoch 15 / 100 with Loss: 24673.976537\n",
            "Epoch 16 / 100 with Loss: 24674.966292\n",
            "Epoch 17 / 100 with Loss: 24678.820137\n",
            "Epoch 18 / 100 with Loss: 24679.545942\n",
            "Epoch 19 / 100 with Loss: 24672.649739\n",
            "Epoch 20 / 100 with Loss: 24676.510984\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 849046.452007\n",
            "('MSELoss', 72)\n",
            "Epoch 1 / 100 with Loss: 32561.928891\n",
            "Epoch 2 / 100 with Loss: 25034.525324\n",
            "Epoch 3 / 100 with Loss: 25026.301241\n",
            "Epoch 4 / 100 with Loss: 25010.858232\n",
            "Epoch 5 / 100 with Loss: 24989.156458\n",
            "Epoch 6 / 100 with Loss: 25003.19153\n",
            "Epoch 7 / 100 with Loss: 25005.929176\n",
            "Epoch 8 / 100 with Loss: 25009.718989\n",
            "Epoch 9 / 100 with Loss: 25026.176202\n",
            "Epoch 10 / 100 with Loss: 25018.989057\n",
            "Epoch 11 / 100 with Loss: 25051.338036\n",
            "Epoch 12 / 100 with Loss: 25026.418492\n",
            "Epoch 13 / 100 with Loss: 25005.725572\n",
            "Epoch 14 / 100 with Loss: 25024.907597\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 842904.372894\n",
            "('MSELoss', 144)\n",
            "Epoch 1 / 100 with Loss: 34801.157137\n",
            "Epoch 2 / 100 with Loss: 25301.307935\n",
            "Epoch 3 / 100 with Loss: 25218.248569\n",
            "Epoch 4 / 100 with Loss: 25258.260137\n",
            "Epoch 5 / 100 with Loss: 25201.735876\n",
            "Epoch 6 / 100 with Loss: 25234.02418\n",
            "Epoch 7 / 100 with Loss: 25227.365259\n",
            "Epoch 8 / 100 with Loss: 25215.58061\n",
            "Epoch 9 / 100 with Loss: 25230.662896\n",
            "Epoch 10 / 100 with Loss: 25221.179515\n",
            "Epoch 11 / 100 with Loss: 25183.976095\n",
            "Epoch 12 / 100 with Loss: 25232.007498\n",
            "Epoch 13 / 100 with Loss: 25226.35816\n",
            "Epoch 14 / 100 with Loss: 25223.492209\n",
            "Epoch 15 / 100 with Loss: 25246.143227\n",
            "Epoch 16 / 100 with Loss: 25203.312053\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 845536.728924\n",
            "('L1Loss', 1)\n",
            "Epoch 1 / 100 with Loss: 102.349246\n",
            "Epoch 2 / 100 with Loss: 93.673077\n",
            "Epoch 3 / 100 with Loss: 93.614376\n",
            "Epoch 4 / 100 with Loss: 93.624633\n",
            "Epoch 5 / 100 with Loss: 93.611729\n",
            "Epoch 6 / 100 with Loss: 93.584428\n",
            "Epoch 7 / 100 with Loss: 93.596242\n",
            "Epoch 8 / 100 with Loss: 93.617448\n",
            "Epoch 9 / 100 with Loss: 93.595081\n",
            "Epoch 10 / 100 with Loss: 93.621093\n",
            "Epoch 11 / 100 with Loss: 93.600521\n",
            "Epoch 12 / 100 with Loss: 93.59178\n",
            "Epoch 13 / 100 with Loss: 93.603982\n",
            "Epoch 14 / 100 with Loss: 93.592684\n",
            "Epoch 15 / 100 with Loss: 93.626735\n",
            "Epoch 16 / 100 with Loss: 93.590647\n",
            "Epoch 17 / 100 with Loss: 93.59161\n",
            "Epoch 18 / 100 with Loss: 93.594856\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 706.808781\n",
            "('L1Loss', 6)\n",
            "Epoch 1 / 100 with Loss: 107.208622\n",
            "Epoch 2 / 100 with Loss: 93.697253\n",
            "Epoch 3 / 100 with Loss: 93.614157\n",
            "Epoch 4 / 100 with Loss: 93.61794\n",
            "Epoch 5 / 100 with Loss: 93.61443\n",
            "Epoch 6 / 100 with Loss: 93.618382\n",
            "Epoch 7 / 100 with Loss: 93.605284\n",
            "Epoch 8 / 100 with Loss: 93.593764\n",
            "Epoch 9 / 100 with Loss: 93.588939\n",
            "Epoch 10 / 100 with Loss: 93.601532\n",
            "Epoch 11 / 100 with Loss: 93.615856\n",
            "Epoch 12 / 100 with Loss: 93.603291\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 707.828784\n",
            "('L1Loss', 72)\n",
            "Epoch 1 / 100 with Loss: 107.820048\n",
            "Epoch 2 / 100 with Loss: 94.844398\n",
            "Epoch 3 / 100 with Loss: 94.68878\n",
            "Epoch 4 / 100 with Loss: 94.673872\n",
            "Epoch 5 / 100 with Loss: 94.649515\n",
            "Epoch 6 / 100 with Loss: 94.608473\n",
            "Epoch 7 / 100 with Loss: 94.670489\n",
            "Epoch 8 / 100 with Loss: 94.610139\n",
            "Epoch 9 / 100 with Loss: 94.640515\n",
            "Epoch 10 / 100 with Loss: 94.668529\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 705.611117\n",
            "('L1Loss', 144)\n",
            "Epoch 1 / 100 with Loss: 110.363907\n",
            "Epoch 2 / 100 with Loss: 96.219678\n",
            "Epoch 3 / 100 with Loss: 95.923592\n",
            "Epoch 4 / 100 with Loss: 95.920122\n",
            "Epoch 5 / 100 with Loss: 95.792546\n",
            "Epoch 6 / 100 with Loss: 95.899356\n",
            "Epoch 7 / 100 with Loss: 95.939521\n",
            "Epoch 8 / 100 with Loss: 95.827842\n",
            "Epoch 9 / 100 with Loss: 96.003368\n",
            "Epoch 10 / 100 with Loss: 95.887991\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 705.363175\n",
            "('CRPS', 1)\n",
            "Epoch 1 / 100 with Loss: 37.309645\n",
            "Epoch 2 / 100 with Loss: 1.352436\n",
            "Epoch 3 / 100 with Loss: 1.124874\n",
            "Epoch 4 / 100 with Loss: 1.145658\n",
            "Epoch 5 / 100 with Loss: 1.202792\n",
            "Epoch 6 / 100 with Loss: 1.020188\n",
            "Epoch 7 / 100 with Loss: 1.104826\n",
            "Epoch 8 / 100 with Loss: 0.981694\n",
            "Epoch 9 / 100 with Loss: 1.523174\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 0.576077\n",
            "('CRPS', 6)\n",
            "Epoch 1 / 100 with Loss: 29.631712\n",
            "Epoch 2 / 100 with Loss: 1.215197\n",
            "Epoch 3 / 100 with Loss: 1.267256\n",
            "Epoch 4 / 100 with Loss: 1.068322\n",
            "Epoch 5 / 100 with Loss: 1.25738\n",
            "Epoch 6 / 100 with Loss: 1.017279\n",
            "Epoch 7 / 100 with Loss: 1.150409\n",
            "Epoch 8 / 100 with Loss: 0.988108\n",
            "Epoch 9 / 100 with Loss: 1.017764\n",
            "Epoch 10 / 100 with Loss: 1.131888\n",
            "Epoch 11 / 100 with Loss: 1.187146\n",
            "Epoch 12 / 100 with Loss: 1.053249\n",
            "Epoch 13 / 100 with Loss: 1.016527\n",
            "Epoch 14 / 100 with Loss: 1.163967\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 0.874778\n",
            "('CRPS', 72)\n",
            "Epoch 1 / 100 with Loss: 38.543299\n",
            "Epoch 2 / 100 with Loss: 11.279447\n",
            "Epoch 3 / 100 with Loss: 11.977857\n",
            "Epoch 4 / 100 with Loss: 11.690317\n",
            "Epoch 5 / 100 with Loss: 10.901148\n",
            "Epoch 6 / 100 with Loss: 9.401088\n",
            "Epoch 7 / 100 with Loss: 10.77788\n",
            "Epoch 8 / 100 with Loss: 13.566019\n",
            "Epoch 9 / 100 with Loss: 12.348189\n",
            "Epoch 10 / 100 with Loss: 9.709986\n",
            "Epoch 11 / 100 with Loss: 11.6529\n",
            "Epoch 12 / 100 with Loss: 15.040256\n",
            "Epoch 13 / 100 with Loss: 9.475283\n",
            "Epoch 14 / 100 with Loss: 9.26349\n",
            "Epoch 15 / 100 with Loss: 11.356939\n",
            "Epoch 16 / 100 with Loss: 10.13194\n",
            "Epoch 17 / 100 with Loss: 8.037491\n",
            "Epoch 18 / 100 with Loss: 10.580305\n",
            "Epoch 19 / 100 with Loss: 13.970926\n",
            "Early stopping\n",
            "\n",
            " Model test loss: 0.665098\n",
            "('CRPS', 144)\n",
            "Epoch 1 / 100 with Loss: 87.748303\n",
            "Epoch 2 / 100 with Loss: 32.919286\n",
            "Epoch 3 / 100 with Loss: 49.739093\n",
            "Epoch 4 / 100 with Loss: 52.23979\n",
            "Epoch 5 / 100 with Loss: 37.912602\n",
            "Epoch 6 / 100 with Loss: 33.229173\n",
            "Epoch 7 / 100 with Loss: 61.517034\n",
            "Epoch 8 / 100 with Loss: 25.476038\n"
          ]
        }
      ],
      "source": [
        "def run_experiment(loss_function_name, hyperparameters):\n",
        "    # Adjust loss function based on the input argument\n",
        "    if loss_function_name == 'CRPS':\n",
        "        loss_fn = CRPS()  # Assuming CRPS is a defined class/function\n",
        "    elif loss_function_name == 'MSELoss':\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_function_name == 'L1Loss':\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_function_name == 'CrossEntropyLoss':\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss function: {loss_function_name}\")\n",
        "\n",
        "    hyperparameters['step_forecast'] = step_forecast\n",
        "\n",
        "    # initiate model etc\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=hyperparameters['patience'],\n",
        "        verbose=True\n",
        "        )\n",
        "    model = LinearModel(\n",
        "        input_size=(hyperparameters['seq_len'] * 2),\n",
        "        output_size=hyperparameters['pred_len']\n",
        "    ).to(device)\n",
        "    optimizer = AdamW(\n",
        "        params = model.parameters(),\n",
        "        lr=hyperparameters['learning_rate'],\n",
        "        weight_decay=hyperparameters['WeightDecay']\n",
        "        )\n",
        "\n",
        "    num_epochs = hyperparameters['train_epochs']\n",
        "\n",
        "    # function to run monitoring in a separate thread\n",
        "    def monitor_system_usage(every_n_seconds=1, keep_running=lambda: True, results_list=[]):\n",
        "        while keep_running():\n",
        "            comp_usage = check_system_conditions()\n",
        "            results_list.append(comp_usage)\n",
        "            time.sleep(every_n_seconds)\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    system_usage_results = []\n",
        "    training_time = []\n",
        "\n",
        "    # Define a lambda function to control the monitoring loop\n",
        "    keep_monitoring = lambda: keep_monitoring_flag\n",
        "    keep_monitoring_flag = True # Initialize the flag before starting training\n",
        "\n",
        "    # Start the monitoring thread\n",
        "    monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
        "    monitor_thread.start()\n",
        "\n",
        "    # Instantiate the UnifiedDataLoader class\n",
        "    loader = UnifiedDataLoader(\n",
        "        df_dict_1=Kelmarsh_dict,\n",
        "        df_dict_2=Penmanshiel_dict,\n",
        "        hyperparameters=hyperparameters)\n",
        "\n",
        "    # Use the new method to get the data loaders\n",
        "    train_loader, test_loader, eval_loader = loader.create_dataloaders()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions, crps_weights = model(features)\n",
        "            loss = loss_fn(predictions.unsqueeze(1), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        train_loss_avg = np.mean(train_losses)\n",
        "        print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        validation_losses = []\n",
        "        with torch.no_grad():\n",
        "            for features, labels in eval_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                predictions, crps_weights = model(features)\n",
        "                val_loss = loss_fn(predictions.unsqueeze(1), labels)\n",
        "                validation_losses.append(val_loss.item())\n",
        "\n",
        "        val_loss_avg = np.mean(validation_losses)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        training_time.append(epoch_duration)\n",
        "        #print(f\"Epoch Duration: \\n {round(epoch_duration, 4)}s\")\n",
        "\n",
        "        early_stopping(val_loss_avg)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # After training is done, set the flag to False to stop the monitoring thread\n",
        "    keep_monitoring_flag = False\n",
        "    monitor_thread.join()  # Wait for the monitoring thread to finish\n",
        "\n",
        "    # Convert the results list to a DataFrame\n",
        "    system_usage = pd.DataFrame(system_usage_results)\n",
        "\n",
        "    # test data set\n",
        "    test_losses = []\n",
        "    predictions_collected = []\n",
        "    groundtruth_collected = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Perform the multi-step forecast\n",
        "            predictions, weights = multi_step_Linear(model, features, hyperparameters['step_forecast'])\n",
        "            test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:])\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            predictions_collected.extend(predictions.tolist())\n",
        "            groundtruth_collected.extend(labels.tolist())\n",
        "\n",
        "        test_loss_avg = np.mean(test_losses)\n",
        "\n",
        "    print(f\"\\n Model test loss: {round(test_loss_avg,6)}\")\n",
        "\n",
        "    # Save Results\n",
        "    df_eval = pd.DataFrame({\n",
        "        'Predictions':predictions_collected,\n",
        "        'GroundTruth':groundtruth_collected,\n",
        "        'TestLoss':round(test_loss_avg,6)})\n",
        "    df_epoch = pd.DataFrame({\n",
        "        'Epoch Duration':training_time})\n",
        "\n",
        "    import re\n",
        "    def re_loss_fn(s):\n",
        "        pattern = r'(?:nn\\.)?(\\w+)\\(\\)'\n",
        "        match = re.search(pattern, s)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "    system_usage.to_csv(f\"../data/df_SystemUsage_Linear_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "    df_eval.to_csv(f\"../data/df_eval_Linear_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "    df_epoch.to_csv(f\"../data/df_epoch_Linear_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "\n",
        "loss_functions = ['MSELoss', 'L1Loss', 'CRPS']\n",
        "step_forecasts = [6,72,144]\n",
        "\n",
        "for loss_function in loss_functions:\n",
        "    for step_forecast in step_forecasts:\n",
        "        print(f\"{loss_function, step_forecast}\")\n",
        "        run_experiment(loss_function, step_forecast, hyperparameters)\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}