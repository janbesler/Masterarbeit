{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.tsa.stattools import acf, adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.svm import SVR\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn import linear_model\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preprocess(data, demand, temperature, n_lags):\n",
    "\n",
    "    # Scale the input data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data[[f'{demand}', f'{temperature}']])\n",
    "\n",
    "    # Create lag features\n",
    "    def create_lag_features(data, n_lags):\n",
    "        lag_features = []\n",
    "        for i in range(1, n_lags + 1):\n",
    "            lag_features.append(data.shift(i).rename(columns=lambda x: f'{x}_lag_{i}'))\n",
    "        return pd.concat(lag_features, axis=1)\n",
    "\n",
    "    lag_features = create_lag_features(pd.DataFrame(scaled_data, columns=[f'{demand}', f'{temperature}']), n_lags)\n",
    "    data = pd.DataFrame(scaled_data, columns=[f'{demand}', f'{temperature}']).join(lag_features).dropna().values\n",
    "\n",
    "    # Train-test split (80:20)\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_train, y_train = train[:, 1:], train[:, 0]\n",
    "    X_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "    # Reshape input\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, -1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dataloader(X_train, y_train, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_hyperparameter(X_train):\n",
    "    hyperparameters_dict = dict({\n",
    "        'input_dim' : X_train.shape[2],\n",
    "        'hidden_dim' : 60,\n",
    "        'num_layers' : 1,\n",
    "        'output_dim' : 1,\n",
    "        'learning_rate' : 0.001,\n",
    "        'num_epochs' : 5,\n",
    "        'batch_size' : 32,\n",
    "        'device' : \"cpu\",\n",
    "        'nhead' : 4\n",
    "    })\n",
    "\n",
    "    return hyperparameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_errors (results_df, model, y_true, y_pred):\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    results_df.loc[f'{model}', 'MSE'] = '{:.6f}'.format(mse)\n",
    "    results_df.loc[f'{model}', 'RMSE'] = '{:.6f}'.format(rmse)\n",
    "    results_df.loc[f'{model}', 'MAE'] = '{:.6f}'.format(mae)\n",
    "    results_df.loc[f'{model}', 'MAPE'] = '{:.6f}'.format(mape)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(hyperparameters, dataloader, scaler, X_test, y_test):\n",
    "    # Create the LSTM model\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)#.to(hyperparameters['device'])\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)#.to(hyperparameters['device'])\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LSTMModel(\n",
    "        hyperparameters['input_dim'],\n",
    "        hyperparameters['hidden_dim'],\n",
    "        hyperparameters['num_layers'],\n",
    "        hyperparameters['output_dim']\n",
    "        ).to(hyperparameters['device'])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(hyperparameters['num_epochs']):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # move to GPU\n",
    "            x_batch = x_batch.to(hyperparameters['device'])\n",
    "            y_batch = y_batch.to(hyperparameters['device'])\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).detach().numpy().squeeze()\n",
    "\n",
    "    # Invert scaling for test data\n",
    "    test_unscaled = np.column_stack((y_test.numpy().reshape(-1, 1), X_test.numpy().squeeze()[:, :1]))\n",
    "    test_unscaled = scaler.inverse_transform(test_unscaled)\n",
    "    y_test_unscaled = test_unscaled[:, 0]\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    y_pred_scaled = np.column_stack((y_pred.reshape(-1, 1), X_test.numpy().squeeze()[:, :1]))\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)[:, 0]\n",
    "\n",
    "    return y_pred_unscaled, y_test_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer (ChatGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Transformer(hyperparameters, dataloader, scaler, X_test, y_test):\n",
    "    # Transformer Model\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
    "            super(TransformerModel, self).__init__()\n",
    "            self.embedding = nn.Linear(input_dim, d_model)\n",
    "            self.transformer_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "            self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers)\n",
    "            self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    # Create the transformer model\n",
    "    model = TransformerModel(\n",
    "                input_dim = hyperparameters['input_dim'],\n",
    "                d_model = hyperparameters['hidden_dim'],\n",
    "                nhead = hyperparameters['nhead'],\n",
    "                num_layers = hyperparameters['num_layers'],\n",
    "                output_dim = hyperparameters['output_dim']\n",
    "                ).to(hyperparameters['device'])\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(hyperparameters['num_epochs']):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move data to the device (CPU or GPU)\n",
    "            x_batch = x_batch.to(hyperparameters['device'])\n",
    "            y_batch = y_batch.to(hyperparameters['device'])\n",
    "\n",
    "            #print(y_batch.shape)\n",
    "            #print(x_batch.shape)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            #print(y_pred.shape)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).detach().cpu().numpy().squeeze()\n",
    "\n",
    "    # Invert scaling for test data\n",
    "    test_unscaled = np.column_stack((y_test.cpu().numpy().reshape(-1, 1), X_test.cpu().numpy().squeeze()[:, :1]))\n",
    "    test_unscaled = scaler.inverse_transform(test_unscaled)\n",
    "    y_test_unscaled = test_unscaled[:, 0]\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    y_pred_scaled = np.column_stack((y_pred.reshape(-1, 1), X_test.cpu().numpy().squeeze()[:, :1]))\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)[:, 0]\n",
    "\n",
    "    return y_pred_unscaled, y_test_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compelete Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(data, demand, temperature, n_lags):\n",
    "    # create results df\n",
    "    results_df = pd.DataFrame(\n",
    "        index=['linear regression', 'LSTM', 'Transformer 1'],\n",
    "        columns=['MSE', 'RMSE', 'MAE', 'MAPE']\n",
    "        )\n",
    "\n",
    "    # preprocess data\n",
    "    X_train, y_train, X_test, y_test, scaler = model_preprocess(data, demand, temperature, n_lags)\n",
    "    linear_x_train, linear_x_test, linear_y_train, linear_y_test = model_linear_data(data, demand, temperature, n_lags)\n",
    "\n",
    "    # hyperparameters\n",
    "    hyperparameters = model_hyperparameter(X_train)\n",
    "\n",
    "    # dataloader\n",
    "    dataloader = model_dataloader(X_train, y_train, hyperparameters['batch_size'])\n",
    "\n",
    "    # predictions\n",
    "    linear_pred = model_linear(linear_x_train, linear_y_train, linear_x_test)\n",
    "    LSTM_pred, LSTM_test = model_LSTM(hyperparameters, dataloader, scaler, X_test, y_test)\n",
    "    #Transformer_pred, Transformer_test = model_Transformer(hyperparameters, dataloader, scaler, X_test, y_test)\n",
    "\n",
    "    # errors\n",
    "    model_errors(results_df, 'linear regression', linear_y_test, linear_pred)\n",
    "    model_errors(results_df, 'LSTM', LSTM_test, LSTM_pred)\n",
    "    #model_errors(results_df, 'Transformer 1', Transformer_test, Transformer_pred)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model(Kelmarsh_1, 'Energy Export (kWh)', 'Long Term Wind (m/s)', 24)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
