{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aKGCTbPM917",
    "outputId": "43c7af4a-6dba-4ad3-f8e2-13d128c71e95"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.tsa.stattools import acf, adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.svm import SVR\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG2muxLYNZiP"
   },
   "source": [
    "# read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data downloaded from:\n",
    "\n",
    " - https://zenodo.org/record/5946808#.ZGNpddbP23I\n",
    " - https://zenodo.org/record/5841834#.ZGNpTNbP23J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wind data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Status data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fBXDHhbXNYrU"
   },
   "outputs": [],
   "source": [
    "# read in files\n",
    "turbine_status = pd.read_csv('../data/Windturbinen/Kelmarsh/Status_Kelmarsh_1_2016-01-03_-_2017-01-01_228.csv',\n",
    "                             skiprows = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "1JOJBOQmJ8vW",
    "outputId": "9ef7cb4d-1722-4065-d36c-5d6e1c78e881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp start', 'Timestamp end', 'Duration', 'Status', 'Code',\n",
      "       'Message', 'Comment', 'Service contract category', 'IEC category'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp start</th>\n",
       "      <th>Timestamp end</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Status</th>\n",
       "      <th>Code</th>\n",
       "      <th>Message</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Service contract category</th>\n",
       "      <th>IEC category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-14 19:28:03</td>\n",
       "      <td>2016-01-23 14:36:32</td>\n",
       "      <td>211:08:29</td>\n",
       "      <td>Stop</td>\n",
       "      <td>111</td>\n",
       "      <td>Emergency stop nacelle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emergency stop switch (Nacelle) (11)</td>\n",
       "      <td>Forced outage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-14 19:28:03</td>\n",
       "      <td>2016-01-14 19:38:03</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>Warning</td>\n",
       "      <td>5720</td>\n",
       "      <td>Brake accumulator defect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warnings (27)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-14 19:28:05</td>\n",
       "      <td>2016-01-23 11:27:46</td>\n",
       "      <td>207:59:41</td>\n",
       "      <td>Informational</td>\n",
       "      <td>3835</td>\n",
       "      <td>Cable panel breaker open</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warnings (27)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-14 19:28:05</td>\n",
       "      <td>2016-01-23 11:27:46</td>\n",
       "      <td>207:59:41</td>\n",
       "      <td>Informational</td>\n",
       "      <td>3830</td>\n",
       "      <td>Supply circuit breaker earthed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warnings (27)</td>\n",
       "      <td>Full Performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-14 19:28:05</td>\n",
       "      <td>2016-01-23 14:09:18</td>\n",
       "      <td>210:41:13</td>\n",
       "      <td>Warning</td>\n",
       "      <td>3870</td>\n",
       "      <td>Overload transformer fan outlet air</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warnings (27)</td>\n",
       "      <td>Full Performance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Timestamp start        Timestamp end   Duration         Status  Code  \\\n",
       "0  2016-01-14 19:28:03  2016-01-23 14:36:32  211:08:29           Stop   111   \n",
       "1  2016-01-14 19:28:03  2016-01-14 19:38:03   00:10:00        Warning  5720   \n",
       "2  2016-01-14 19:28:05  2016-01-23 11:27:46  207:59:41  Informational  3835   \n",
       "3  2016-01-14 19:28:05  2016-01-23 11:27:46  207:59:41  Informational  3830   \n",
       "4  2016-01-14 19:28:05  2016-01-23 14:09:18  210:41:13        Warning  3870   \n",
       "\n",
       "                               Message  Comment  \\\n",
       "0               Emergency stop nacelle      NaN   \n",
       "1             Brake accumulator defect      NaN   \n",
       "2             Cable panel breaker open      NaN   \n",
       "3       Supply circuit breaker earthed      NaN   \n",
       "4  Overload transformer fan outlet air      NaN   \n",
       "\n",
       "              Service contract category      IEC category  \n",
       "0  Emergency stop switch (Nacelle) (11)     Forced outage  \n",
       "1                         Warnings (27)               NaN  \n",
       "2                         Warnings (27)               NaN  \n",
       "3                         Warnings (27)  Full Performance  \n",
       "4                         Warnings (27)  Full Performance  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(turbine_status.columns)\n",
    "turbine_status.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status data is not relevant to this thesis !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Turbine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_Windturbines (turbine_directory):\n",
    "    # Columns to keep\n",
    "    columns_to_keep = [\n",
    "        '# Date and time',\n",
    "        'Wind speed (m/s)',\n",
    "        'Long Term Wind (m/s)',\n",
    "        'Energy Export (kWh)'\n",
    "    ]\n",
    "\n",
    "    # Directory containing CSV files\n",
    "    directory = f'../data/Windturbinen/{turbine_directory}/'\n",
    "\n",
    "    # Dictionary to hold DataFrames for each turbine\n",
    "    turbine_dataframes = defaultdict(list)\n",
    "\n",
    "    # Get a list of CSV files in the directory\n",
    "    csv_files = [f for f in os.listdir(directory) if f.startswith(f\"Turbine_Data_{turbine_directory}_\") and f.endswith(\".csv\")]\n",
    "\n",
    "    # Iterate through the files in the directory with a tqdm progress bar\n",
    "    for filename in tqdm(csv_files, desc='Processing files'):\n",
    "        # Extract the turbine number from the filename\n",
    "        turbine_number = filename.split(\"_\")[3]  # Assuming the number is in this position\n",
    "\n",
    "        # Read the CSV file, skipping the first 9 rows\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath, skiprows=9, usecols=columns_to_keep)\n",
    "\n",
    "        # Convert the \"Date and time\" column to datetime\n",
    "        df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "\n",
    "        # Append the DataFrame to the appropriate turbine's list\n",
    "        turbine_dataframes[turbine_number].append(df)\n",
    "\n",
    "    # Concatenate the DataFrames for each turbine\n",
    "    for turbine_number, dfs in turbine_dataframes.items():\n",
    "        turbine_dataframes[turbine_number] = pd.concat(dfs)\n",
    "        turbine_dataframes[turbine_number].sort_values('# Date and time', inplace=True)\n",
    "\n",
    "    # Print the keys for the dictionary\n",
    "    print(\"\\n dictionary keys:\")\n",
    "    print(turbine_dataframes.keys())\n",
    "    # print descriptive stuff for exemplary key\n",
    "    print('\\n Information for exemplary key:')\n",
    "    first_key = list(turbine_dataframes.keys())[0]\n",
    "    print('shape')\n",
    "    print(turbine_dataframes[first_key].shape)\n",
    "    print('\\n missing values')\n",
    "    print(turbine_dataframes[first_key].isna().sum())\n",
    "\n",
    "    return turbine_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 36/36 [00:18<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dictionary keys:\n",
      "dict_keys(['6', '4', '1', '3', '2', '5'])\n",
      "\n",
      " Information for exemplary key:\n",
      "shape\n",
      "(288864, 4)\n",
      "\n",
      " missing values\n",
      "# Date and time            0\n",
      "Wind speed (m/s)        9223\n",
      "Long Term Wind (m/s)       0\n",
      "Energy Export (kWh)     5072\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Kelmarsh_df = reading_Windturbines('Kelmarsh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 84/84 [00:45<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dictionary keys:\n",
      "dict_keys(['07', '08', '02', '05', '06', '15', '10', '14', '01', '04', '11', '12', '13', '09'])\n",
      "\n",
      " Information for exemplary key:\n",
      "shape\n",
      "(267014, 4)\n",
      "\n",
      " missing values\n",
      "# Date and time            0\n",
      "Wind speed (m/s)        5881\n",
      "Long Term Wind (m/s)       0\n",
      "Energy Export (kWh)     1032\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Penmanshiel_df = reading_Windturbines('Penmanshiel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlexGuide Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ENIT_2022_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# merge files\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ENIT_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([ENIT_2022_1, ENIT_2022_2, ENIT_2023_1])\n\u001b[1;32m      4\u001b[0m \u001b[39m# rename columns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ENIT_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m ENIT_df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mWirkarbeit (Bezug) \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENIT_2022_1' is not defined"
     ]
    }
   ],
   "source": [
    "# merge files\n",
    "ENIT_df = pd.concat([ENIT_2022_1, ENIT_2022_2, ENIT_2023_1])\n",
    "\n",
    "# rename columns\n",
    "ENIT_df.columns = ENIT_df.columns.str.replace('Wirkarbeit (Bezug) ', '').str.strip()\n",
    "\n",
    "# add residual columns\n",
    "ENIT_df['1.8 - Residual'] = ENIT_df.iloc[:, 1] - ENIT_df.iloc[:, 2:8].sum(axis=1)\n",
    "ENIT_df['2.7 - Residual'] = ENIT_df.iloc[:, 9] - ENIT_df.iloc[:, 10:15].sum(axis=1)\n",
    "ENIT_df['0.1 - Residual'] = ENIT_df.iloc[:, 16] - ENIT_df.iloc[:, 1] - ENIT_df.iloc[:, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in each column:\n",
      "1.0 - Trafo 1 [Wh]                                   0\n",
      "1.1 - Neubau, Wohnhaus, Holzplatz [Wh]               0\n",
      "1.2 - Halle 3/1 Absaugung [Wh]                       0\n",
      "1.3 - Halle 4/2 Maschinensaal/Tischfertigung [Wh]    0\n",
      "1.4 - Halle 2/2 Verwaltung, Entwicklung [Wh]         0\n",
      "1.5 - Halle 4/5 Lackieranlage [Wh]                   0\n",
      "1.6 - Halle 1/6 Hausmeister [Wh]                     0\n",
      "1.7 - Halle 3/3 Kompressor, Stuhlmontage [Wh]        0\n",
      "2.0 - Trafo 2 [Wh]                                   0\n",
      "2.1 - Halle 4/5 Schrankfertigung [Wh]                0\n",
      "2.2 - Halle 2/4 Rilsan [Wh]                          0\n",
      "2.3 - Halle 4/5 Stahlstuhl [Wh]                      0\n",
      "2.4 - Halle 4/1 Schichtholz [Wh]                     0\n",
      "2.5 - Halle 4/1 Absaugung Schichtholz [Wh]           0\n",
      "dtype: int64\n",
      "\n",
      "Indices of outliers:\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean and standard deviation for each column\n",
    "mean = ENIT_df.iloc[:, 1:15].mean()\n",
    "std_dev = ENIT_df.iloc[:, 1:15].std()\n",
    "\n",
    "# Identify outliers using twice the standard deviation\n",
    "outliers = (ENIT_df.iloc[:, 1:15] < (mean - 2 * std_dev)) | (ENIT_df.iloc[:, 1:15] > (mean + 2 * std_dev))\n",
    "\n",
    "# Print the number of True values in each column\n",
    "print(\"Number of outliers in each column:\")\n",
    "print(outliers.sum())\n",
    "\n",
    "# Get the row and column indices of the True values\n",
    "true_values_indices = outliers.where(outliers).stack().index\n",
    "\n",
    "# Print the row and column indices of the True values\n",
    "print(\"\\nIndices of outliers:\")\n",
    "for row, col in true_values_indices:\n",
    "    print(f\"Row: {row}, Column: {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ENIT_df.shape)\n",
    "ENIT_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.  , 20000.  , 40000.  , ..., 66666.67, 86666.67, 73333.33])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENIT_df['Übergabezähler [Wh]'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA values\n",
    "print(ENIT_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data as dict with each company as a key\n",
    "FlexGuideData = {\n",
    "    'MechTron': ENIT_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV-ZBfp_N0g9"
   },
   "source": [
    "# pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IOsKtDGGN3XK"
   },
   "outputs": [],
   "source": [
    "def outlier_detection(data, dependent_var, independent_var):\n",
    "    # only keep demand and one other variable\n",
    "    df = data[[f'{dependent_var}', f'{independent_var}']]\n",
    "\n",
    "    # check for outliers\n",
    "    # Calculate the mean and standard deviation for each column\n",
    "    mean = df.mean()\n",
    "    std_dev = df.std()\n",
    "\n",
    "    # Identify outliers using twice the standard deviation\n",
    "    outliers = (df < (mean - 3 * std_dev)) | (df > (mean + 3 * std_dev))\n",
    "\n",
    "    # Print the number of True values in each column\n",
    "    print(\"Number of outliers in each column:\")\n",
    "    print(outliers.sum())\n",
    "\n",
    "    # Get the row and column indices of the True values\n",
    "    true_values_indices = outliers.where(outliers).stack().index\n",
    "\n",
    "    # Print the row and column indices of the True values\n",
    "    print(\"\\nIndices of outliers:\")\n",
    "    for row, col in true_values_indices:\n",
    "        print(f\"Row: {row}, Column: {col}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l2Cx0G9nPnJw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in each column:\n",
      "Energy Export (kWh)     6\n",
      "Long Term Wind (m/s)    0\n",
      "dtype: int64\n",
      "\n",
      "Indices of outliers:\n",
      "Row: 27407, Column: Energy Export (kWh)\n",
      "Row: 10456, Column: Energy Export (kWh)\n",
      "Row: 17788, Column: Energy Export (kWh)\n",
      "Row: 24557, Column: Energy Export (kWh)\n",
      "Row: 26026, Column: Energy Export (kWh)\n",
      "Row: 37678, Column: Energy Export (kWh)\n"
     ]
    }
   ],
   "source": [
    "# column names as strings\n",
    "Kelmarsh_1 = outlier_detection(Kelmarsh_df['1'], 'Energy Export (kWh)', 'Long Term Wind (m/s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'Kelmarsh_df' (defaultdict)\n",
      "Stored 'Penmanshiel_df' (defaultdict)\n"
     ]
    }
   ],
   "source": [
    "%store Kelmarsh_df Penmanshiel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hf8xKiOkoMt"
   },
   "source": [
    "# Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SV9yrwwoludt"
   },
   "source": [
    "### data preprocessing for data loader and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FXxofyUVkp1-"
   },
   "outputs": [],
   "source": [
    "def model_preprocess(data, demand, temperature, n_lags):\n",
    "\n",
    "    # Scale the input data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data[[f'{demand}', f'{temperature}']])\n",
    "\n",
    "    # Create lag features\n",
    "    def create_lag_features(data, n_lags):\n",
    "        lag_features = []\n",
    "        for i in range(1, n_lags + 1):\n",
    "            lag_features.append(data.shift(i).rename(columns=lambda x: f'{x}_lag_{i}'))\n",
    "        return pd.concat(lag_features, axis=1)\n",
    "\n",
    "    lag_features = create_lag_features(pd.DataFrame(scaled_data, columns=[f'{demand}', f'{temperature}']), n_lags)\n",
    "    data = pd.DataFrame(scaled_data, columns=[f'{demand}', f'{temperature}']).join(lag_features).dropna().values\n",
    "\n",
    "    # Train-test split (80:20)\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    train, test = data[:train_size], data[train_size:]\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_train, y_train = train[:, 1:], train[:, 0]\n",
    "    X_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "    # Reshape input\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, -1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFSxTQUYOACR"
   },
   "source": [
    "### linear data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bkIjbz6fOCiz"
   },
   "outputs": [],
   "source": [
    "def model_linear_data(data, demand, temperature, n_lags):\n",
    "\n",
    "    data[f\"{demand}_lag_{n_lags}\"] = data[f'{demand}'].shift(n_lags)\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Prepare data for modeling\n",
    "    X = data[[f'{temperature}', f\"{demand}_lag_{n_lags}\"]]\n",
    "    y = data[f\"{demand}\"]\n",
    "\n",
    "    # Train-test split (80:20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dunBmuMvsWcb"
   },
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qm252QdAsZGw"
   },
   "outputs": [],
   "source": [
    "def model_dataloader(X_train, y_train, batch_size):\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYYWOIQesybI"
   },
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FWJ2aPVPs0YJ"
   },
   "outputs": [],
   "source": [
    "def model_hyperparameter(X_train):\n",
    "    hyperparameters_dict = dict({\n",
    "        'input_dim' : X_train.shape[2],\n",
    "        'hidden_dim' : 60,\n",
    "        'num_layers' : 1,\n",
    "        'output_dim' : 1,\n",
    "        'learning_rate' : 0.001,\n",
    "        'num_epochs' : 5,\n",
    "        'batch_size' : 32,\n",
    "        'device' : \"cpu\",\n",
    "        'nhead' : 4\n",
    "    })\n",
    "\n",
    "    return hyperparameters_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWY6Vk-AF6WJ"
   },
   "source": [
    "### errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_ZSgZs0TF5p2"
   },
   "outputs": [],
   "source": [
    "def model_errors (results_df, model, y_true, y_pred):\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "    results_df.loc[f'{model}', 'MSE'] = '{:.6f}'.format(mse)\n",
    "    results_df.loc[f'{model}', 'RMSE'] = '{:.6f}'.format(rmse)\n",
    "    results_df.loc[f'{model}', 'MAE'] = '{:.6f}'.format(mae)\n",
    "    results_df.loc[f'{model}', 'MAPE'] = '{:.6f}'.format(mape)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13hFG5tvHieq"
   },
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B1_6KOCRrfm0"
   },
   "outputs": [],
   "source": [
    "def model_linear(X_train, y_train, X_test):\n",
    "    # make linear prediction\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train, y_train)\n",
    "    y_pred_linear = linear_reg.predict(X_test)\n",
    "\n",
    "    return y_pred_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pYSYeXQHqQV"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PNORoKShHrTU"
   },
   "outputs": [],
   "source": [
    "def model_LSTM(hyperparameters, dataloader, scaler, X_test, y_test):\n",
    "    # Create the LSTM model\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)#.to(hyperparameters['device'])\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)#.to(hyperparameters['device'])\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                out, _ = self.lstm(x, (h0, c0))\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LSTMModel(\n",
    "        hyperparameters['input_dim'],\n",
    "        hyperparameters['hidden_dim'],\n",
    "        hyperparameters['num_layers'],\n",
    "        hyperparameters['output_dim']\n",
    "        ).to(hyperparameters['device'])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(hyperparameters['num_epochs']):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # move to GPU\n",
    "            x_batch = x_batch.to(hyperparameters['device'])\n",
    "            y_batch = y_batch.to(hyperparameters['device'])\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).detach().numpy().squeeze()\n",
    "\n",
    "    # Invert scaling for test data\n",
    "    test_unscaled = np.column_stack((y_test.numpy().reshape(-1, 1), X_test.numpy().squeeze()[:, :1]))\n",
    "    test_unscaled = scaler.inverse_transform(test_unscaled)\n",
    "    y_test_unscaled = test_unscaled[:, 0]\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    y_pred_scaled = np.column_stack((y_pred.reshape(-1, 1), X_test.numpy().squeeze()[:, :1]))\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)[:, 0]\n",
    "\n",
    "    return y_pred_unscaled, y_test_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ8_NlXQeX7Q"
   },
   "source": [
    "### Transformer (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "URwhZPwYXx9A"
   },
   "outputs": [],
   "source": [
    "def model_Transformer(hyperparameters, dataloader, scaler, X_test, y_test):\n",
    "    # Transformer Model\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):\n",
    "            super(TransformerModel, self).__init__()\n",
    "            self.embedding = nn.Linear(input_dim, d_model)\n",
    "            self.transformer_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "            self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers)\n",
    "            self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    # Create the transformer model\n",
    "    model = TransformerModel(\n",
    "                input_dim = hyperparameters['input_dim'],\n",
    "                d_model = hyperparameters['hidden_dim'],\n",
    "                nhead = hyperparameters['nhead'],\n",
    "                num_layers = hyperparameters['num_layers'],\n",
    "                output_dim = hyperparameters['output_dim']\n",
    "                ).to(hyperparameters['device'])\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(hyperparameters['num_epochs']):\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Move data to the device (CPU or GPU)\n",
    "            x_batch = x_batch.to(hyperparameters['device'])\n",
    "            y_batch = y_batch.to(hyperparameters['device'])\n",
    "\n",
    "            #print(y_batch.shape)\n",
    "            #print(x_batch.shape)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            #print(y_pred.shape)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test).detach().cpu().numpy().squeeze()\n",
    "\n",
    "    # Invert scaling for test data\n",
    "    test_unscaled = np.column_stack((y_test.cpu().numpy().reshape(-1, 1), X_test.cpu().numpy().squeeze()[:, :1]))\n",
    "    test_unscaled = scaler.inverse_transform(test_unscaled)\n",
    "    y_test_unscaled = test_unscaled[:, 0]\n",
    "\n",
    "    # Invert scaling for predictions\n",
    "    y_pred_scaled = np.column_stack((y_pred.reshape(-1, 1), X_test.cpu().numpy().squeeze()[:, :1]))\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred_scaled)[:, 0]\n",
    "\n",
    "    return y_pred_unscaled, y_test_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-DVw1JEyMXg"
   },
   "source": [
    "# complete product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jOJuddfvyQJ0"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "def final_model(data, demand, temperature, n_lags):\n",
    "    # create results df\n",
    "    results_df = pd.DataFrame(\n",
    "        index=['linear regression', 'LSTM', 'Transformer 1'],\n",
    "        columns=['MSE', 'RMSE', 'MAE', 'MAPE']\n",
    "        )\n",
    "\n",
    "    # preprocess data\n",
    "    X_train, y_train, X_test, y_test, scaler = model_preprocess(data, demand, temperature, n_lags)\n",
    "    linear_x_train, linear_x_test, linear_y_train, linear_y_test = model_linear_data(data, demand, temperature, n_lags)\n",
    "\n",
    "    # hyperparameters\n",
    "    hyperparameters = model_hyperparameter(X_train)\n",
    "\n",
    "    # dataloader\n",
    "    dataloader = model_dataloader(X_train, y_train, hyperparameters['batch_size'])\n",
    "\n",
    "    # predictions\n",
    "    linear_pred = model_linear(linear_x_train, linear_y_train, linear_x_test)\n",
    "    LSTM_pred, LSTM_test = model_LSTM(hyperparameters, dataloader, scaler, X_test, y_test)\n",
    "    #Transformer_pred, Transformer_test = model_Transformer(hyperparameters, dataloader, scaler, X_test, y_test)\n",
    "\n",
    "    # errors\n",
    "    model_errors(results_df, 'linear regression', linear_y_test, linear_pred)\n",
    "    model_errors(results_df, 'LSTM', LSTM_test, LSTM_pred)\n",
    "    #model_errors(results_df, 'Transformer 1', Transformer_test, Transformer_pred)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "qoyfXaeSvIP9",
    "outputId": "50665aa6-075c-428d-da7c-df8210d62a58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1583/932529838.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f\"{demand}_lag_{n_lags}\"] = data[f'{demand}'].shift(n_lags)\n",
      "/tmp/ipykernel_1583/1017121790.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.float))\n",
      "/tmp/ipykernel_1583/3599282347.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
      "/tmp/ipykernel_1583/3599282347.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear regression</th>\n",
       "      <td>4652.087830</td>\n",
       "      <td>68.206215</td>\n",
       "      <td>49.674727</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>1201.843506</td>\n",
       "      <td>34.667614</td>\n",
       "      <td>23.500977</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformer 1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MSE       RMSE        MAE MAPE\n",
       "linear regression  4652.087830  68.206215  49.674727  inf\n",
       "LSTM               1201.843506  34.667614  23.500977  inf\n",
       "Transformer 1              NaN        NaN        NaN  NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model(Kelmarsh_1, 'Energy Export (kWh)', 'Long Term Wind (m/s)', 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PeJvjfXvIzT"
   },
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty-I917_Vf8m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SV9yrwwoludt",
    "WFSxTQUYOACR",
    "bWY6Vk-AF6WJ",
    "13hFG5tvHieq",
    "-pYSYeXQHqQV"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
