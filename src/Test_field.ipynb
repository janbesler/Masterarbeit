{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# reading data\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.fft import rfft, irfft, fftn, ifftn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# measuring ressources\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "# eFormer\n",
    "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
    "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
    "from eFormer.loss_function import CRPS, weighted_CRPS\n",
    "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
    "from eFormer.Dataloader import TimeSeriesDataProcessor\n",
    "\n",
    "# transformer Benchmarks\n",
    "from Benchmarks.Benchmarks import VanillaTransformer, Informer\n",
    "\n",
    "%store -r Kelmarsh_df Penmanshiel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'hyperparameters' (dict)\n"
     ]
    }
   ],
   "source": [
    "# set global parameters\n",
    "hyperparameters = {\n",
    "    'n_heads': 4,\n",
    "    'ProbabilisticModel': False,\n",
    "    # embeddings\n",
    "    'len_embedding': 64,\n",
    "    'batch_size': 512,\n",
    "    # general\n",
    "    'pred_len': 1,\n",
    "    'seq_len': 72,\n",
    "    'patience': 7,\n",
    "    'dropout': 0.05,\n",
    "    'learning_rate': 6e-4,\n",
    "    'WeightDecay': 1e-1,\n",
    "    'train_epochs': 2,\n",
    "    'num_workers': 10,\n",
    "    'step_forecast': 6,\n",
    "    # benchmarks\n",
    "    'factor': 1,\n",
    "    'output_attention': True,\n",
    "    'd_model': 64,\n",
    "    'c_out': 6,\n",
    "    'e_layers': 2,\n",
    "    'd_layers': 2,\n",
    "    'activation': 'relu',\n",
    "    'd_ff': 1,\n",
    "    'distil': True,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "%store hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_conditions():\n",
    "    # Get CPU usage for each core\n",
    "    cpu_percent = round(psutil.cpu_percent(), 4)\n",
    "\n",
    "    # Get memory information\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    memory_used_gb = round(memory_info.used / (1024 ** 3), 4)\n",
    "\n",
    "    # Get GPU information\n",
    "    try:\n",
    "      gpu_info = GPUtil.getGPUs()[0]\n",
    "      gpu_memory_used_gb = round(gpu_info.memoryUsed / 1024, 4)\n",
    "    except IndexError:\n",
    "      # If no GPU is found, set variables to None\n",
    "      gpu_memory_used_gb = None\n",
    "\n",
    "    # Collect data in a dictionary\n",
    "    comp_usage = {\n",
    "        'CPU Usage': cpu_percent,\n",
    "        'Memory Usage (GB)': memory_used_gb,\n",
    "        'GPU Usage (GB)': gpu_memory_used_gb\n",
    "    }\n",
    "\n",
    "    return comp_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing status files: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "Processing turbine files: 100%|██████████| 36/36 [00:20<00:00,  1.74it/s]\n",
      "Processing status files: 0it [00:00, ?it/s]\n",
      "Processing turbine files: 100%|██████████| 84/84 [00:45<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "class WindTurbineDataProcessor:\n",
    "    def __init__(self, turbine_directory, dependent_var='Power (kW)'):\n",
    "        self.directory = f'../data/Windturbinen/{turbine_directory}/'\n",
    "        self.dependent_var = dependent_var\n",
    "\n",
    "    def safe_datetime_conversion(self, s):\n",
    "        try:\n",
    "            return pd.to_datetime(s)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "    def days_since_last_maintenance(self, row_date, maintenance_dates):\n",
    "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
    "        if not preceding_maintenance_dates:\n",
    "            return float('NaN')\n",
    "        last_maintenance_date = max(preceding_maintenance_dates)\n",
    "        delta = (row_date - last_maintenance_date).days\n",
    "        return delta\n",
    "\n",
    "    def check_missing_sequences(self, df):\n",
    "        sequences = []\n",
    "        current_sequence = 0\n",
    "        long_sequence_indices = []\n",
    "        start_index = None\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            if pd.isnull(row[self.dependent_var]):\n",
    "                current_sequence += 1\n",
    "                if start_index is None:\n",
    "                    start_index = i\n",
    "            else:\n",
    "                if current_sequence >= 19:\n",
    "                    sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                    long_sequence_indices.extend(sequence_indices)\n",
    "                    df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "                if current_sequence > 0:\n",
    "                    sequences.append(current_sequence)\n",
    "                current_sequence = 0\n",
    "                start_index = None\n",
    "        \n",
    "        if current_sequence > 0:\n",
    "            sequences.append(current_sequence)\n",
    "            if current_sequence >= 19:\n",
    "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
    "                long_sequence_indices.extend(sequence_indices)\n",
    "                df.loc[sequence_indices, self.dependent_var] = np.inf\n",
    "\n",
    "        df[self.dependent_var] = df[self.dependent_var].replace(np.inf, np.nan).interpolate(method='linear')\n",
    "        df.drop(long_sequence_indices, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def process_and_load_data(self):\n",
    "        turbine_dataframes = defaultdict(list)\n",
    "        status_lists = defaultdict(list)\n",
    "\n",
    "        columns_turbine = ['# Date and time', 'Wind speed (m/s)', 'Power (kW)']\n",
    "        columns_status = ['Timestamp end', 'IEC category']\n",
    "\n",
    "        turbine_files = [f for f in os.listdir(self.directory) if f.startswith(\"Turbine_Data_\") and f.endswith(\".csv\")]\n",
    "        status_files = [f for f in os.listdir(self.directory) if f.startswith(\"Status_\") and f.endswith(\".csv\")]\n",
    "\n",
    "        for filename in tqdm(status_files, desc='Processing status files'):\n",
    "            turbine_number = filename.split(\"_\")[2]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
    "            df['Timestamp end'] = df['Timestamp end'].apply(self.safe_datetime_conversion)\n",
    "            maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].unique()\n",
    "            status_lists[turbine_number].extend(maintenance_dates)\n",
    "\n",
    "        for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
    "            turbine_number = filename.split(\"_\")[3]\n",
    "            filepath = os.path.join(self.directory, filename)\n",
    "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
    "            df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
    "            turbine_dataframes[turbine_number].append(df)\n",
    "\n",
    "        for turbine_number, dfs in turbine_dataframes.items():\n",
    "            turbine_dataframes[turbine_number] = pd.concat(dfs).sort_values('# Date and time').reset_index(drop=True)\n",
    "            turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
    "            turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1, inplace=True)\n",
    "            self.check_missing_sequences(turbine_dataframes[turbine_number])\n",
    "\n",
    "        gc.collect()\n",
    "        return turbine_dataframes\n",
    "\n",
    "def process_wind_turbines(turbine_directory, dependent_var):\n",
    "    processor = WindTurbineDataProcessor(turbine_directory, dependent_var)\n",
    "    return processor.process_and_load_data()\n",
    "\n",
    "Kelmarsh_dict = process_wind_turbines('Kelmarsh', 'Power (kW)')\n",
    "Penmanshiel_dict = process_wind_turbines('Penmanshiel', 'Power (kW)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataLoader:\n",
    "    def __init__(self, df_dict_1, df_dict_2, hyperparameters):\n",
    "        self.df_dict_1 = df_dict_1\n",
    "        self.df_dict_2 = df_dict_2\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.train_datasets = []\n",
    "        self.test_datasets = []\n",
    "        self.eval_datasets = []\n",
    "\n",
    "    def process_datasets(self, dataframe_dict):\n",
    "        for key, df in dataframe_dict.items():\n",
    "            processor = TimeSeriesDataProcessor(\n",
    "                dataframe=df,\n",
    "                forecast=1,\n",
    "                look_back=self.hyperparameters['seq_len'],\n",
    "                batch_size=self.hyperparameters['batch_size']\n",
    "            )\n",
    "            train_dataset, test_dataset, eval_dataset = processor.prepare_datasets()\n",
    "            self.train_datasets.append(train_dataset)\n",
    "            self.test_datasets.append(test_dataset)\n",
    "            self.eval_datasets.append(eval_dataset)\n",
    "            # Invoke garbage collection after processing each dataframe\n",
    "            gc.collect()\n",
    "\n",
    "    def create_concat_datasets(self):\n",
    "        self.process_datasets(self.df_dict_1)\n",
    "        self.process_datasets(self.df_dict_2)\n",
    "\n",
    "        # Concatenating the datasets\n",
    "        self.concat_train_dataset = ConcatDataset(self.train_datasets)\n",
    "        self.concat_test_dataset = ConcatDataset(self.test_datasets)\n",
    "        self.concat_eval_dataset = ConcatDataset(self.eval_datasets)\n",
    "        \n",
    "        # Clear the lists to free up memory\n",
    "        self.train_datasets.clear()\n",
    "        self.test_datasets.clear()\n",
    "        self.eval_datasets.clear()\n",
    "        \n",
    "        # Invoke garbage collection after clearing the lists\n",
    "        gc.collect()\n",
    "\n",
    "    def create_dataloaders(self):\n",
    "        self.create_concat_datasets()\n",
    "\n",
    "        # Creating the data loaders\n",
    "        self.train_loader = DataLoader(self.concat_train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
    "        self.test_loader = DataLoader(self.concat_test_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        self.eval_loader = DataLoader(self.concat_eval_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Invoke garbage collection after dataloaders are created\n",
    "        gc.collect()\n",
    "\n",
    "        return self.train_loader, self.test_loader, self.eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
    "            self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the UnifiedDataLoader class\n",
    "loader = UnifiedDataLoader(\n",
    "    df_dict_1=Kelmarsh_dict,\n",
    "    df_dict_2=Penmanshiel_dict,\n",
    "    hyperparameters=hyperparameters)\n",
    "\n",
    "# Use the new method to get the data loaders\n",
    "train_loader, test_loader, eval_loader = loader.create_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Layer Perceptron Model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        weights = self.linear.weight.data\n",
    "        \n",
    "        return x.squeeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Linear(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = LinearModel(\n",
    "        input_size=(hyperparameters['seq_len']),\n",
    "        output_size=hyperparameters['pred_len'],\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred.unsqueeze(1)), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction)\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]).unsqueeze(1), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: pred: torch.Size([512, 1]), labels: torch.Size([512, 1])\n",
      "Epoch 1 / 2 with Loss: 32879.579059\n",
      "eval: pred: torch.Size([512, 1]), labels: torch.Size([512, 1])\n",
      "Epoch Duration: 76.0484s\n",
      "training: pred: torch.Size([512, 1]), labels: torch.Size([512, 1])\n",
      "Epoch 2 / 2 with Loss: 24705.288462\n",
      "eval: pred: torch.Size([512, 1]), labels: torch.Size([512, 1])\n",
      "Epoch Duration: 67.3507s\n",
      "Validation loss decreased (inf --> 24684.749064)\n"
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = LinearModel(\n",
    "    input_size=(hyperparameters['seq_len'] * 2),\n",
    "    output_size=hyperparameters['pred_len'],\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print(f\"training: pred: {(predictions.unsqueeze(1)).shape}, labels: {labels.shape}\")\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions.unsqueeze(1), labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"eval: pred: {(predictions.unsqueeze(1)).shape}, labels: {labels.shape}\")\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    #print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test loss: 834704.876366\n"
     ]
    }
   ],
   "source": [
    "hyperparameters['step_forecast'] = 1\n",
    "\n",
    "# test data set\n",
    "test_losses = []\n",
    "predictions_collected = []\n",
    "groundtruth_collected = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_Linear(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        predictions_collected.extend(predictions.tolist())\n",
    "        groundtruth_collected.extend(labels.tolist())\n",
    "\n",
    "    test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[:-hyperparameters['step_forecast']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773120\n",
      "1510\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions_collected))\n",
    "print(len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837675.556809\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>GroundTruth</th>\n",
       "      <th>TestLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[617.960693359375]</td>\n",
       "      <td>[357.0]</td>\n",
       "      <td>837675.556809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3.798929214477539]</td>\n",
       "      <td>[-0.7543690204620361]</td>\n",
       "      <td>837675.556809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[679.9482421875]</td>\n",
       "      <td>[1596.283203125]</td>\n",
       "      <td>837675.556809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3.419466733932495]</td>\n",
       "      <td>[-7.6255340576171875]</td>\n",
       "      <td>837675.556809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[286.9339599609375]</td>\n",
       "      <td>[705.94384765625]</td>\n",
       "      <td>837675.556809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predictions            GroundTruth       TestLoss\n",
       "0   [617.960693359375]                [357.0]  837675.556809\n",
       "1  [3.798929214477539]  [-0.7543690204620361]  837675.556809\n",
       "2     [679.9482421875]       [1596.283203125]  837675.556809\n",
       "3  [3.419466733932495]  [-7.6255340576171875]  837675.556809\n",
       "4  [286.9339599609375]      [705.94384765625]  837675.556809"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected,\n",
    "    'TestLoss':round(test_loss_avg,6)})\n",
    "\n",
    "print(min(df_eval['TestLoss']))\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eFormer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eFormer(nn.Module):\n",
    "    def __init__(self, batch_size, seq_len, in_features, forecast, len_embedding_vector, n_heads, probabilistic_model=False):\n",
    "        super(eFormer, self).__init__()\n",
    "        self.probabilistic_model = probabilistic_model\n",
    "        self.n_heads = n_heads\n",
    "        self.len_embedding_vector = len_embedding_vector\n",
    "\n",
    "        # Initialize encoding model\n",
    "        if probabilistic_model:\n",
    "            self.encoding_model = ProbEncoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "        else:\n",
    "            self.encoding_model = Encoding(in_features=in_features, batch_size=batch_size, seq_len=seq_len, len_embedding_vector=len_embedding_vector)\n",
    "\n",
    "        # Initialize attention module\n",
    "        if probabilistic_model:\n",
    "            self.attention_module = ProbSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "        else:\n",
    "            self.attention_module = DetSparseAttentionModule(d_model=len_embedding_vector, n_heads=n_heads, prob_sparse_factor=5, seq_len=seq_len)\n",
    "\n",
    "        # Initialize decoder\n",
    "        # Assuming the decoder initialization does not actually require the output shape directly but parameters that depend on the model configuration\n",
    "        if probabilistic_model:\n",
    "            self.decoder = ProbSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "        else:\n",
    "            self.decoder = DetSparseDecoder(d_model=len_embedding_vector, n_heads=n_heads, batch_size=batch_size, seq_len=seq_len, forecast_horizon=forecast)\n",
    "\n",
    "    def forward(self, features_matrix):\n",
    "        if torch.isnan(features_matrix).any():\n",
    "            raise ValueError('NaN values detected in Input')\n",
    "\n",
    "        embeddings = self.encoding_model(features_matrix)\n",
    "        if torch.isnan(embeddings).any():\n",
    "            raise ValueError('NaN values detected in Embeddings')\n",
    "\n",
    "        encoder_output = self.attention_module(embeddings, embeddings, embeddings)\n",
    "        if torch.isnan(encoder_output).any():\n",
    "            raise ValueError('NaN values detected in Sparse Attention Output')\n",
    "\n",
    "        forecasts, crps_weights = self.decoder(encoder_output)\n",
    "        return forecasts, crps_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_eFormer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X25sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = eFormer(\n",
    "    in_features=(hyperparameters['seq_len'] * 2),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Perform the multi-step forecast\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m predictions, weights \u001b[39m=\u001b[39m multi_step_eFormer(model, features, hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_loss \u001b[39m=\u001b[39m loss_fn(predictions[:\u001b[39m-\u001b[39m(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)], labels[(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m batches_collected \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m:\n",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Perform the multi-step forecast\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m predictions, weights \u001b[39m=\u001b[39m multi_step_eFormer(model, features, hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m test_loss \u001b[39m=\u001b[39m loss_fn(predictions[:\u001b[39m-\u001b[39m(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)], labels[(hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_losses\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m batches_collected \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test data set\n",
    "test_losses = []\n",
    "predictions_collected = []\n",
    "groundtruth_collected = []\n",
    "batches_collected = 0  # Counter to keep track of the batches\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_eFormer(model, features, hyperparameters['step_forecast'])\n",
    "        test_loss = loss_fn(predictions[:-(hyperparameters['step_forecast']-1)], labels[(hyperparameters['step_forecast']-1):])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        if batches_collected >= 10:\n",
    "            pass  # Exit loop after collecting from 10 batches\n",
    "        else:\n",
    "            predictions_collected.extend(predictions.tolist())\n",
    "            groundtruth_collected.extend(labels.tolist())\n",
    "            batches_collected += 1\n",
    "\n",
    "test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "df_eval = pd.DataFrame({\n",
    "        'Predictions':predictions_collected,\n",
    "        'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_VanillaTransformer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = VanillaTransformer(\n",
    "        configs = Config(hyperparameters),\n",
    "        seq_len = (hyperparameters['seq_len'])).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            \n",
    "            # create new tensor\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction)\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X35sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m predictions, crps_weights \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X35sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X35sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/Benchmarks.py:85\u001b[0m, in \u001b[0;36mVanillaTransformer.forward\u001b[0;34m(self, input_data, enc_self_mask, dec_self_mask, dec_enc_mask)\u001b[0m\n\u001b[1;32m     82\u001b[0m enc_out, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(enc_in, attn_mask\u001b[39m=\u001b[39menc_self_mask)\n\u001b[1;32m     84\u001b[0m dec_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDecoderInput(enc_in)\n\u001b[0;32m---> 85\u001b[0m dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(dec_in, enc_in, x_mask\u001b[39m=\u001b[39;49mdec_self_mask, cross_mask\u001b[39m=\u001b[39;49mdec_enc_mask)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_attention:\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m dec_out[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_len:, \u001b[39m0\u001b[39m], attns\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:123\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, cross, x_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cross_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 123\u001b[0m         x \u001b[39m=\u001b[39m layer(x, cross, x_mask\u001b[39m=\u001b[39;49mx_mask, cross_mask\u001b[39m=\u001b[39;49mcross_mask)\n\u001b[1;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/Transformer_EncDec.py:102\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, x, cross, x_mask, cross_mask)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m     97\u001b[0m     x, x, x,\n\u001b[1;32m     98\u001b[0m     attn_mask\u001b[39m=\u001b[39mx_mask\n\u001b[1;32m     99\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    100\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[0;32m--> 102\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention(\n\u001b[1;32m    103\u001b[0m     x, cross, cross,\n\u001b[1;32m    104\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mcross_mask\n\u001b[1;32m    105\u001b[0m )[\u001b[39m0\u001b[39m])\n\u001b[1;32m    107\u001b[0m y \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)\n\u001b[1;32m    108\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(y\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))))\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/Benchmarks/layers/SelfAttention_Family.py:150\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, queries, keys, values, attn_mask)\u001b[0m\n\u001b[1;32m    147\u001b[0m H \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads\n\u001b[1;32m    149\u001b[0m queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_projection(queries)\u001b[39m.\u001b[39mview(B, L, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_projection(keys)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    151\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_projection(values)\u001b[39m.\u001b[39mview(B, S, H, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    153\u001b[0m out, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_attention(\n\u001b[1;32m    154\u001b[0m     queries,\n\u001b[1;32m    155\u001b[0m     keys,\n\u001b[1;32m    156\u001b[0m     values,\n\u001b[1;32m    157\u001b[0m     attn_mask\n\u001b[1;32m    158\u001b[0m )\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = VanillaTransformer(\n",
    "    configs = Config(hyperparameters),\n",
    "    seq_len = (hyperparameters['seq_len'] * 2)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    print(f\"training: \\n pred: {(predictions).shape}, labels: {labels.shape}\")\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    print(f\"eval: \\n pred: {(predictions).shape}, labels: {labels.shape}\")\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      " pred: torch.Size([506, 1]), labels: torch.Size([506, 1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     predictions, weights \u001b[39m=\u001b[39m multi_step_VanillaTransformer(model, features, hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m pred: \u001b[39m\u001b[39m{\u001b[39;00m(predictions[:\u001b[39m-\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]])\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, labels: \u001b[39m\u001b[39m{\u001b[39;00m(labels[hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]:])\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     test_loss \u001b[39m=\u001b[39m loss_fn(predictions[:\u001b[39m-\u001b[39mhyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]], labels[hyperparameters[\u001b[39m'\u001b[39m\u001b[39mstep_forecast\u001b[39m\u001b[39m'\u001b[39m]:])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     test_losses\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m test_loss_avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(test_losses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "model = VanillaTransformer(\n",
    "    configs = Config(hyperparameters),\n",
    "    seq_len = (hyperparameters['seq_len'] * 2)\n",
    ").to(device)\n",
    "\n",
    "# test data set\n",
    "test_losses = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Perform the multi-step forecast\n",
    "        predictions, weights = multi_step_VanillaTransformer(model, features, hyperparameters['step_forecast'])\n",
    "        print(f\"test:\\n pred: {(predictions[:-hyperparameters['step_forecast']]).shape}, labels: {(labels[hyperparameters['step_forecast']:]).shape}\")\n",
    "        test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    test_loss_avg = np.mean(test_losses)\n",
    "\n",
    "print(f\"Model test loss: {round(test_loss_avg,6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 2 with Loss: 18785.266602\n",
      "Validation Loss: 16526.207458\n",
      "Epoch Duration: 129.1507s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m features, labels \u001b[39min\u001b[39;49;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     features, labels \u001b[39m=\u001b[39;49m features\u001b[39m.\u001b[39;49mto(device), labels\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jan/Documents/Masterarbeit/src/2_0_eFormer.ipynb#X41sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.var/app/com.vscodium.codium/data/python/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/Masterarbeit/src/eFormer/Dataloader.py:91\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[39m# %% [markdown]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Training Data Set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# recurrent forecast\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mclass TimeSeriesDataProcessor:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    def __init__(self, dataframe, forecast, look_back, batch_size=64, train_size=0.7, test_size=0.5, random_state=42):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m        self.dataframe = dataframe\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m        self.forecast = forecast\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m        self.look_back = look_back\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m        self.batch_size = batch_size\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m        self.train_size = train_size\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m        self.test_size = test_size\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m        self.random_state = random_state\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    def padding_data(self, dataframe):\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m        remainder = dataframe.shape[0] % self.batch_size\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m        if remainder == 0:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m            return dataframe # Already divisible by batch size\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m        discard = remainder\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m        if isinstance(dataframe, pd.DataFrame):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m            return dataframe[discard:]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    def shifted_data(self):\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m        data = self.dataframe\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m        forecast = self.forecast\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m        look_back = self.look_back\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m        shifts = range(forecast, look_back + forecast)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m        variables = data.columns\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39m        shifted_columns = []\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m        for column in variables:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m            for i in shifts:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m                shifted_df = data[[column]].shift(i)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m                shifted_df.rename(columns={column: f\"{column} (lag {i})\"}, inplace=True)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m                shifted_columns.append(shifted_df)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m        \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m        data_shifted = pd.concat([data] + shifted_columns, axis=1)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m        data_shifted.dropna(inplace=True)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39m        return data_shifted\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    def prepare_datasets(self):\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m        try:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m            s_df = self.shifted_data().drop(['Wind speed (m/s)'], axis=1)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m        except KeyError:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m            s_df = self.shifted_data().copy()\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m        # Splitting dataset\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m        df_train, df_rem = train_test_split(s_df, train_size=self.train_size, random_state=False)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m        df_eval, df_test = train_test_split(df_rem, test_size=self.test_size, random_state=False)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[39m        df_train = self.padding_data(df_train)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m        df_eval = self.padding_data(df_eval)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m        df_test = self.padding_data(df_test)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m        # Wrapping datasets\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m        self.train_dataset = TimeSeriesDataset(df_train)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m        self.test_dataset = TimeSeriesDataset(df_test)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m        self.eval_dataset = TimeSeriesDataset(df_eval)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[39m    def create_dataloaders(self):\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m        self.prepare_datasets()\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[39m        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m        self.eval_loader = DataLoader(self.eval_dataset, batch_size=self.batch_size, shuffle=False)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39m        return self.train_loader, self.test_loader, self.eval_loader\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[39mclass TimeSeriesDataset(Dataset):\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    def __init__(self, dataframe):\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m        self.labels = dataframe.iloc[:, 0].values\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m        self.features = dataframe.iloc[:, 1:].values\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m    def __len__(self):\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m        return len(self.labels)\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m    def __getitem__(self, idx):\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m        features = torch.FloatTensor(self.features[idx])\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m        labels = torch.FloatTensor([self.labels[idx]])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m        return features, labels\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTimeSeriesDataProcessor\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, forecast, look_back, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, train_size\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initiate model etc\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=hyperparameters['patience'],\n",
    "    verbose=True\n",
    "    )\n",
    "model = Informer(\n",
    "    configs = Config(hyperparameters)\n",
    ").to(device)\n",
    "optimizer = AdamW(\n",
    "    params = model.parameters(),\n",
    "    lr=hyperparameters['learning_rate'],\n",
    "    weight_decay=hyperparameters['WeightDecay']\n",
    "    )\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = hyperparameters['train_epochs']\n",
    "\n",
    "# function to run monitoring in a separate thread\n",
    "def monitor_system_usage(every_n_seconds=10, keep_running=lambda: True, results_list=[]):\n",
    "    while keep_running():\n",
    "        comp_usage = check_system_conditions()\n",
    "        results_list.append(comp_usage)\n",
    "        time.sleep(every_n_seconds)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "system_usage_results = []\n",
    "\n",
    "# Define a lambda function to control the monitoring loop\n",
    "keep_monitoring = lambda: keep_monitoring_flag\n",
    "keep_monitoring_flag = True # Initialize the flag before starting training\n",
    "\n",
    "# Start the monitoring thread\n",
    "monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
    "monitor_thread.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions, crps_weights = model(features)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_avg = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    validation_losses = []\n",
    "    predictions_collected = []\n",
    "    groundtruth_collected = []\n",
    "    batches_collected = 0  # Counter to keep track of the batches\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            predictions, crps_weights = model(features)\n",
    "            val_loss = loss_fn(predictions, labels)\n",
    "            validation_losses.append(val_loss.item())\n",
    "\n",
    "            if batches_collected >= 10:\n",
    "                pass  # Exit loop after collecting from 10 batches\n",
    "            else:\n",
    "                predictions_collected.extend(predictions.tolist())\n",
    "                groundtruth_collected.extend(labels.tolist())\n",
    "                batches_collected += 1\n",
    "\n",
    "    val_loss_avg = np.mean(validation_losses)\n",
    "    print(f\"Validation Loss: {round(val_loss_avg, 6)}\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    print(f\"Epoch Duration: {round(epoch_duration, 4)}s\")\n",
    "\n",
    "    early_stopping(val_loss_avg)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# After training is done, set the flag to False to stop the monitoring thread\n",
    "keep_monitoring_flag = False\n",
    "monitor_thread.join()  # Wait for the monitoring thread to finish\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "system_usage_df = pd.DataFrame(system_usage_results)\n",
    "\n",
    "df_eval_Linear = pd.DataFrame({\n",
    "    'Predictions':predictions_collected,\n",
    "    'GroundTruth':groundtruth_collected})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_Informer(model, initial_input, steps):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input\n",
    "    model_2 = eFormer(\n",
    "    in_features=(hyperparameters['seq_len']),\n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    seq_len=hyperparameters['seq_len'],\n",
    "    forecast=hyperparameters['pred_len'],\n",
    "    len_embedding_vector=hyperparameters['len_embedding'],\n",
    "    n_heads=hyperparameters['n_heads'],\n",
    "    probabilistic_model=hyperparameters['ProbabilisticModel']).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # split tensor\n",
    "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
    "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
    "            # forecast output\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "            # forecast wind\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "            # update input\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # store values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.Tensor(forecasted_steps[-1]), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step(model, initial_input, steps, hyperparameters, device):\n",
    "    \"\"\"\n",
    "    Perform a recurrent multi-step forecast using the provided model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model for prediction.\n",
    "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
    "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
    "    - steps: The number of steps to forecast ahead.\n",
    "    - hyperparameters: A dictionary containing hyperparameters for the model.\n",
    "    - device: The device (CPU or GPU) on which the models are to run.\n",
    "\n",
    "    Returns:\n",
    "    - A list containing the forecasted values for each step ahead.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    forecasted_steps = []\n",
    "    current_input = initial_input.to(device)\n",
    "\n",
    "    # Assuming model_2 is already initialized and available here.\n",
    "    # If model_2's in_features need to be adjusted dynamically, you would adjust it here.\n",
    "    # Since the exact mechanism depends on the model's architecture, we'll assume\n",
    "    # it can be done without affecting the model's trained state or performance.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # Split tensor into first and second halves\n",
    "            first_half = current_input[:, :(current_input.shape[1] // 2)]\n",
    "            second_half = current_input[:, (current_input.shape[1] // 2):]\n",
    "\n",
    "            # Adjust model_2 for first half forecasting (conceptual)\n",
    "            # This would be replaced with your model-specific adjustment logic\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'])\n",
    "\n",
    "            # Forecast the first half\n",
    "            first_half_pred, _ = model_2(first_half)\n",
    "\n",
    "            # Update the first half with new predictions\n",
    "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
    "\n",
    "            # Restore model_2 to original state if needed (conceptual)\n",
    "            # This step depends on your specific model's design\n",
    "            # For example:\n",
    "            # model_2.adjust_in_features(new_in_features=hyperparameters['seq_len'] * 2)\n",
    "\n",
    "            # Use the original model for the second half forecast\n",
    "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
    "            prediction, weights = model(current_input)\n",
    "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
    "\n",
    "            # Create new tensor for the next step\n",
    "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
    "\n",
    "            # Store forecasted values\n",
    "            forecasted_steps.append(prediction.cpu().numpy())\n",
    "\n",
    "    return torch.tensor(forecasted_steps[-1]).to(device), weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
    "Kelmarsh_full_df = pd.concat(Kelmarsh_df.values(), ignore_index=True)\n",
    "Penmanshiel_full_df = pd.concat(Penmanshiel_df.values(), ignore_index=True)\n",
    "\n",
    "Wind_df = pd.concat([Kelmarsh_full_df, Penmanshiel_full_df], ignore_index=True, sort=False)\n",
    "\n",
    "Wind_df = Wind_df.drop('date', axis=1)\n",
    "\n",
    "Wind_df.to_csv('../data/Windturbinen/Wind_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wind_df = pd.read_csv('../data/Windturbinen/Wind_df.csv')\n",
    "Wind_df = Wind_df.drop(['Unnamed: 0', 'date', 'Long Term Wind (m/s)'], axis = 1)\n",
    "Wind_df = Wind_df.set_index('# Date and time')\n",
    "Wind_df.index.names = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To-Do's\n",
    "\n",
    "- ground truth and forecasted values in graph as time series\n",
    "- seperate model for wind and company, one model can't output 2 different results for same equation\n",
    "\n",
    "- label_length = look_back\n",
    "- sequence_length -> window size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
