{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFWDSxtCQ-2H",
        "outputId": "79bee8ac-4ab0-4946-e79f-87c1bad17078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POWLTqLDmkFu",
        "outputId": "9777013a-ab87-4850-d7c2-f8f64b19412b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/eFormer/src\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/eFormer/src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAAw4wBhQ76j",
        "outputId": "c29b243e-a4ce-49b5-d67a-b349c5d14c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=7e86948fb23b16bf66671ad5706ecb4cc7b38bdd61fdbda434156bf48a5e283c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory_profiler) (5.9.5)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil\n",
        "!pip install memory_profiler\n",
        "\n",
        "# standard\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from math import sqrt\n",
        "import time\n",
        "\n",
        "# reading data\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import DataParallel\n",
        "import torch.nn.functional as F\n",
        "from torch.fft import rfft, irfft, fftn, ifftn\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# visuals\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# resources\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "import threading\n",
        "import gc\n",
        "\n",
        "# eFormer\n",
        "from eFormer.embeddings import Encoding, ProbEncoding, PositionalEncoding\n",
        "from eFormer.sparse_attention import ProbSparseAttentionModule, DetSparseAttentionModule\n",
        "from eFormer.loss_function import CRPS, weighted_CRPS\n",
        "from eFormer.sparse_decoder import DetSparseDecoder, ProbSparseDecoder\n",
        "from eFormer.Dataloader import TimeSeriesDataProcessor\n",
        "\n",
        "# transformer Benchmarks\n",
        "from Benchmarks.Benchmarks import VanillaTransformer, Informer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W__DA9QC-fJK",
        "outputId": "396d1126-35f0-4b93-90c5-bc7627ec4ab9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing status files: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Processing turbine files: 100%|██████████| 36/36 [02:11<00:00,  3.66s/it]\n",
            "Processing status files: 0it [00:00, ?it/s]\n",
            "Processing turbine files: 100%|██████████| 84/84 [05:11<00:00,  3.70s/it]\n"
          ]
        }
      ],
      "source": [
        "class WindTurbineDataProcessor:\n",
        "    def __init__(self, turbine_directory, dependent_var='Power (kW)'):\n",
        "        self.directory = f'../data/Windturbinen/{turbine_directory}/'\n",
        "        self.dependent_var = dependent_var\n",
        "\n",
        "    def safe_datetime_conversion(self, s):\n",
        "        try:\n",
        "            return pd.to_datetime(s)\n",
        "        except:\n",
        "            return pd.NaT\n",
        "\n",
        "    def days_since_last_maintenance(self, row_date, maintenance_dates):\n",
        "        preceding_maintenance_dates = [date for date in maintenance_dates if date is not None and date <= row_date]\n",
        "        if not preceding_maintenance_dates:\n",
        "            return float('NaN')\n",
        "        last_maintenance_date = max(preceding_maintenance_dates)\n",
        "        delta = (row_date - last_maintenance_date).days\n",
        "        return delta\n",
        "\n",
        "    def check_missing_sequences(self, df):\n",
        "        sequences = []\n",
        "        current_sequence = 0\n",
        "        long_sequence_indices = []\n",
        "        start_index = None\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            if pd.isnull(row[self.dependent_var]):\n",
        "                current_sequence += 1\n",
        "                if start_index is None:\n",
        "                    start_index = i\n",
        "            else:\n",
        "                if current_sequence >= 19:\n",
        "                    sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
        "                    long_sequence_indices.extend(sequence_indices)\n",
        "                    df.loc[sequence_indices, self.dependent_var] = np.inf\n",
        "                if current_sequence > 0:\n",
        "                    sequences.append(current_sequence)\n",
        "                current_sequence = 0\n",
        "                start_index = None\n",
        "\n",
        "        if current_sequence > 0:\n",
        "            sequences.append(current_sequence)\n",
        "            if current_sequence >= 19:\n",
        "                sequence_indices = pd.date_range(start=start_index, periods=current_sequence, freq='10T')\n",
        "                long_sequence_indices.extend(sequence_indices)\n",
        "                df.loc[sequence_indices, self.dependent_var] = np.inf\n",
        "\n",
        "        df[self.dependent_var] = df[self.dependent_var].replace(np.inf, np.nan).interpolate(method='linear')\n",
        "        df.drop(long_sequence_indices, inplace=True)\n",
        "        return df\n",
        "\n",
        "    def process_and_load_data(self):\n",
        "        turbine_dataframes = defaultdict(list)\n",
        "        status_lists = defaultdict(list)\n",
        "\n",
        "        columns_turbine = ['# Date and time', 'Wind speed (m/s)', 'Power (kW)']\n",
        "        columns_status = ['Timestamp end', 'IEC category']\n",
        "\n",
        "        turbine_files = [f for f in os.listdir(self.directory) if f.startswith(\"Turbine_Data_\") and f.endswith(\".csv\")]\n",
        "        status_files = [f for f in os.listdir(self.directory) if f.startswith(\"Status_\") and f.endswith(\".csv\")]\n",
        "\n",
        "        for filename in tqdm(status_files, desc='Processing status files'):\n",
        "            turbine_number = filename.split(\"_\")[2]\n",
        "            filepath = os.path.join(self.directory, filename)\n",
        "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_status)\n",
        "            df['Timestamp end'] = df['Timestamp end'].apply(self.safe_datetime_conversion)\n",
        "            maintenance_dates = df[df['IEC category'] == 'Scheduled Maintenance']['Timestamp end'].unique()\n",
        "            status_lists[turbine_number].extend(maintenance_dates)\n",
        "\n",
        "        for filename in tqdm(turbine_files, desc='Processing turbine files'):\n",
        "            turbine_number = filename.split(\"_\")[3]\n",
        "            filepath = os.path.join(self.directory, filename)\n",
        "            df = pd.read_csv(filepath, skiprows=9, usecols=columns_turbine)\n",
        "            df['# Date and time'] = pd.to_datetime(df['# Date and time'])\n",
        "            turbine_dataframes[turbine_number].append(df)\n",
        "\n",
        "        for turbine_number, dfs in turbine_dataframes.items():\n",
        "            turbine_dataframes[turbine_number] = pd.concat(dfs).sort_values('# Date and time').reset_index(drop=True)\n",
        "            turbine_dataframes[turbine_number].set_index(pd.to_datetime(turbine_dataframes[turbine_number]['# Date and time']), inplace=True)\n",
        "            turbine_dataframes[turbine_number].drop(['# Date and time'], axis=1, inplace=True)\n",
        "            self.check_missing_sequences(turbine_dataframes[turbine_number])\n",
        "\n",
        "        gc.collect()\n",
        "        return turbine_dataframes\n",
        "\n",
        "def process_wind_turbines(turbine_directory, dependent_var):\n",
        "    processor = WindTurbineDataProcessor(turbine_directory, dependent_var)\n",
        "    return processor.process_and_load_data()\n",
        "\n",
        "Kelmarsh_dict = process_wind_turbines('Kelmarsh', 'Power (kW)')\n",
        "Penmanshiel_dict = process_wind_turbines('Penmanshiel', 'Power (kW)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsh0gu9Q76k"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06y1izDiQ76k",
        "outputId": "5ee1becc-35c2-4a7d-e394-dfa2fb1b8e1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on GPU\n"
          ]
        }
      ],
      "source": [
        "# set global parameters\n",
        "hyperparameters = {\n",
        "    'n_heads': 4,\n",
        "    'ProbabilisticModel': False,\n",
        "    # embeddings\n",
        "    'len_embedding': 32,\n",
        "    'batch_size': 512,\n",
        "    # general\n",
        "    'pred_len': 1,\n",
        "    'seq_len': 72,\n",
        "    'patience': 7,\n",
        "    'dropout': 0.05,\n",
        "    'learning_rate': 6e-4,\n",
        "    'WeightDecay': 1e-1,\n",
        "    'train_epochs': 100,\n",
        "    'num_workers': 10,\n",
        "    'step_forecast': 6,\n",
        "    # benchmarks\n",
        "    'factor': 1,\n",
        "    'output_attention': True,\n",
        "    'd_model': 32,\n",
        "    'c_out': 6,\n",
        "    'e_layers': 2,\n",
        "    'd_layers': 2,\n",
        "    'activation': 'relu',\n",
        "    'd_ff': 1,\n",
        "    'distil': True,\n",
        "    }\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # Use the first GPU available\n",
        "    print(\"Running on GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Fallback to CPU if no GPU is available\n",
        "    print(\"Running on CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5P-Gqo8261r"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CldfKwDhQ76o"
      },
      "outputs": [],
      "source": [
        "def check_system_conditions():\n",
        "    # Get CPU usage for each core\n",
        "    cpu_percent = round(psutil.cpu_percent(), 4)\n",
        "\n",
        "    # Get memory information\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    memory_used_gb = round(memory_info.used / (1024 ** 3), 4)\n",
        "\n",
        "    # Get GPU information\n",
        "    try:\n",
        "      gpu_info = GPUtil.getGPUs()[0]\n",
        "      gpu_memory_used_gb = round(gpu_info.memoryUsed / 1024, 4)\n",
        "    except IndexError:\n",
        "      # If no GPU is found, set variables to None\n",
        "      gpu_memory_used_gb = None\n",
        "\n",
        "    # Collect data in a dictionary\n",
        "    comp_usage = {\n",
        "        'CPU Usage': cpu_percent,\n",
        "        'Memory Usage (GB)': memory_used_gb,\n",
        "        'GPU Usage (GB)': gpu_memory_used_gb\n",
        "    }\n",
        "\n",
        "    return comp_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX0Oxtkr2uGq"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            #print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            #if self.verbose:\n",
        "                #print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f})')\n",
        "            self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0YBoQ7OXy4"
      },
      "outputs": [],
      "source": [
        "class UnifiedDataLoader:\n",
        "    def __init__(self, df_dict_1, df_dict_2, hyperparameters):\n",
        "        self.df_dict_1 = df_dict_1\n",
        "        self.df_dict_2 = df_dict_2\n",
        "        self.hyperparameters = hyperparameters\n",
        "        self.train_datasets = []\n",
        "        self.test_datasets = []\n",
        "        self.eval_datasets = []\n",
        "\n",
        "    def process_datasets(self, dataframe_dict):\n",
        "        for key, df in dataframe_dict.items():\n",
        "            processor = TimeSeriesDataProcessor(\n",
        "                dataframe=df,\n",
        "                forecast=1,\n",
        "                look_back=self.hyperparameters['seq_len'],\n",
        "                batch_size=self.hyperparameters['batch_size']\n",
        "            )\n",
        "            train_dataset, test_dataset, eval_dataset = processor.prepare_datasets()\n",
        "            self.train_datasets.append(train_dataset)\n",
        "            self.test_datasets.append(test_dataset)\n",
        "            self.eval_datasets.append(eval_dataset)\n",
        "            # Invoke garbage collection after processing each dataframe\n",
        "            gc.collect()\n",
        "\n",
        "    def create_concat_datasets(self):\n",
        "        self.process_datasets(self.df_dict_1)\n",
        "        self.process_datasets(self.df_dict_2)\n",
        "\n",
        "        # Concatenating the datasets\n",
        "        self.concat_train_dataset = ConcatDataset(self.train_datasets)\n",
        "        self.concat_test_dataset = ConcatDataset(self.test_datasets)\n",
        "        self.concat_eval_dataset = ConcatDataset(self.eval_datasets)\n",
        "\n",
        "        # Clear the lists to free up memory\n",
        "        self.train_datasets.clear()\n",
        "        self.test_datasets.clear()\n",
        "        self.eval_datasets.clear()\n",
        "\n",
        "        # Invoke garbage collection after clearing the lists\n",
        "        gc.collect()\n",
        "\n",
        "    def create_dataloaders(self):\n",
        "        self.create_concat_datasets()\n",
        "\n",
        "        # Creating the data loaders\n",
        "        self.train_loader = DataLoader(self.concat_train_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=True)\n",
        "        self.test_loader = DataLoader(self.concat_test_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
        "        self.eval_loader = DataLoader(self.concat_eval_dataset, batch_size=self.hyperparameters['batch_size'], shuffle=False)\n",
        "\n",
        "        # Invoke garbage collection after dataloaders are created\n",
        "        gc.collect()\n",
        "\n",
        "        return self.train_loader, self.test_loader, self.eval_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VOpyP-wikfw"
      },
      "source": [
        "## Informer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwnJJO3Fil9B"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            setattr(self, key, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNJyitEgSEY2"
      },
      "outputs": [],
      "source": [
        "def multi_step_Informer(model, model_2, initial_input, steps):\n",
        "    \"\"\"\n",
        "    Perform a recurrent multi-step forecast using the provided model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained model for prediction.\n",
        "    - initial_input: The initial input to start the forecast. This should be a tensor\n",
        "                     of shape (batch_size, seq_len * 2) based on your model definition.\n",
        "    - steps: The number of steps to forecast ahead.\n",
        "\n",
        "    Returns:\n",
        "    - A list containing the forecasted values for each step ahead.\n",
        "    \"\"\"\n",
        "    forecasted_steps = []\n",
        "    current_input = initial_input\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(steps):\n",
        "            # split tensor\n",
        "            first_half = current_input[:,:(current_input.shape[1]//2)]\n",
        "            second_half = current_input[:,(current_input.shape[1]//2):]\n",
        "            # forecast wind\n",
        "            first_half_pred, _ = model_2(first_half)\n",
        "            updated_first_half = torch.cat((first_half[:, 1:], first_half_pred), dim=1)\n",
        "            # update input\n",
        "            current_input = torch.cat((updated_first_half, second_half), dim=1)\n",
        "            # forecast output\n",
        "            prediction, weights = model(current_input)\n",
        "            updated_second_half = torch.cat((second_half[:, 1:], prediction), dim=1)\n",
        "\n",
        "            # create new tensor\n",
        "            current_input = torch.cat((updated_first_half, updated_second_half), dim=1)\n",
        "\n",
        "            # store values\n",
        "            forecasted_steps.append(prediction)\n",
        "\n",
        "    return torch.Tensor(forecasted_steps[-1]), weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtU39_bUimJ2",
        "outputId": "9f5b6c16-b17d-46ef-bbd4-ac1b89f0f259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('L1Loss', 72)\n",
            "Epoch 1 / 100 with Loss: 438.902588\n",
            "Epoch 2 / 100 with Loss: 170.61066\n",
            "Epoch 3 / 100 with Loss: 106.747448\n",
            "Epoch 4 / 100 with Loss: 99.33632\n",
            "Epoch 5 / 100 with Loss: 98.113184\n",
            "Epoch 6 / 100 with Loss: 98.267474\n",
            "Epoch 7 / 100 with Loss: 98.822313\n",
            "Epoch 8 / 100 with Loss: 99.545271\n",
            "Epoch 9 / 100 with Loss: 100.216402\n",
            "Epoch 10 / 100 with Loss: 100.886574\n",
            "Epoch 11 / 100 with Loss: 101.459352\n",
            "Epoch 12 / 100 with Loss: 101.893814\n",
            "Early stopping\n",
            "start of test phase: Thu Apr 25 14:28:22 2024\n"
          ]
        }
      ],
      "source": [
        "def run_experiment(loss_function_name, embedding_size, hyperparameters):\n",
        "    # Adjust loss function based on the input argument\n",
        "    if loss_function_name == 'CRPS':\n",
        "        loss_fn = CRPS()  # Assuming CRPS is a defined class/function\n",
        "    elif loss_function_name == 'MSELoss':\n",
        "        loss_fn = nn.MSELoss()\n",
        "    elif loss_function_name == 'L1Loss':\n",
        "        loss_fn = nn.L1Loss()\n",
        "    elif loss_function_name == 'CrossEntropyLoss':\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown loss function: {loss_function_name}\")\n",
        "\n",
        "    hyperparameters['step_forecast'] = step_forecast\n",
        "\n",
        "    if step_forecast == 1:\n",
        "        hyperparameters['seq_len'] = 72\n",
        "    elif step_forecast == 6:\n",
        "        hyperparameters['seq_len'] = 72\n",
        "    elif step_forecast == 72:\n",
        "        hyperparameters['seq_len'] = 288\n",
        "    elif step_forecast == 144:\n",
        "        hyperparameters['seq_len'] = 576\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown forecast horizon: {step_forecast}\")\n",
        "\n",
        "    # initiate model etc\n",
        "    early_stopping = EarlyStopping(\n",
        "        patience=hyperparameters['patience'],\n",
        "        verbose=True\n",
        "        )\n",
        "    model = Informer(\n",
        "        configs = Config(hyperparameters),\n",
        "        seq_len = (hyperparameters['seq_len'] * 2)\n",
        "        ).to(device)\n",
        "    model_2 = Informer(\n",
        "        configs = Config(hyperparameters),\n",
        "        seq_len = (hyperparameters['seq_len'])\n",
        "        ).to(device)\n",
        "    optimizer = AdamW(\n",
        "        params = model.parameters(),\n",
        "        lr=hyperparameters['learning_rate'],\n",
        "        weight_decay=hyperparameters['WeightDecay']\n",
        "        )\n",
        "\n",
        "    num_epochs = hyperparameters['train_epochs']\n",
        "\n",
        "    # function to run monitoring in a separate thread\n",
        "    def monitor_system_usage(every_n_seconds=1, keep_running=lambda: True, results_list=[]):\n",
        "        while keep_running():\n",
        "            comp_usage = check_system_conditions()\n",
        "            results_list.append(comp_usage)\n",
        "            time.sleep(every_n_seconds)\n",
        "\n",
        "    # Initialize a list to store the results\n",
        "    system_usage_results = []\n",
        "    training_time = []\n",
        "\n",
        "    # Define a lambda function to control the monitoring loop\n",
        "    keep_monitoring = lambda: keep_monitoring_flag\n",
        "    keep_monitoring_flag = True # Initialize the flag before starting training\n",
        "\n",
        "    # Start the monitoring thread\n",
        "    monitor_thread = threading.Thread(target=monitor_system_usage, args=(5, keep_monitoring, system_usage_results))\n",
        "    monitor_thread.start()\n",
        "\n",
        "    # Instantiate the UnifiedDataLoader class\n",
        "    loader = UnifiedDataLoader(\n",
        "        df_dict_1=Kelmarsh_dict,\n",
        "        df_dict_2=Penmanshiel_dict,\n",
        "        hyperparameters=hyperparameters)\n",
        "\n",
        "    # Use the new method to get the data loaders\n",
        "    train_loader, test_loader, eval_loader = loader.create_dataloaders()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            predictions, crps_weights = model(features)\n",
        "            loss = loss_fn(predictions, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        train_loss_avg = np.mean(train_losses)\n",
        "        print(f\"Epoch {epoch + 1} / {num_epochs} with Loss: {round(train_loss_avg, 6)}\")\n",
        "\n",
        "         # Validation phase\n",
        "        model.eval()\n",
        "        validation_losses = []\n",
        "        with torch.no_grad():\n",
        "            for features, labels in eval_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                predictions, crps_weights = model(features)\n",
        "                val_loss = loss_fn(predictions, labels)\n",
        "                validation_losses.append(val_loss.item())\n",
        "\n",
        "        val_loss_avg = np.mean(validation_losses)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        training_time.append(epoch_duration)\n",
        "        #print(f\"Epoch Duration: \\n {round(epoch_duration, 4)}s\")\n",
        "\n",
        "        early_stopping(val_loss_avg)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    # After training is done, set the flag to False to stop the monitoring thread\n",
        "    keep_monitoring_flag = False\n",
        "    monitor_thread.join()  # Wait for the monitoring thread to finish\n",
        "\n",
        "    # Convert the results list to a DataFrame\n",
        "    system_usage = pd.DataFrame(system_usage_results)\n",
        "\n",
        "    print(f\"start of test phase: {time.ctime()}\")\n",
        "\n",
        "    # test data set\n",
        "    test_losses = []\n",
        "    predictions_collected = []\n",
        "    groundtruth_collected = []\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = DataParallel(model)\n",
        "        model_2 = DataParallel(model_2)\n",
        "\n",
        "    model.eval()\n",
        "    model_2.eval()\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Perform the multi-step forecast\n",
        "            predictions, weights = multi_step_Informer(model, model_2, features, hyperparameters['step_forecast'])\n",
        "            test_loss = loss_fn(predictions[:-hyperparameters['step_forecast']], labels[hyperparameters['step_forecast']:])\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            predictions_collected.extend(predictions.tolist())\n",
        "            groundtruth_collected.extend(labels.tolist())\n",
        "\n",
        "        test_loss_avg = np.mean(test_losses)\n",
        "\n",
        "    print(f\"\\n Model test loss: {round(test_loss_avg,6)}\")\n",
        "\n",
        "    # Save Results\n",
        "    df_eval = pd.DataFrame({\n",
        "        'Predictions':predictions_collected,\n",
        "        'GroundTruth':groundtruth_collected,\n",
        "        'TestLoss':round(test_loss_avg,6)})\n",
        "    df_epoch = pd.DataFrame({\n",
        "        'Epoch Duration':training_time})\n",
        "\n",
        "    import re\n",
        "    def re_loss_fn(s):\n",
        "        pattern = r'(?:nn\\.)?(\\w+)\\(\\)'\n",
        "        match = re.search(pattern, s)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "    system_usage.to_csv(f\"../data/df_SystemUsage_Informer_emb{hyperparameters['len_embedding']}_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "    df_eval.to_csv(f\"../data/df_eval_Informer_emb{hyperparameters['len_embedding']}_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "    df_epoch.to_csv(f\"../data/df_epoch_Informer_emb{hyperparameters['len_embedding']}_pred{hyperparameters['step_forecast']}_{re_loss_fn(str(loss_fn))}.csv\")\n",
        "\n",
        "loss_functions = ['L1Loss', 'MSELoss']\n",
        "step_forecasts = [72]\n",
        "\n",
        "for loss_function in loss_functions:\n",
        "    for step_forecast in step_forecasts:\n",
        "        print(f\"{loss_function, step_forecast}\")\n",
        "        run_experiment(loss_function, step_forecast, hyperparameters)\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}